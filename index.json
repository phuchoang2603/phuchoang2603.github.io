[{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/","section":"/home/fel1x","summary":"","title":"/home/fel1x","type":"page"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/argo-cd/","section":"Tags","summary":"","title":"Argo-Cd","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/github-actions/","section":"Tags","summary":"","title":"Github-Actions","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/series/on-premise-101/","section":"Series","summary":"","title":"On-Premise 101","type":"series"},{"content":"I\u0026rsquo;m the kind of person who will happily spend 10 hours building an automation script just to save 10 minutes of manual work every day. If that sounds like you, you\u0026rsquo;re in the right place.\nAfter covering the foundations of our hypervisor and storage platform, it\u0026rsquo;s time to move on to the most interesting part of this series: provisioning a Kubernetes cluster on Proxmox. This means creating a bunch of VMs, and I refuse to do that manually. Luckily, I have learned Terraform and Ansible somewhat enough to be able to create this project below to automate it all.\nphuchoang2603/kubernetes-proxmox auto provision and deploy k8s to proxmox using terraform + ansible Jinja 7 0 This post is first part of my deep dive on the project, focusing on how I use Terraform to create a repeatable, automated, and scalable VMs cluster on Proxmox, all with a single command. If you want to see how this project performs in action, you can watch the video below.\nWhy Automate? The Case for IaC with Terraform # \u0026ldquo;But what is Terraform?\u0026rdquo; you might ask. Again, we need to consider the core problem it solves to truly appreciate it.\nBefore Terraform, and before the \u0026ldquo;cloud\u0026rdquo; even existed, people deployed infrastructure by manually creating VMs with repeated configurations (CPU, RAM, network, firewall) and then SSH into each machine to run commands,or if they were fancy, a bash script. This did the job, but doing it manually was error-prone, time-consuming, and a nightmare to document.\nThe \u0026ldquo;cloud\u0026rdquo; was born to help solve this: it\u0026rsquo;s still someone\u0026rsquo;s computer, but those computers are now abstracted with many services on top, so we no longer need to build the infrastructure ourselves and can create VMs faster. Heck, we might not need to create VMs; instead, we just need to write a Dockerfile, or even just the function of the code, to get it deployed directly. However, we still often need to manually log in to a website and create VMs through a clunky UI.\nThat\u0026rsquo;s where Terraform comes in. It\u0026rsquo;s an open-source, Infrastructure as Code (IaC) tool from HashiCorp that lets you define and provision infrastructure using a declarative configuration language called HCL. It allows you to build, change, and manage infrastructure safely and efficiently. By writing code to describe your desired end state, Terraform automates the process of creating, updating, and destroying infrastructure, reducing manual effort and improving consistency.\nBecause your infrastructure is now code, it inherits awesome features like version history (via Git) and a documented process. If something goes wrong, you can tear down and bring up the entire infrastructure again with just two commands: terraform destroy and terraform apply. If someone new joins the team, they can easily get a grasp of the infrastructure by reading the code, without endless clicking on a cloud provider\u0026rsquo;s website.\nBy using Terraform, I can easily create tons of VMs on Proxmox and show you exactly what I\u0026rsquo;m doing.\nMy Terraform Blueprint: Project Structure \u0026amp; Principles # In the image above, we see 4 files on the left. These also represent the 4 primary components of a Terraform project: providers, variables, state, and instances (resources/modules).\nBefore diving into each component, let\u0026rsquo;s establish the core design principles that keep our Terraform code from becoming a messy nightmare (otherwise, it\u0026rsquo;s no better than the clunky UI):\nSeparation of Concerns: Each file should has a distinct and clear purpose. Variable-Driven Configuration: The infrastructure\u0026rsquo;s composition (how many nodes, their IPs, etc.) is defined in variables file, not hardcoded in the logic. Modularity: Reusable components (like a virtual machine) are encapsulated in modules. Environment Isolation: Configurations for different environments (like dev and prod) are kept separate. To give you a map of what I\u0026rsquo;m doing, here\u0026rsquo;s my Terraform file structure:\n. ├── create_user_config.tf ├── download_img.tf ├── env │ ├── dev │ │ ├── backend.hcl │ │ ├── k8s_nodes.json │ │ ├── longhorn_nodes.json │ │ └── main.tfvars │ └── prod │ ├── backend.hcl │ ├── k8s_nodes.json │ ├── longhorn_nodes.json │ └── main.tfvars ├── modules │ └── vm │ ├── main.tf │ ├── outputs.tf │ └── variables.tf ├── provider.tf ├── variables.tf └── vms.tf Connecting to Proxmox and S3 # provider.tf: Defining Our Connections # Providers are plugins that enable Terraform to interact with various platforms (AWS, Azure, GCP) and services (Kubernetes, DNS, etc.). They understand the resource types and make the API calls to create, read, update, and delete them.\nIn my case, I need to spin up VMs on Proxmox, so I looked for a provider that integrates with it. There are two prominent ones: Telmate/terraform-provider-proxmox and bpg/terraform-provider-proxmox. The one from telmate has been around for a long time but is limited in functionality. The one from bpg is newer and allows you to manage every aspect of a Proxmox environment. Thus, I opted for bpg.\nLater, I\u0026rsquo;ll introduce the concept of \u0026ldquo;remote state,\u0026rdquo; which uses a backend provider. I\u0026rsquo;ll be using an S3-compatible object storage (Minio), so I\u0026rsquo;ll define that as well. Enough talk, here\u0026rsquo;s my provider.tf file.\nterraform { required_version = \u0026#34;\u0026gt;= 1.6.6\u0026#34; required_providers { `Proxmox` = { source = \u0026#34;bpg/proxmox\u0026#34; version = \u0026#34;0.77.1\u0026#34; } } backend \u0026#34;s3\u0026#34; {} } provider \u0026#34;proxmox\u0026#34; { endpoint = var.proxmox_endpoint insecure = var.proxmox_insecure min_tls = var.proxmox_min_tls username = var.proxmox_username password = var.proxmox_password } Notice the version constraint - this is a critical best practice that locks your project to a specific provider version, preventing unexpected breaking changes from automatic updates.\nterraform.tfstate: Using a Remote Backend # Terraform state is a crucial component that stores a mapping of your configured resources to the real-world objects that have been created. It’s how Terraform knows what it manages.\nThere are two primary ways to store state: local (a terraform.tfstate file on your computer) and remote (on an online storage service like S3). The local state is fine for solo experiments but is disastrous for teams where multiple people are trying to make changes. What happens is Terraform can\u0026rsquo;t match the desired state (your code) with the actual state (on the server) because there\u0026rsquo;s no single source of truth.\nTherefore, I decided to use a remote backend. The credentials for it are handled by the env/dev/backend.hcl file below.\naccess_key = \u0026#34;terraform\u0026#34; secret_key = \u0026#34;QmwRghG9X80kC%\u0026#34; bucket = \u0026#34;terraform\u0026#34; key = \u0026#34;dev.tfstate\u0026#34; region = \u0026#34;us-east-1\u0026#34; # --- MinIO Specific Settings --- # The URL of your MinIO server endpoints = { s3 = \u0026#34;http://10.69.1.102:9000\u0026#34; } # Use path-style addressing (e.g., http://minio/bucket/key) use_path_style = true # Skip AWS-specific validation checks skip_credentials_validation = true # Skip AWS related checks and validations skip_requesting_account_id = true skip_metadata_api_check = true skip_region_validation = true What\u0026rsquo;s more, using remote state, I can now use my Terraform code for multiple backends. I just need to keep my two backends (dev and prod) separate and I can decide which environment to initialize Terraform with by running:\nterraform init -reconfigure -backend-config=env/dev/backend.hcl # For production: # terraform init -reconfigure -backend-config=env/prod/backend.hcl This keeps your core configuration clean and environment-agnostic. It securely stores your state in a remote, centralized location, enabling collaboration and preventing state file loss.\nbackend.hcl: Using Minio for Remote State # To get the credentials for the backend.hcl file, you first need to install Minio on your storage platform (in my case, TrueNAS). You may also need to install minio-console. This is because we need to create an IAM user for terraform and get its credentials, and the primary minio container UI has removed some of those features. The minio-console brings those features back. The \u0026ldquo;Inputs\u0026rdquo;: Variables, tfvars, and JSON # Next up are variables, which are the parameters for your Terraform configuration. They allow you to create flexible and reusable code by separating the \u0026ldquo;what\u0026rdquo; (the resource logic) from the \u0026ldquo;how\u0026rdquo; (the specific values).\nvariables.tf: Defining the \u0026ldquo;API\u0026rdquo; # This file acts as the \u0026ldquo;API\u0026rdquo; for the entire project. It defines every possible input, from provider credentials to VM specs, and sets optional default values. This file makes the project self-documenting.\nvariable \u0026#34;env\u0026#34; { description = \u0026#34;Environment name (e.g., dev, prod)\u0026#34; type = string } variable \u0026#34;proxmox_endpoint\u0026#34; { type = string description = \u0026#34;Proxmox API endpoint (e.g., https://your-proxmox-ip:8006)\u0026#34; } variable \u0026#34;proxmox_insecure\u0026#34; { type = bool description = \u0026#34;Skip TLS verification\u0026#34; default = true } variable \u0026#34;proxmox_min_tls\u0026#34; { type = string description = \u0026#34;Minimum TLS version\u0026#34; default = \u0026#34;1.3\u0026#34; } variable \u0026#34;proxmox_username\u0026#34; { description = \u0026#34;Proxmox username\u0026#34; type = string } variable \u0026#34;proxmox_password\u0026#34; { description = \u0026#34;Proxmox password\u0026#34; type = string sensitive = true } variable \u0026#34;proxmox_ssh_public_key\u0026#34; { description = \u0026#34;SSH public key for VM access\u0026#34; type = string } variable \u0026#34;vm_node_name\u0026#34; { description = \u0026#34;Proxmox node where VMs are created\u0026#34; type = string } variable \u0026#34;vm_datastore_id\u0026#34; { description = \u0026#34;Proxmox datastore ID where snippets cloud img are stored\u0026#34; type = string } variable \u0026#34;vm_bridge\u0026#34; { description = \u0026#34;Network bridge used for VM network\u0026#34; type = string } variable \u0026#34;vm_timezone\u0026#34; { description = \u0026#34;Timezone for the VM\u0026#34; type = string } variable \u0026#34;vm_username\u0026#34; { description = \u0026#34;Username for the VM template\u0026#34; type = string } variable \u0026#34;vm_ip_gateway\u0026#34; { description = \u0026#34;Gateway for Kubernetes VMs\u0026#34; type = string } variable \u0026#34;dns_server\u0026#34; { description = \u0026#34;DNS server for Kubernetes VMs\u0026#34; type = string } variable \u0026#34;k8s_cpu_cores\u0026#34; { description = \u0026#34;Number of CPU cores per VM\u0026#34; type = number } variable \u0026#34;k8s_cpu_type\u0026#34; { description = \u0026#34;CPU type for VM\u0026#34; type = string } variable \u0026#34;k8s_memory_mb\u0026#34; { description = \u0026#34;Memory size in MB per VM\u0026#34; type = number } variable \u0026#34;k8s_datastore_id\u0026#34; { description = \u0026#34;k8s datastore ID where VM disks are stored\u0026#34; type = string } variable \u0026#34;k8s_disk_size_gb\u0026#34; { description = \u0026#34;Disk size in GB for VM disk\u0026#34; type = number } variable \u0026#34;longhorn_cpu_cores\u0026#34; { description = \u0026#34;Number of CPU cores per VM\u0026#34; type = number } variable \u0026#34;longhorn_cpu_type\u0026#34; { description = \u0026#34;CPU type for VM\u0026#34; type = string } variable \u0026#34;longhorn_memory_mb\u0026#34; { description = \u0026#34;Memory size in MB per VM\u0026#34; type = number } variable \u0026#34;longhorn_datastore_id\u0026#34; { description = \u0026#34;longhorn datastore ID where VM disks are stored\u0026#34; type = string } variable \u0026#34;longhorn_disk_size_gb\u0026#34; { description = \u0026#34;Disk size in GB for VM disk\u0026#34; type = number } locals { k8s_nodes = jsondecode(file(\u0026#34;${path.root}/env/${var.env}/k8s_nodes.json\u0026#34;)) longhorn_nodes = jsondecode(file(\u0026#34;${path.root}/env/${var.env}/longhorn_nodes.json\u0026#34;)) } main.tfvars: Setting Environment Values # While variables.tf defines the inputs, this file provides the concrete values for a specific environment. The reason I separate my *.tfvars files into env/dev/ and env/prod/ is the same as for the backend state: it allows me to spin up a completely new environment just by creating a new directory. You can tell Terraform which one to use like this:\nterraform apply -var-file=\u0026#34;env/dev/main.tfvars\u0026#34; # For production: # terraform apply -var-file=\u0026#34;env/prod/main.tfvars\u0026#34; Here\u0026rsquo;s an example of env/dev/main.tfvars. The variable names here must exactly match the names in variables.tf.\n# Environment name env = \u0026#34;dev\u0026#34; # `Proxmox` API details - PLEASE FILL THESE IN proxmox_endpoint = \u0026#34;https://10.69.1.1:8006/\u0026#34; proxmox_username = \u0026#34;root@pam\u0026#34; proxmox_password = \u0026#34;Phuc@2006\u0026#34; proxmox_ssh_public_key = \u0026#34;/home/felix/.ssh/id_ed25519.pub\u0026#34; # Absolute path of your ssh public key on your machine # General VM settings - PLEASE REVIEW AND ADJUST vm_node_name = \u0026#34;pve\u0026#34; # `Proxmox` node where VMs are created vm_datastore_id = \u0026#34;truenas\u0026#34; # storage for downloading cloud img, storing snippets, etc. vm_bridge = \u0026#34;vmbr0\u0026#34; vm_timezone = \u0026#34;America/New_York\u0026#34; vm_username = \u0026#34;ubuntu\u0026#34; vm_ip_gateway = \u0026#34;10.69.0.1\u0026#34; dns_server = \u0026#34;1.1.1.1\u0026#34; # k8s cluster settings k8s_cpu_cores = 2 k8s_cpu_type = \u0026#34;x86-64-v2-AES\u0026#34; k8s_memory_mb = 4096 k8s_disk_size_gb = 64 k8s_datastore_id = \u0026#34;local-lvm\u0026#34; # longhorn cluster settings longhorn_cpu_cores = 2 longhorn_cpu_type = \u0026#34;x86-64-v2-AES\u0026#34; longhorn_memory_mb = 2048 longhorn_disk_size_gb = 300 longhorn_datastore_id = \u0026#34;local-lvm\u0026#34; k8s_nodes.json: A Dynamic Node Inventory # locals { k8s_nodes = jsondecode(file(\u0026#34;${path.root}/env/${var.env}/k8s_nodes.json\u0026#34;)) longhorn_nodes = jsondecode(file(\u0026#34;${path.root}/env/${var.env}/longhorn_nodes.json\u0026#34;)) } You might have noticed the locals block at the end of variables.tf. This block dynamically loads the correct node inventory based on the env variable and makes it available to the rest of your configuration files. For example, it will read env/dev/k8s_nodes.json, which is the source of truth for my Kubernetes nodes. It defines each node’s name, ID, and IP address. To scale the cluster, you simply add a new entry to it.\n{ \u0026#34;dev-server1\u0026#34;: { \u0026#34;vm_id\u0026#34;: 111, \u0026#34;node\u0026#34;: \u0026#34;pve\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;servers\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;10.69.1.111/16\u0026#34; }, \u0026#34;dev-server2\u0026#34;: { \u0026#34;vm_id\u0026#34;: 112, \u0026#34;node\u0026#34;: \u0026#34;pve2\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;servers\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;10.69.1.112/16\u0026#34; }, \u0026#34;dev-server3\u0026#34;: { \u0026#34;vm_id\u0026#34;: 113, \u0026#34;node\u0026#34;: \u0026#34;pve3\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;servers\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;10.69.1.113/16\u0026#34; } } Not only does this make it easy to define multiple nodes, but it also helps Ansible (in the next part) read this same file to configure the IPs.\nThe \u0026ldquo;Blueprint\u0026rdquo;: Assembling the Cluster # Now that we have our variables configured, Terraform can talk to our infrastructure. The next step is to define what to say.\nThese are the .tf files that contain the logic - the blueprint for your infrastructure. They define resources, call modules, and orchestrate the entire deployment.\ndownload_img.tf: Acquiring the Base Image # resource \u0026#34;proxmox_virtual_environment_download_file\u0026#34; \u0026#34;ubuntu_cloud_image\u0026#34; { content_type = \u0026#34;iso\u0026#34; datastore_id = var.vm_datastore_id node_name = var.vm_node_name file_name = \u0026#34;${var.env}-noble-server-cloudimg-amd64.img\u0026#34; url = \u0026#34;https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\u0026#34; } This file has one job: download the Ubuntu cloud image from the internet and store it in Proxmox. The proxmox_virtual_environment_download_file resource from the bpg/proxmox provider handles this, ensuring the base image is available before any VMs are created. Its output ID is used later to specify the base disk for the VMs installation.\nThis ISO file is special because you can modify the VM configuration directly with the cloud-init bundle with it.\nResult: create_user_config.tf: Preparing Cloud-Init # data \u0026#34;local_file\u0026#34; \u0026#34;ssh_public_key\u0026#34; { filename = var.proxmox_ssh_public_key } resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;user_data_cloud_config\u0026#34; { content_type = \u0026#34;snippets\u0026#34; datastore_id = var.vm_datastore_id node_name = var.vm_node_name source_raw { data = \u0026lt;\u0026lt;-EOF #cloud-config host-name: ubuntu_cloud_image timezone: ${var.vm_timezone} users: - default - name: ${var.vm_username} groups: - sudo shell: /bin/bash ssh_authorized_keys: - ${trimspace(data.local_file.ssh_public_key.content)} sudo: ALL=(ALL) NOPASSWD:ALL package_update: true packages: - qemu-guest-agent - net-tools - curl - cryptsetup runcmd: - systemctl start qemu-guest-agent EOF file_name = \u0026#34;user-data-cloud-config.yaml\u0026#34; } } resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;meta_data_cloud_config\u0026#34; { for_each = merge(local.k8s_nodes, local.longhorn_nodes) content_type = \u0026#34;snippets\u0026#34; datastore_id = var.vm_datastore_id node_name = var.vm_node_name source_raw { data = \u0026lt;\u0026lt;-EOF #cloud-config local-hostname: ${each.key} EOF file_name = \u0026#34;${each.key}-meta-data-cloud-config.yaml\u0026#34; } } This is one of the most important files for automation. It creates the configuration “snippets” that cloud-init will use to configure the VMs on their first boot.\nuser_data_cloud_config: This is a generic configuration applied to all VMs. It sets the timezone, creates a default user (ubuntu), injects your SSH public key for passwordless access, and installs essential packages like qemu-guest-agent. meta_data_cloud_config: This creates a specific metadata file for each VM. It iterates through the node maps loaded in locals.tf and creates a snippet that sets the correct hostname for each machine (e.g., dev-server1). Result: modules/vm/: The Reusable VM \u0026ldquo;Factory\u0026rdquo; # This directory encapsulates all the logic for creating a single Proxmox VM. It\u0026rsquo;s a Terraform module—a reusable package of configuration, much like a \u0026ldquo;class\u0026rdquo; in object-oriented programming.\nvariables.tf: This is the “order form” for the factory. It defines all the parameters needed to create one VM, such as its name, CPU cores, memory, IP address, etc. main.tf: This is the “assembly line.” It contains the proxmox_virtual_environment_vm resource block. It takes all the variables and uses them to configure the VM’s properties, from its CPU and memory to its disk and network settings. outputs.tf: This defines the “shipping label,” outputting the final vm_id of the created machine. # main.tf terraform { required_providers { `Proxmox` = { source = \u0026#34;bpg/proxmox\u0026#34; version = \u0026#34;0.77.1\u0026#34; } } } resource \u0026#34;proxmox_virtual_environment_vm\u0026#34; \u0026#34;vm\u0026#34; { name = var.name node_name = var.node_name vm_id = var.vm_id agent { enabled = true } stop_on_destroy = true machine = \u0026#34;q35\u0026#34; bios = \u0026#34;ovmf\u0026#34; description = \u0026#34;Cloud-Init ready Kubernetes template managed by Terraform\u0026#34; cpu { cores = var.cpu_cores type = var.cpu_type } memory { dedicated = var.memory_mb } efi_disk { datastore_id = var.datastore_id type = \u0026#34;4m\u0026#34; } disk { datastore_id = var.datastore_id file_id = var.disk_file_id interface = \u0026#34;virtio0\u0026#34; iothread = true discard = \u0026#34;on\u0026#34; size = var.disk_size_gb ssd = true } network_device { bridge = var.bridge } initialization { dns { servers = [var.dns_server] } ip_config { ipv4 { address = var.ip_address gateway = var.ip_gateway } } user_data_file_id = var.user_data_file_id meta_data_file_id = var.meta_data_file_id } } vms.tf: Orchestrating the Build with for_each # Now for the magic. All that setup—the variables, the JSON file, the module—was for this. This one file is our master orchestration file. Watch how we use for_each to loop through our JSON node list and pass the values to our VM module. This is how we create 12 (or 100) VMs as easily as one.\nmodule \u0026#34;k8s_nodes\u0026#34; { source = \u0026#34;./modules/vm\u0026#34; for_each = local.k8s_nodes name = each.key node_name = each.value.node vm_id = each.value.vm_id cpu_cores = var.k8s_cpu_cores cpu_type = var.k8s_cpu_type memory_mb = var.k8s_memory_mb datastore_id = var.k8s_datastore_id disk_file_id = proxmox_virtual_environment_download_file.ubuntu_cloud_image.id disk_size_gb = var.k8s_disk_size_gb bridge = var.vm_bridge dns_server = var.dns_server ip_address = each.value.address ip_gateway = var.vm_ip_gateway user_data_file_id = proxmox_virtual_environment_file.user_data_cloud_config.id meta_data_file_id = proxmox_virtual_environment_file.meta_data_cloud_config[each.key].id } module \u0026#34;longhorn_nodes\u0026#34; { source = \u0026#34;./modules/vm\u0026#34; for_each = local.longhorn_nodes name = each.key node_name = each.value.node vm_id = each.value.vm_id cpu_cores = var.longhorn_cpu_cores cpu_type = var.longhorn_cpu_type memory_mb = var.longhorn_memory_mb datastore_id = var.longhorn_datastore_id disk_file_id = proxmox_virtual_environment_download_file.ubuntu_cloud_image.id disk_size_gb = var.longhorn_disk_size_gb bridge = var.vm_bridge dns_server = var.dns_server ip_address = each.value.address ip_gateway = var.vm_ip_gateway user_data_file_id = proxmox_virtual_environment_file.user_data_cloud_config.id meta_data_file_id = proxmox_virtual_environment_file.meta_data_cloud_config[each.key].id } For every entry in local.k8s_nodes (and longhorn_nodes) variables, this file calls the ./modules/vm module, passing in the specific configuration. It combines:\nThe node-specific data from the JSON file (each.key, each.value.address). The generic VM settings from main.tfvars (var.k8s_cpu_cores). The IDs of the resources created in other files (the downloaded image and the cloud-init snippets). Result: Conclusion: The 12-Hour, 12-Minute Payoff # And with that, I\u0026rsquo;ve successfully created 12 VMs on Proxmox, all with specific, defined configurations, just by running terraform apply.\nYou could say I took 12 hours to automate a 12-minute job, and you\u0026rsquo;d be right. But I\u0026rsquo;m still incredibly proud of this work. Now, I can spin down the entire cluster and spin it back up again, at will, in minutes. What\u0026rsquo;s more, I can scale up or change the configuration for all my VMs just by editing a single JSON file.\nThat is the power of well-designed Infrastructure as Code.\n","date":"6 November 2025","externalUrl":null,"permalink":"/posts/on-premise-provision-terraform/","section":"Posts","summary":"\u003cp\u003eI\u0026rsquo;m the kind of person who will happily spend 10 hours building an automation script just to save 10 minutes of manual work every day. If that sounds like you, you\u0026rsquo;re in the right place.\u003c/p\u003e","title":"On-Premise 101 (Part 4): Automating 12 VMs creation on Proxmox with Terraform","type":"posts"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"Prometheus","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/proxmox/","section":"Tags","summary":"","title":"Proxmox","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":"https://github.com/phuchoang2603/realtime-credit-card-fraud-detection","permalink":"/projects/realtime-credit-card-fraud-detection/","section":"Projects","summary":"This repository contains a complete end-to-end system for real-time credit card fraud detection, including data analysis notebooks, a machine learning API, and infrastructure-as-code for cloud deployment. The system is designed with a modern, observable, and scalable architecture.","title":"Scalable Real-Time Credit Card Fraud Detection System","type":"projects"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/tags/arxiv/","section":"Tags","summary":"","title":"Arxiv","type":"tags"},{"content":"This is my story of how I attempt to build an AI-powered pipeline for ArXiv papers. It was a journey that started with a cool idea about AI agents and ended with me wrestling Docker, n8n, and Python into submission.\nphuchoang2603/sis-arxiv-vad-papers hosts arxiv papers on the topic \u0026ldquo;video anomaly detection\u0026rdquo; Python 2 0 Part 1: The AI Agent Dream (and Subsequent Nightmare) # My first idea was to build a smart AI agent using n8n that could talk to ArXiv. I wanted an LLM that could actually search for papers, download them, and even read them.\nThe Docker Model Context Protocol (MCP) looked like the perfect tool for this. I found an arxiv-mcp-server on GitHub and my first task was just getting the thing to build.\nBuilding the ArXiv Server # blazickjp/arxiv-mcp-server A Model Context Protocol server for searching and analyzing arXiv papers Python 1861 134 I found the repo above, which had already configured an mcp server just as I wanted. However, when I attempted to get it running, I found the original Dockerfile was\u0026hellip; a bit much. I figured I could simplify it using uv, since its official base images are clean and come with it pre-installed.\nMy new Dockerfile was way simpler:\n# Use a single Python base image with \u0026#39;uv\u0026#39; pre-installed FROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim WORKDIR /app COPY . . # Install the project and all its deps RUN uv pip install . --system # Run the server ENTRYPOINT [\u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;arxiv_mcp_server\u0026#34;] The \u0026ldquo;Stateless\u0026rdquo; Problem # With the server built, I set up a custom catalog.yaml above to define my tools and hooked it all into docker-compose.yml. A custom catalog is essentially your own personal, self-contained list of MCP servers. Instead of relying on a public registry, you define everything about the servers you want to use in a single catalog.yaml file.\nThe process is straightforward:\nCreate catalog.yaml: You define one or more servers in a YAML file following the specified format. Mount the Catalog: In your docker-compose.yml file, you use a volume mount to make your local catalog.yaml file available inside the gateway container. volumes: - ./catalog.yaml:/mcp/catalog.yaml Tell the Gateway to Use It: You use command arguments to point the gateway to your mounted catalog file and specify which server(s) from that file you want to activate. command: - --catalog=/mcp/catalog.yaml - --servers=duckduckgo registry: arxiv-mcp-server: title: \u0026#34;ArXiv MCP Server\u0026#34; description: \u0026#34;An MCP server that enables AI assistants to search, download, and read papers from the arXiv research repository.\u0026#34; type: \u0026#34;server\u0026#34; image: \u0026#34;arxiv-mcp-server:latest\u0026#34; tools: - name: \u0026#34;search_papers\u0026#34; - name: \u0026#34;download_paper\u0026#34; - name: \u0026#34;list_papers\u0026#34; - name: \u0026#34;read_paper\u0026#34; - name: \u0026#34;deep-paper-analysis\u0026#34; env: - name: \u0026#34;ARXIV_STORAGE_PATH\u0026#34; value: \u0026#34;/data\u0026#34; volumes: - \u0026#34;/mnt/storage/media/docs:/data\u0026#34; docker-compose file\nservices: n8n: image: docker.n8n.io/n8nio/n8n hostname: n8n ports: - \u0026#34;5678:5678\u0026#34; environment: - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true - N8N_HOST=${SUBDOMAIN}.${DOMAIN_NAME} - N8N_PORT=5678 - N8N_PROTOCOL=http - N8N_RUNNERS_ENABLED=true - WEBHOOK_URL=https://${SUBDOMAIN}.${DOMAIN_NAME}/ - OLLAMA_HOST=${OLLAMA_HOST:-ollama:11434} - TZ=${GENERIC_TIMEZONE} volumes: - ${APPDATA}/n8n/storage:/home/node/.n8n - ${SHARED_FOLDER}:/files restart: always mcp-gateway: image: docker/mcp-gateway hostname: mcp-gateway ports: - \u0026#34;8811:8811\u0026#34; command: - --servers=arxiv-mcp-server - --catalog=/mcp/catalog.yaml - --transport=sse - --port=8811 volumes: - /var/run/docker.sock:/var/run/docker.sock - ./catalog.yaml:/mcp/catalog.yaml I then wired it up in n8n with an AI Agent node, first trying a local qwen3 model (way too slow) and then switching to gpt-4o-mini (much faster).\nIt worked! The agent called the search_papers tool.\nThen, I hit the first major wall. The agent would download_paper, and on the very next step, the read_paper tool would fail.\nThe Problem: The MCP gateway, by default, is stateless. It spins up a brand new container for every single tool call. The container that downloaded the paper was instantly destroyed, so the new container for read_paper had no idea the file existed. Bruh.\nThe \u0026ldquo;Static\u0026rdquo; Mode Saga # The fix was static=true mode. This tells the gateway to connect to an already-running server container. I dutifully refactored my docker-compose.yml to have mcp-gateway depend on a long-running mcp-arxiv-server.\nmcp-gateway: image: docker/mcp-gateway hostname: mcp-gateway ports: - \u0026#34;8811:8811\u0026#34; command: - --servers=arxiv-mcp-server,duckduckgo - --static=true - --transport=streaming - --port=8811 depends_on: - mcp-arxiv-server mcp-arxiv-server: image: mcp/arxiv-mcp-server entrypoint: [\u0026#34;/docker-mcp/misc/docker-mcp-bridge\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;arxiv_mcp_server\u0026#34;] init: true labels: - docker-mcp=true - docker-mcp-tool-type=mcp - docker-mcp-name=arxiv-mcp-server - docker-mcp-transport=stdio volumes: - type: image source: docker/mcp-gateway target: /docker-mcp - ${SHARED_FOLDER}:/app/papers It failed.\nI tried the exact same setup with the official duckduckgo server, and it worked perfectly. My ArXiv server? Nothing. Just network errors.\nI was losing my mind, so I filed a GitHub issue. A collaborator saved me. Turns out, the gateway auto-prefixes the server name with mcp- to find the service.\nMy service was named mcp-arxiv-server. The gateway was looking for mcp-mcp-arxiv-server.\nI renamed my service to arxiv-mcp-server (so the gateway would find it at mcp-arxiv-mcp-server) and just like that, it worked. The agent could finally search, download, and read papers in one session.\nPart 2: The New Task - Batch Processing 300 PDFs # Right after that win, I had a meeting with Lokman Belkit. We shifted gears. I now had a folder of 300 PDFs he\u0026rsquo;d sent me. The priority was no longer the live agent, but batch-processing this existing data.\nI needed a good PDF-to-Markdown converter. LlamaParse looked amazing, but you have to negotiate a license. No thanks. I settled on Docling because it was open-source and had a docling-serve image.\nConfiguring docling-serve # This took way longer than I expected. docling-serve is great, but it needs to download all its models before it can run. I ended up creating a two-stage setup in my docker-compose.yml:\ndocling-serve-initial: A service that runs once (restart: \u0026quot;no\u0026quot;) and just runs the download command, saving the models to a shared Docker volume. docling-serve: The main server. It depends_on the initial service, mounts that same volume, and reads the pre-downloaded models. services: # To download the required models before first run docling-serve-initial: image: ghcr.io/docling-project/docling-serve-cu126:main command: - docling-tools - models - download - --all volumes: - ${APPDATA}/n8n/docling_artifacts:/opt/app-root/src/.cache/docling/models restart: \u0026#34;no\u0026#34; # For document parsing docling-serve: image: ghcr.io/docling-project/docling-serve-cu126:main hostname: docling-serve ports: - \u0026#34;5001:5001\u0026#34; environment: DOCLING_SERVE_ENABLE_UI: \u0026#34;true\u0026#34; NVIDIA_VISIBLE_DEVICES: \u0026#34;all\u0026#34; DOCLING_SERVE_ARTIFACTS_PATH: \u0026#34;/models\u0026#34; DOCLING_SERVE_ENABLE_REMOTE_SERVICES: \u0026#34;true\u0026#34; DOCLING_SERVE_ALLOW_EXTERNAL_PLUGINS: \u0026#34;true\u0026#34; deploy: # This section is for compatibility with Swarm resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - ${APPDATA}/n8n/docling_artifacts:/models runtime: nvidia restart: always depends_on: docling-serve-initial: condition: service_completed_successfully First, I tried to get n8n to load all the papers from the arxiv-existing folder Lokman sent me. It contained 56 items, and it took a really long time to load. Next, I started poking around the docling API. Before unleashing it on all 56 papers, I tested it out with just two to see what would actually happen.\nI first tried all of the features that docling offers:\npipeline (str). The choice of which pipeline to use. Allowed values are standard and vlm. Defaults to standard. do_table_structure (bool): If enabled, the table structure will be extracted. Defaults to true. do_code_enrichment (bool): If enabled, perform OCR code enrichment. Defaults to false. do_formula_enrichment (bool): If enabled, perform formula OCR, return LaTeX code. Defaults to false. do_picture_classification (bool): If enabled, classify pictures in documents. Defaults to false. do_picture_description (bool): If enabled, describe pictures in documents. Defaults to false. However, it took a ridiculous amount of time and memory to run, and it was prone to crashing. After digging around, I found out that other people were hitting this same issue. The formula and code awareness models, while small, will apparently eat your entire GPU VRAM. So I had to turn off all the enrichments.\nPart 3: The n8n Orchestration Nightmare # This was my first time really using n8n for a complex batch job, and honestly, it was frustrating.\nProblem 1: The ZIP File docling-serve returns a ZIP file with paper.md and an artifacts/ folder. n8n\u0026rsquo;s \u0026ldquo;Decompress\u0026rdquo; node doesn\u0026rsquo;t output a nice array you can loop over. It\u0026rsquo;s\u0026hellip; not an array. It\u0026rsquo;s a single item with multiple binary properties. After digging through forums, I found you have to use a custom Code node to manually split it:\n// This code is required to split a single item with multiple binary files // into multiple items, each with one binary file. let results = []; for (item of items) { for (key of Object.keys(item.binary)) { results.push({ json: { fileName: item.binary[key].fileName }, binary: { data: item.binary[key] } }); } } return results; Problem 2: Nested Loops are Buggy My next instinct was one giant workflow:\nLoop over all 50 PDFs. (Nested) Call Docling API. (Nested) Get the ZIP, run the code above. (Nested) Loop over the resulting files to save them. This failed miserably. Data from the first paper\u0026rsquo;s loop would \u0026ldquo;bleed\u0026rdquo; into the second paper\u0026rsquo;s execution. It would just process the first paper over and over.\nThe Solution: Master/Child Workflows The only stable solution was to refactor:\nMaster Workflow: Its only job is to loop through the 50 PDFs and call a \u0026ldquo;Child\u0026rdquo; workflow once per paper. Child Workflow: Receives one paper, calls Docling, saves the files, and finishes. This isolated the execution and finally worked. I also created a merge node to filter out previously processed items, in case I wanted to run the workflow again in the future.\nThis process might look easy, but it took me several hours. At first, I didn\u0026rsquo;t know the Edit Fields node existed, so I was comparing the filenames directly, without realizing the file extensions were different (silly me). I had to modify the items in the output node, using an expression to cut the .md extension and replace it with .pdf just to get the filter to work.\nIn the end, it processed 39 papers in 11 minutes (with table enrichment on). A success\u0026hellip; but it felt way too complicated.\nPart 4: The Final Pivot - From n8n Hell to Python # The next step was extracting metadata from these new Markdown files.\nI briefly tried pdfVector because of its JSON schema feature. Then I saw the price: 2 credits per page. A 25-page paper would cost 75 credits. It was a complete non-starter. The results was actually good tho. At least I can utilize the JSON schema of it.\n{ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Title of the research paper\u0026#34; }, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Type of the paper\u0026#34;, \u0026#34;enum\u0026#34;: [ \u0026#34;method\u0026#34;, \u0026#34;benchmark\u0026#34;, \u0026#34;dataset\u0026#34;, \u0026#34;application\u0026#34;, \u0026#34;survey\u0026#34;, \u0026#34;other\u0026#34; ] }, \u0026#34;categories\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A list of the paper\u0026#39;s methodology.\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [ \u0026#34;Weakly Supervised\u0026#34;, \u0026#34;Semi Supervised\u0026#34;, \u0026#34;Training Free\u0026#34;, \u0026#34;Instruction Tuning\u0026#34;, \u0026#34;Unsupervised\u0026#34;, \u0026#34;Hybrid\u0026#34; ] } }, \u0026#34;github_link\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Link to the GitHub repository (if available)\u0026#34;, \u0026#34;nullable\u0026#34;: true }, \u0026#34;summary\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Description of the novelty of the paper\u0026#34; }, \u0026#34;benchmarks\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A list of benchmarks used in the paper\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [ \u0026#34;cuhk-avenue\u0026#34;, \u0026#34;shanghaitech\u0026#34;, \u0026#34;xd-violence\u0026#34;, \u0026#34;ubnormal\u0026#34;, \u0026#34;ucf-crime\u0026#34;, \u0026#34;ucsd-ped\u0026#34;, \u0026#34;other\u0026#34; ] }, \u0026#34;nullable\u0026#34;: true }, \u0026#34;authors\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A list of the paper\u0026#39;s authors\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Publication date of the paper (YYYY-MM-DD)\u0026#34; } }, \u0026#34;required\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;type\u0026#34;, \u0026#34;categories\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;summary\u0026#34; ], \u0026#34;additionalProperties\u0026#34;: false } So I decided to roll my own json extractor on n8n. I added an \u0026ldquo;Information Extractor\u0026rdquo; node to my n8n workflow, using the JSON schema I\u0026rsquo;d built. Again, the local model disappointed me with inferior results.\nLocal qwen3-4b: Failed to call the tool. OpenRouter gpt-4.1-nano: Worked perfectly. The final nail in the n8n-coffin came when I tried to set up my Hugo site. To make images work, Hugo needs a \u0026ldquo;Page Bundle\u0026rdquo; structure:\n- content/ ----papers/ -------\u0026lt;paper-name\u0026gt;/ -----------index.md -----------artifacts/ ---------------image.png Trying to make n8n create this dynamic directory (\u0026lt;paper-name\u0026gt;) and rename the file to index.md was a joke. I was writing crazy expressions, using Execute Command nodes, Edit Fields nodes\u0026hellip; it was just a mess.\nI was so frustrated with how n8n handles basic file and path manipulation that I just gave up on it for orchestration.\nThe Final Architecture: Python as the Orchestrator # I threw away the complex \u0026ldquo;Master\u0026rdquo; n8n workflow and replaced it with a single Python script.\nThis new hybrid architecture is the best of all worlds:\nPython (main.py) is the \u0026ldquo;Master.\u0026rdquo; It\u0026rsquo;s simple, I can debug it, and it handles file I/O perfectly. Docker runs my services (docling-serve, n8n, mcp-server). n8n is now just a simple \u0026ldquo;serverless function.\u0026rdquo; It\u0026rsquo;s a single webhook that receives a file path, extracts the JSON, and sends it back. Here\u0026rsquo;s the new flow:\nPython script loops through all PDFs. Calls docling-serve (running in Docker) to get the ZIP. Unzips the files locally into the correct Hugo Page Bundle structure (\u0026lt;paper-name\u0026gt;/index.md). Calls the n8n webhook with the absolute path to the new index.md. The n8n workflow reads the file, extracts the JSON using the LLM, and sends the JSON back as the webhook response. Python receives the JSON, formats it with ruamel.yaml, and writes it as front matter to the top of the index.md. The script moves the original PDF to a done folder. This journey was\u0026hellip; a lot. But the final system is clean, robust, and a perfect example of using the right tool for the right job—even if it takes a few frustrating detours to find it.\n","date":"2 November 2025","externalUrl":null,"permalink":"/posts/arxiv-n8n-docling/","section":"Posts","summary":"\u003cp\u003eThis is my story of how I attempt to build an AI-powered pipeline for ArXiv papers. It was a journey that started with a cool idea about AI agents and ended with me wrestling Docker, n8n, and Python into submission.\u003c/p\u003e","title":"Building an AI-Powered ArXiv Pipeline: Thought n8n was the future, but not yet","type":"posts"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/tags/mcp/","section":"Tags","summary":"","title":"Mcp","type":"tags"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/tags/n8n/","section":"Tags","summary":"","title":"N8n","type":"tags"},{"content":"In the previous parts of this series, we went from a single hand-me-down PC to a full 3-node cluster, and then we installed Proxmox as our hypervisor. We even managed to pass through a GPU to a VM for near-native performance.\nThis time, we\u0026rsquo;re diving into one of the most critical parts of any homelab: storage.\nIn fact, many people (myself included) start this homelab journey just because we need a platform to store data other than Google Drive as we creep toward that 15GB limit. Now, imagine having your own private Google Drive at home, but with 10s of terabytes.\nWe will be virtualizing TrueNAS, a world-class, open-source NAS (Network Attached Storage) platform, right inside Proxmox. We\u0026rsquo;ll pass through our physical hard drives, set up a ZFS storage pool, and create network shares (like NFS and SMB) that our entire homelab will use.\nSo, what is a NAS # But first, to understand what a NAS is, you have to consider the core problem it solves.\nImagine you\u0026rsquo;re a video editor, working on your local machine with 10GB of footage. It\u0026rsquo;s going smoothly\u0026hellip; as long as you\u0026rsquo;re working alone.\nBut what happens when the workload is heavy and you need to collaborate with other editors? Are you going to wait for someone to finish, copy all 10GB of files onto a USB drive, and walk it over to them? This could take an hour. Then you do your small part, and you have to transfer it back for another hour. This painful, time-wasting process is a complete nightmare. Not to mention, if someone\u0026rsquo;s computer has problems and their files are gone, how can you restore them?\nThis is where a NAS comes to the rescue. A NAS device is essentially a smart server full of hard drives that plugs directly into your network. Instead of everyone having their own 10GB copy, the single, master copy of the project lives on the NAS. Now, everyone on the team can connect to the NAS over the fast local network at the same time. It appears on their computer just like another hard drive.\nYou, the other editor, and the animator can all read and write to the same central files simultaneously. There\u0026rsquo;s no more copying, no more hour-long USB transfers, and no more confusion about \u0026ldquo;Which file is the latest version?\u0026rdquo; No more worrying about lost files if your computer has problems, as long as the central NAS is still active. Now, in our homelab context, replace \u0026ldquo;editors\u0026rdquo; with \u0026ldquo;VMs\u0026rdquo; or \u0026ldquo;PCs\u0026rdquo;. The number isn\u0026rsquo;t three, but maybe 10 or 100. You will save a bunch of time by having a NAS so your VMs can access data efficiently, without relying on their own local storage in case one of them fails.\nWhich NAS platform to choose # I used to be a Synology user, running Xpenology to get the DSM (Disk Station Manager) platform on my non-Synology hardware. After using it for a while, although the software support is top-notch and very user-friendly, I migrated to TrueNAS because I wanted deeper control and customization. The OS TrueNAS provides also feels lighter to me and seems to be more focused and specialized as a storage platform. Synology, on the other hand, seems more like an All-in-One platform, encompassing a hypervisor and integrated apps, which feels a bit bloated for my needs.\nOne more thing: TrueNAS officially supports ZFS and Synology officially supports BTRFS as a file system. From what I understand, BTRFS is for flexibility and less demanding roles, while ZFS is for maximum reliability and data integrity, especially in servers. I have to admit that I don\u0026rsquo;t use all the advanced features of either, but with the correct configuration (log, cache, etc.), ZFS can outperform BTRFS in both speed and scale.\nVirtualize NAS \u0026hellip; why not? # Preface: For best practices, and if you have the money, get yourself a dedicated NAS device like a Synology.\nIn theory, dedicating a machine to be a NAS means I shouldn\u0026rsquo;t run other compute resources on it, as it breaks the \u0026ldquo;specialized tool\u0026rdquo; idea I mentioned before. You should have a separate Synology or custom NAS build, separate from your compute resources like Proxmox.\nAt first, I thought about using one of the other machines (HP Prodesk or Dell Optiplex) as the main NAS device, without Proxmox installed. But a NAS requires lots of HDD slots; otherwise, what\u0026rsquo;s the point? I quickly realized those SFF computers only have one or two HDD slots. The only computer I have that allows multiple slots is the Lenovo P520.\nBut the Lenovo P520 is the strongest of all; sacrificing it only to store data would be so wasteful. Since I\u0026rsquo;m still a student, I don\u0026rsquo;t want to spend more money and wanted to consider an alternative.\nLuckily, with Proxmox, I can virtualize the NAS platform in a VM and pass through the HDDs directly to it. This isn\u0026rsquo;t \u0026ldquo;best practice\u0026rdquo; because it creates a single point of failure: if the Lenovo P520 is in maintenance mode, everything else loses access to storage. I only chose this because it allows me to use the PC\u0026rsquo;s multiple HDD slots while still running other compute resources, all without breaking my bank. With proper HDD passthrough, we give the TrueNAS VM direct, bare-metal control of the drives. The performance is basically native, and it\u0026rsquo;s been rock-solid for me.\nThis is almost the same process we used for the GPU. We need to tell Proxmox, \u0026ldquo;Don\u0026rsquo;t touch these drives; give them directly to this VM.\nFirst, we need the unique IDs for our drives. Don\u0026rsquo;t use /dev/sda or /dev/sdb, as those can change on reboot. We need the permanent \u0026ldquo;by-id\u0026rdquo; path. Hop on to the pve/Disks tab to see the information and take note of the disk serial numbers you want to pass through. Then, in the shell, run:\nls -l /dev/disk/by-id/ Look for the ata- names that match the serial numbers of your whole disks (e.g., ata-WDCWD10EZEX-00WN4A0...). You want the main disk ID, not the ...-part1 or ...-part2 names.\nOnce you have your disk IDs, make sure your TrueNAS VM is shut down. Then, run this command for each disk:\n# qm set \u0026lt;VMID\u0026gt; --\u0026lt;controller\u0026gt; \u0026lt;controllernumber\u0026gt; /dev/disk/by-id/\u0026lt;your-disk-id\u0026gt; # Example for the first disk: qm set 100 --scsi1 /dev/disk/by-id/ata-WDCWD10EZEX-00WN4A0WD-WCC6Y... # Example for the second disk: qm set 100 --scsi2 /dev/disk/by-id/ata-WDCWD10EZEX-00WN4A0WD-WCC6Z... Repeat this for all your drives, incrementing the scsi number. Now, when you boot your TrueNAS VM, it will see these disks as if they were physically attached. I decided to reserve only my 4TB + 1TB HDD drives for TrueNAS; the remaining HDD is used for the Proxmox OS itself.\nRAID Theory (but I use Stripe, btw) # Preface: Again, for best practice, don\u0026rsquo;t be like me.\nAfter you\u0026rsquo;ve passed through the drives and booted TrueNAS, you\u0026rsquo;ll go to Storage \u0026gt; Pools \u0026gt; Create Pool. This is where you make the most important decision: how to combine your drives.\nRAID (Redundant Array of Independent Disks) combines multiple drives into one logical unit. ZFS uses its own software-based RAID, with a few main types:\nStripe (RAID 0): This is for max speed and max risk. It \u0026ldquo;stripes\u0026rdquo; data across all drives.\nPros: Super-fast reads/writes. You get 100% of the storage (e.g., 4TB + 1TB = 5TB).\nCons: If one drive fails, all data on all drives is gone. Zero redundancy.\nMirror (RAID 1): This is for max safety. It\u0026rsquo;s a 1-to-1 clone (e.g., two 4TB drives mirrored = 4TB usable).\nPros: If one drive fails, your data is 100% safe on the other.\nCons: You only get 50% of your total storage.\nRAIDZ1 (RAID 5): The classic balance. It stripes data and writes a \u0026ldquo;parity\u0026rdquo; block, which can be used to rebuild a lost drive.\nPros: Good performance and safety. You can lose one drive.\nCons: You lose the capacity of one drive to parity (e.g., three 4TB drives = 8TB usable).\nRAIDZ2 (RAID 6): Same as above, but with two parity blocks. You can lose two drives.\nSo you might guess I should be choosing Mirror or RAIDZ1, right? Well, I chose Stripe.\nI know what you\u0026rsquo;re thinking: \u0026ldquo;That\u0026rsquo;s insane! You have no redundancy!\u0026rdquo; And you\u0026rsquo;re right. I\u0026rsquo;m fearless, and I honestly don\u0026rsquo;t care about a hardware failure.\nMy philosophy is that my data is the most important thing, not the hardware. I already have a 5TB Microsoft 365 (OneDrive) plan that backs up 100% of my critical data. If a drive in my Stripe pool fails, who cares? The data itself is safe in the cloud. I\u0026rsquo;ll just buy a new drive, create a new pool, and restore from my backup.\nThis is my personal trade-off. I lose time (it will be a pain to wait for restoration). But I gain money and speed, as I don\u0026rsquo;t have to buy extra HDDs just for parity, and I get 100% of my storage capacity and maximum performance. But seriously, do not do this in production.\nSo, I went ahead and created a Stripe pool that combines my 4TB and 1TB drives. Additionally, I also created a 128GB SLOG by passing through an NVMe drive (a similar process to the GPU, as it\u0026rsquo;s a PCI device). Since I\u0026rsquo;m going to use this NAS as file storage for other VMs and containers, I need a SLOG to improve performance for synchronous writes by storing them in a fast log before they\u0026rsquo;re written to the slower pool drives. You don\u0026rsquo;t need this if you only use your NAS for storing media.\nNext, after creating a pool (in my case, storage), we need to create datasets for better isolation. For example, I\u0026rsquo;ll use TrueNAS for:\nappdata: storing application data for containers\niso: storing PS4, Switch, and Windows games\nmedia: storing media files for Jellyfin, manga for Suwayomi, etc.\nproxmox: storing backups, ISO images, and CT templates for Proxmox\nminio: storing S3 object storage\nTherefore, I\u0026rsquo;ll create these 5 different datasets in my pool.\nFile sharing # Now that we have our pool and datasets, the next step is to share them over the network. There are three file-sharing protocols I currently use: SMB, NFS, and FTP.\nNFS for appdata # As I mentioned in the previous part, I primarily use TrueNAS to store persistent data for my stateful applications (containers). This way, even if I delete and recreate a container, it still functions the same as before.\nNFS is the native file-sharing protocol for Linux/Unix environments. Since all my containers and Proxmox are Linux-based, it\u0026rsquo;s the best tool for the job.\nFirst, set up NFS on TrueNAS via the Shares tab. The main thing is to enable NFSv4 support. Next, for the appdata dataset share, go to Advanced Options and change the Mapall User and Mapall Group to root.\nThis maps any request from a non-root user (e.g., user 1000 inside a container) to the root user on TrueNAS. This ensures that when a container tries to write data, TrueNAS grants it the required permissions. Don\u0026rsquo;t do this in production, but in a homelab, you can mitigate the risk by limiting the client IPs that can connect (in the Hosts section).\nAfter configuring TrueNAS, we SSH into our Docker VM to mount this NFS storage on boot.\n# Install package for nfs client sudo apt update \u0026amp;\u0026amp; sudo apt install nfs-common # Create a mount point in local directory sudo mkdir -p /mnt/storage/appdata # Open the configuration file for drive mount sudo vim /etc/fstab Go to the bottom of the file and add a new line, like this example:\n10.69.1.102:/mnt/storage/appdata /mnt/storage/appdata nfs defaults,_netdev,nofail 0 0 Notice these three flags after the nfs block:\ndefaults: Uses standard options (like rw for read-write and auto to mount at boot). _netdev: This tells your system \u0026ldquo;this is a network device, wait for the network to be online before trying to mount it.\u0026rdquo; This prevents your system from hanging at boot. nofail: This tells your system \u0026ldquo;if you can\u0026rsquo;t mount this for any reason (e.g., the NFS server is offline), don\u0026rsquo;t hang the boot process. Just skip it and continue.\u0026rdquo; After that, run these commands to reload the config and mount the share:\nsudo systemctl daemon-reload sudo systemctl restart remote-fs.target After verifying the mount, we tell Docker to use it as a local volume by creating an APPDATA environment variable and referencing it in our docker-compose.yml files.\nNFS for proxmox # I use this exact same NFS protocol for the proxmox dataset, which I add as a storage target in the Proxmox datacenter. This is especially useful for a Proxmox cluster: once you set it up on one node, the storage is shared with all nodes. This allows for instant live migration of VMs, as you\u0026rsquo;re only moving the RAM, not the disk.\nHere\u0026rsquo;s the example demo of live migration VMs on Proxmox using shared storage from TrueNAS.\nSmb for laptop, phone clients # If you don\u0026rsquo;t want to install an app like Jellyfin, you can still watch your movies by using SMB to mount the media dataset directly in your laptop\u0026rsquo;s file browser. You can open, edit, and delete files as if it were a local folder. This is the same protocol the video editors in my earlier example would use.\nBelow is a showcase where I can watch a movie on both my laptop and phone by just opening the file, no download needed.\nFTP for edge devices like PS4 and Nintendo Switch # For devices that don\u0026rsquo;t support NFS or SMB (like my hacked PS4 and Nintendo Switch), no problem. TrueNAS also offers the FTP protocol. I use it to install games on my console. I can easily download a game via qbittorrent (running on the NAS) and then connect the device to my FTP server to transfer the file.\nAdditional quality-of-life applications # Remember when I said it\u0026rsquo;s \u0026ldquo;best practice\u0026rdquo; to separate compute from storage? Well, your life will be much easier if you install a few lightweight, storage-related applications on your NAS.\nDownload clients # Firstly, aria2 is a lightweight, multi-protocol, cross-platform download utility that operates on the command line. When bundled with a web UI like aria2-ng, it makes downloading to your NAS so much easier. There are browser extensions that can even capture download links and send them directly to your aria2 instance, automating file downloads right onto your NAS.\nSecondly, if you\u0026rsquo;ve been \u0026ldquo;sailing the seas\u0026rdquo; for a long time, you\u0026rsquo;ve definitely heard of uTorrent, which is a client to download\u0026hellip; \u0026ldquo;Linux ISOs\u0026rdquo;\u0026hellip; from a decentralized peer-to-peer network. However, uTorrent only provides a desktop application and is full of ads, so the community has switched to qbittorrent. It has a great web interface and supports being deployed as a Docker image, so I\u0026rsquo;ve been using it to download my \u0026ldquo;Linux ISOs\u0026rdquo; for a long time now.\nS3-compatible storage # I\u0026rsquo;ve just started playing around with this recently, but I can already see its tremendous potential. If you don\u0026rsquo;t know, S3 is a cloud storage service from AWS that allows you to store data as objects (a file plus metadata) organized into \u0026ldquo;buckets.\u0026rdquo; It\u0026rsquo;s used for a wide range of applications, including backups, data lakes, and hosting static websites.\nWith the recent AWS outage, for anyone wanting to migrate to an alternative, self-hosted object storage platform, I recommend Minio. It\u0026rsquo;s open-source and fully S3-compatible. I\u0026rsquo;m definitely going to use this for my Terraform remote state, which I\u0026rsquo;ll introduce in the next part.\nAnd that\u0026rsquo;s all the applications I\u0026rsquo;ve installed on my TrueNAS server. As you can see, there aren\u0026rsquo;t many, and they consume very little RAM, but they bring so much quality-of-life value.\nJust so you know, if you want to deploy any app, TrueNAS also has a dedicated interface in the \u0026ldquo;Apps\u0026rdquo; tab. You\u0026rsquo;ll likely just need to configure a dataset as an \u0026ldquo;app\u0026rdquo; type and set the user/group for the application to 568, which is the default ID for the built-in \u0026ldquo;apps\u0026rdquo; user (yours might look different).\nBackup # This is critical. Who knows when your homelab will suddenly die? (wink, wink).\nThis section even deserves its own article. I\u0026rsquo;ve switched between many backup solutions in the past: from only using Proxmox and rclone-to-OneDrive, to using Restic inside a Docker VM, and finally to using the integrated TrueNAS data protection features. I\u0026rsquo;m quite satisfied with the current setup, as it offers minimal restore time, minimal storage overhead, and easy progress tracking to see if anything went wrong.\nHere\u0026rsquo;s my strategy:\n1. Proxmox Backups First, for Proxmox, I enabled the integrated backup feature. I have it scheduled to store backups of all my VMs and LXCs directly to the proxmox dataset (via NFS) on the TrueNAS server we just set up.\n2. Application Data Backups Secondly, for my application data, I don\u0026rsquo;t need a separate tool like Restic. Since I designed all my containers to store their persistent data directly on the appdata NFS share, the data is already centralized on the NAS. Backing up the NAS is backing up the apps.\n3. Local Snapshots (The First Copy) Thirdly, now that I have all my centralized data (Proxmox backups and appdata) on TrueNAS, I enable snapshots. According to the 3-2-1 backup rule, you need at least 3 copies of your data. TrueNAS helps with this: it has the ability to configure automatic snapshots every night and allows you to restore even a single file or subfolder from those snapshots easily.\n4. Off-site Backup (The \u0026ldquo;3-2-1\u0026rdquo; Completion) Finally, the 3-2-1 rule states you need 3 copies, on 2 different media types, with 1 copy off-site.\nAgain, I don\u0026rsquo;t have the budget for a second on-site backup machine, so I\u0026rsquo;m relying on my 1 off-site copy. I\u0026rsquo;ve had a 5TB OneDrive plan (for free!) for a long time, so I\u0026rsquo;ve been using it as my primary off-site backup, and I still haven\u0026rsquo;t had any issues with it, so yeah.\nTrueNAS has also made configuring rclone to connect with OneDrive much easier through its web interface. No need to remember all the quirky command-line flags anymore; you just need to toggle settings and click a few buttons to have it all configured. You can also view logs if a task fails and get notified via email immediately.\nSummary # And that\u0026rsquo;s it! We now have a rock-solid, (mostly) independent, and high-performance NAS running as a VM inside Proxmox.\nWe passed through our HDDs for bare-metal performance, explored the trade-offs of RAID (and my own \u0026ldquo;fearless\u0026rdquo; Stripe setup), and created SMB and NFS shares for every device in our lab.\nThis NFS share is the final, critical piece of our infrastructure. We have prepared for next part where we create our Kubernetes cluster. With this design, we won\u0026rsquo;t store any persistent data inside the cluster\u0026rsquo;s VMs. We\u0026rsquo;ll tell them to store everything on our new TrueNAS server. If a Kubernetes VM dies, it\u0026rsquo;s no big deal - we just spin up another, connect it to the NFS share, and all our application data is still right where we left it.\n","date":"2 November 2025","externalUrl":null,"permalink":"/posts/on-premise-storage/","section":"Posts","summary":"\u003cp\u003eIn the previous parts of this series, we went from a single hand-me-down PC to a full 3-node cluster, and then we installed Proxmox as our hypervisor. We even managed to pass through a GPU to a VM for near-native performance.\u003c/p\u003e","title":"On-Premise 101 (Part 3): My \"Fearless\" NAS Build with Virtualized TrueNAS, ZFS, and Cloud Backups","type":"posts"},{"content":"","date":"2 November 2025","externalUrl":"https://github.com/phuchoang2603/sis-arxiv-vad-papers","permalink":"/projects/sis-arxiv-vad-papers/","section":"Projects","summary":"This project is a comprehensive platform for managing, processing, and displaying ArXiv research papers. It combines a Hugo static site with a powerful backend of containerized services for AI-driven PDF processing, metadata extraction, and ArXiv interaction.","title":"SIS ArXiv VAD Papers","type":"projects"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/tags/truenas/","section":"Tags","summary":"","title":"Truenas","type":"tags"},{"content":"In the previous part, we covered how I started playing around with my hand-me-down computer and then escalated to building a whole 3-node cluster. It\u0026rsquo;s been a 3-year journey, and there have been so many changes to the software stack I host. But no matter what, there\u0026rsquo;s one thing I\u0026rsquo;ve used consistently from beginning to end: Proxmox (Hypervisor). So before we dive into how I deploy my applications on Docker or Kubernetes, I\u0026rsquo;ll show you the glue that connects the hardware we just built to all the software we\u0026rsquo;re going to deploy.\nWhy Proxmox? The Magic of Type 1 Hypervisors # Many of you have probably heard the term Virtual Machines (VMs), the ones that programs like VirtualBox and VMWare create. Those programs, which are often called a Type 2 Hypervisor, let you basically run a whole Guest OS (i.e., Windows, Linux, \u0026hellip;) on top of your current one (which is called the Host OS). Because it doesn\u0026rsquo;t actually use the machine\u0026rsquo;s hardware resources directly - it goes through a translation layer in the Host OS - it lets you do anything inside without worrying about the machine being compromised. Therefore, it\u0026rsquo;s frequently used for testing out dangerous programs like Honorlock online exam proctoring (oops, don\u0026rsquo;t arrest me prof Eggers). It runs fine with basic, lightweight programs, sure. But if you want to push it harder to run heavier ones, it might not utilize all the potential performance the machine has.\nThat\u0026rsquo;s where the Type 1 Hypervisor comes in. It can still create Virtual machines like above. But right now, notice it no longer has the Host OS between it and the Hardware resources. That means it can now directly access the full performance of the machine, without relying on the Host OS\u0026rsquo;s translation layer, making the Virtual machine running on top of it much more efficient. Maybe you don\u0026rsquo;t know this, but when you enable Windows Subsystem for Linux 2 (WSL2), Windows re-configures itself to run on top of its own Type 1 Hypervisor called Hyper-V. WSL2 then runs as a separate, lightweight VM directly on that same hypervisor, side-by-side with Windows, providing full system call compatibility. Other examples include VMWare ESX or ESXi, and notably, Proxmox.\nProxmox is an open-source virtualization management solution that uses KVM to manage virtual machines and LXC to manage containers. It is a Type 1 Hypervisor that built on a Debian-based Linux distribution, and is managed through a central web interface for tasks like clustering, high availability, storage, and networking.\nSo that explains Proxmox. For me, the biggest benefit of using Proxmox was to easily pass through computer resources to bypass even the Hypervisor layer itself, eliminating significant overhead and allowing the VM to receive near-native hardware performance. This allows me to pass through a GPU into a VM to run inference on LLM models at the highest VRAM usage and computation; or pass through HDDs to a VM to use as a NAS (more on this in the next part). Additionally, as mentioned above, you get a beautiful web interface to manage the VMs and other useful features, so it has the potential to replace even Kubernetes or TrueNAS. However, I prefer specialization over convenience. I\u0026rsquo;d rather use a dedicated tool that does one job perfectly than a single, all-in-one tool that does multiple jobs just \u0026lsquo;okay\u0026rsquo;.\nTo demonstrate the ability of Proxmox, I will show you a guide to create a VM that is capable of directly inheriting a Nvidia GPU from the host machine, allowing the applications running inside the VM to utilize its fullest performance. However, note that this is just one of many examples you can do with Proxmox. In the next part, I will show you how to even virtualize a NAS device on it, or provision multiple VMs in one command.\nGetting Started: My Proxmox Post-Install Toolkit # I\u0026rsquo;m not going to cover the in-depth guide to install Proxmox as it\u0026rsquo;s similar to installing any other OS: just get the ISO file, burn it to a USB, turn off the computer, boot from the USB, and follow the installation guide (the default options are pretty solid). The settings they ask you to configure from the beginning can also be changed later after installation. So no need to be afraid of anything.\nNext, I will show you commands that I actually use after installing Proxmox, as I believe they are really useful.\nThis script provides options for managing Proxmox repositories, including disabling the Enterprise Repo, adding or correcting PVE sources, enabling the No-Subscription Repo, adding the test Repo, disabling the subscription nag, updating Proxmox VE, and rebooting the system.\nbash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/community-scripts/`Proxmox`VE/main/tools/pve/post-pve-install.sh)\u0026#34; To create a new Proxmox VE Ubuntu 24.04 VM, run the command below in the Proxmox Shell. Note that if you want to use pass-through features like a GPU, you must select q35 as the machine type, host as the CPU type, and make sure not to start the machine after installation. In the cloud-init tab, change the username, password, network address, and SSH public key so you can SSH into it later.\nbash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/community-scripts/`Proxmox`VE/main/vm/ubuntu2404-vm.sh)\u0026#34; This script is fantastic because it creates a cloud-init template, which is much faster than manually installing from an ISO. It lets you pre-configure your username, password, and networking, so the VM is ready to go in seconds. It\u0026rsquo;s the perfect way to spin up new VM s quickly, especially when you plan to make more for a Kubernetes cluster later. If you want more useful scripts like this, feel free to hop over to here.\nPassing a GPU to a VM # Firstly, we don\u0026rsquo;t want the Proxmox host system utilizing our GPU(s), so we need to blacklist the drivers. I\u0026rsquo;m not gonna provide the whole full guide, as it\u0026rsquo;s a really long process and requires you to know your system to adjust accordingly. But here\u0026rsquo;s the ultimate guide that provides detailed explanations and steps for doing so.\nSecondly, after we verify the GPU was passed through successfully, we need to set up the VM to receive it:\nAfter Ubuntu has finished installing and is reachable by SSH on your network, shut down the `VM and go to the \u0026ldquo;Hardware\u0026rdquo; tab. Click \u0026ldquo;Add\u0026rdquo; \u0026gt; \u0026ldquo;PCI Device\u0026rdquo;. Select \u0026ldquo;Raw Device\u0026rdquo; and find your GPU. Click the \u0026ldquo;Advanced\u0026rdquo; checkbox, \u0026ldquo;All Functions\u0026rdquo; checkbox, and \u0026ldquo;PCI-Express\u0026rdquo; checkbox, then hit Add. Start the VM again and type lspci in the console. Search for your GPU. If you see it, you\u0026rsquo;re good to go! Thirdly, we need to install the drivers inside the VM so that the software can utilize the GPU correctly. There are multiple ways to do this, but I always refer to this site to install.\nsudo apt update \u0026amp;\u0026amp; sudo apt install ubuntu-drivers-common # List drivers sudo ubuntu-drivers list --gpgpu # Let’s assume we want to install the `570-server` driver (listed as `nvidia-driver-570-server`): sudo ubuntu-drivers install --gpgpu nvidia:570-server #You will also want to install the following additional components: sudo apt install nvidia-utils-570-server # After that, reboot your system sudo reboot # Once the machine is back up, check to be sure your drivers are functioning properly nvidia-smi Installing Docker (And Why I Use a VM, Not LXC) # Note that Proxmox also has LXC, as an alternative to Docker. While LXC offers better performance, the software support and ease of use aren\u0026rsquo;t as good as Docker. Not to mention it is harder to pass through a full physical device (like a GPU) to an LXC container, because by design, it\u0026rsquo;s tightly integrated with the host kernel.\nTherefore, I only use LXC for critical software that needs to stay active alongside the machine, such as Tailscale or Cloudflared. For most other things, I use a VM with Docker to experiment and test. If you want to install Docker and Nvidia Container Toolkit on the VM to help containers leverage your GPU, you\u0026rsquo;ll want to run these commands:\n# Add Docker\u0026#39;s repostiory to apt sources sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \\ \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null ## Add Nvidia Container Toolkit\u0026#39;s repostiory to apt sources curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list ## Install sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin sudo apt-get install -y nvidia-container-toolkit # Use docker without sudo, need to log out then back in to apply this sudo usermod -aG docker $USER # Configure the container runtime by using the nvidia-ctk command and restart docker sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker After successful installation, this will test to make sure that the NVIDIA container toolkit can access the GPU correctly.\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi # You should see the same output as running nvidia-smi without Docker. Summary \u0026amp; What\u0026rsquo;s Next: Prepping for TrueNAS # And that finishes the whole step of setting up a VM on top of a Type 1 Hypervisor to use an Nvidia GPU with near-native performance. Now you can go ahead and install any Docker containers you want.\nBut I\u0026rsquo;m not doing that right away, because I want those containers to use NFS storage. That way, even if the VM stops working correctly, I can still spin up a new one without losing my persistent application data. This approach is also helpful if you want to migrate from Docker to Kubernetes, as it allows you to reuse the data by just pointing to the correct NFS path. To do this, we have to set up TrueNAS, a network-attached storage platform.\n","date":"30 October 2025","externalUrl":null,"permalink":"/posts/on-premise-hypervisor/","section":"Posts","summary":"\u003cp\u003eIn the previous part, we covered how I started playing around with my hand-me-down computer and then escalated to building a whole 3-node cluster. It\u0026rsquo;s been a 3-year journey, and there have been so many changes to the software stack I host. But no matter what, there\u0026rsquo;s one thing I\u0026rsquo;ve used consistently from beginning to end: \u003ccode\u003eProxmox\u003c/code\u003e (Hypervisor). So before we dive into how I deploy my applications on Docker or Kubernetes, I\u0026rsquo;ll show you the glue that connects the hardware we just built to all the software we\u0026rsquo;re going to deploy.\u003c/p\u003e","title":"On-Premise 101 (Part 2): Proxmox, the \"Glue\" for My Homelab","type":"posts"},{"content":"You might have heard about the recent AWS outage that caused many services to go down. To think that half of the internet relies on a single service is crazy, even if it might be the best in its field. The engineers at AWS have done many things to prevent this, such as high availability, multi-region, zones, etc., but if a critical part of it still goes down, the whole thing goes down too. Therefore, if you don\u0026rsquo;t want to give all your money and trust to a single company, you might want to explore the on-premise option, which is self-hosting.\nBut deploying on-premise isn\u0026rsquo;t that easy, though. You have to spend time, effort, and money to configure the system for yourself—from PC specs, network performance, and security, to virtualization. All of that is not a one-day setup. But it also gives you the power to manage everything yourself, too.\nSo if you want to start exploring and build an infrastructure like the one I\u0026rsquo;m currently hosting, here\u0026rsquo;s the guide for you to do so. This is the first part of the series On-Premise 101. In this part, I\u0026rsquo;ll share my whole journey from the beginning: how I acquired the hardware, the decisions I made, and finally, how I wired it all together with networking to get it ready for the software.\nIn this part, you\u0026rsquo;ll see me refer to Proxmox a lot and might ask, \u0026ldquo;What the hell is it?\u0026rdquo; For now, just think of it as the platform that allows me to create virtual machines. I will cover it in-depth in the next part.\nThe Beginning: My First SFF Server and Its Limits # Looking back, I started this journey 3 years ago. At the time, I had an HP Prodesk 600 G4 SFF that my mother gave me; she picked it up from the warehouse at her office. It was an old computer, so I didn\u0026rsquo;t know what to do with it at the time, since I already had a PC that I could do anything with. But when I was browsing my YouTube feed, I saw a video talking about how a creator used his Synology NAS (Network Attached Storage) to replace Google Drive and help him collaborate with his video editors easily.\nI thought it was so cool that you could transfer a file that big from one computer to another, without any wires, almost instantly. I checked the specs of the NAS and found out it was nothing powerful—just a bunch of HDDs and storage. That\u0026rsquo;s when I realized I could probably do all of that with my old computer, too.\nIf you want to read what I was doing with my old computer at that time, feel free to read my initial self-hosting story.\nThe specs of my PC weren\u0026rsquo;t really powerful, but its iGPU was capable enough to run Frigate (an NVR camera detection service) and Jellyfin (for video transcoding) pretty well. Later on, I tried to upgrade the PC specs , such as plugging in a graphics card for LLM models and upgrading the RAM to 16GB. But since it was a Small-Form-Factor (SFF) PC, it had limited upgrade options.\nFirst, I tried plugging in a GTX 1030, and although it worked, I had to remove the side panel to make room for it. Second, I tried to modify the HDD holder to fit two SATA HDDs in the case, but after running it for a while, I noticed it got really hot because the two HDDs were stuck so close together. So, I was tempted to look for another PC.\nLesson Learned: Small Form Factor (SFF) PCs are a great, cheap way to start, but their limited space for upgrades (especially for GPUs and storage) means you might outgrow them fast.\u0026quot;\nThe First Big Upgrade: The Lenovo P520 Workstation # After hitting the bottlenecks of my computer and having to move to the US for my undergrad studies, I had to leave my entire homelab back in Vietnam. I could still access it via Tailscale (a VPN service, which I\u0026rsquo;ll share more about later), but the latency was really high, so I also wanted to buy a new one. Actually, at that time, ChatGPT and LLMs were also getting popular, so I wanted to get a GPU to help me \u0026ldquo;study\u0026rdquo; more in that field. Since I\u0026rsquo;m also a student, I set my budget to under $500 total.\nBrowsing through eBay, YouTube, and Reddit for suggestions, I settled on the Lenovo P520 as the PC. Initially, I wanted to build my own PC, but I realized that if I bought all new hardware, it would cost much more than $500, and the specs wouldn\u0026rsquo;t be powerful enough. So I wanted to choose pre-built enterprise hardware, similar to my HP Prodesk 600 G4 above. Instead of an SFF, I wanted a Full-Tower this time so that I could upgrade it more easily later on. There were options like the Dell Optiplex, HP Prodesk, HP Elitedesk, etc., but I decided to choose Lenovo because of its price-to-performance ratio. My favorite YouTuber, Hardware Haven, also released a video on this PC.\nWith only $274, I got:\nCPU: Intel Xeon W-2123 (4 cores, 8 threads, 3.60 GHz)\nRAM: 32 GB of 2666 MHz ECC DDR4\nStorage: 1TB SATA SSD and 4TB HDD\nAnd a bunch of ports and upgrade options on the motherboard\nAs for the GPU, there were also lots of options to choose from, but I felt the Nvidia RTX 3060 and Nvidia Tesla P40 were the strongest candidates. On one hand, the Nvidia Tesla P40 attracted me because it has 24GB of VRAM, and as you may know, VRAM capacity is king in the LLM world. But I was afraid the outdated software support would cost me in the future since it\u0026rsquo;s a card that was released nearly 10 years ago and uses the Pascal architecture. The RTX 3060, on the other hand, has 12GB of VRAM, was launched recently, and has active CUDA support, although I can\u0026rsquo;t run some of the largest LLM models on it. But with a price of only $200, and considering the fact that I might occasionally game on it, I decided to settle on the 3060.\nI also upgraded it to 64GB of ECC RAM with two purchases of 16GB sticks on eBay, because I also planned to use this machine as a NAS server (TrueNAS loves ECC RAM !). Later in the year, I also 3D-printed an HDD bay (the green part in the bottom right corner of the image below). This allowed me to install two additional 1TB HDDs, bringing the total NAS storage to 6TB.\nThe \u0026ldquo;Free PC\u0026rdquo; and My Dream of a 3-Node Cluster # A year later, which is just recently, I also received a new PC (notice I said \u0026ldquo;received\u0026rdquo;). The story of how I got this PC is really surprising. In the summer of 2025, I needed to go back to Vietnam to visit my family. Since it was a short trip, I decided not to bring the Lenovo P520 with me because it was really clunky, and at home, I still had the HP Prodesk 600 G4. But that summer was also the first time I learned Kubernetes and I really wanted to experiment with it. So I ran those experiments on the HP Prodesk 600 G4 with multiple virtual machines, and planned to upgrade to multiple physical nodes.\nWhen the summer ended, I planned to bring the HP Prodesk 600 G4 with me to the US. But unfortunately, after the flight, my machine would no longer boot up. Since I didn\u0026rsquo;t bring a monitor or any cables with me, I couldn\u0026rsquo;t troubleshoot the issue myself. I decided to bring it to the IT team at my school to hopefully fix it. After some back-and-forth discussion, they reached the conclusion that my machine had faulty RAM issues, and they only needed to plug out two 4 GB RAM sticks to boot it up normally.\nWhen I went to their office to pick up my device, I saw a bunch of old Dell Optiplex machines lying around, so I just asked what they planned to do with them. They answered that they were replacing them with newer devices for the library and planned to trash or give them away. I asked if I could have one for \u0026ldquo;study reasons,\u0026rdquo; and they happily gave me a Dell Optiplex 7020, and even threw in a Dell monitor. That\u0026rsquo;s the happiest day of my entire month since the HP Prodesk decided to leave me; not only I got it relived but I also welcomed a new one to my family ! The specs of the machine are:\nCPU: 3.6 GHz Intel Core i7-4790 Quad-Core\nRAM: 16GB of 1600 MHz DDR3\nStorage: 1TB HDD\nSo right now, I have 3 PCs, which is perfect for me to set up the Proxmox 3-node cluster I had dreamed of in the summer. But after a month of testing with it, I realized two of the nodes (the HP Prodesk and Dell Optiplex) constantly went offline. After unplugging and replugging the LAN cable, it would connect again, but only for a very short period. I figured this must be a networking bottleneck. And then, the upgrading journey continued.\nThe Bottleneck: Fixing a Failing Cluster with a Dedicated Network # If you want to know more about the background of my network design for my homelab, feel free to read this post.\nFor a long time while working with my homelab, I only had a single PC at one place at the time: the HP Prodesk when I was in Vietnam, and the Lenovo P520 when I was in the US. Therefore, I always connected the LAN cable from the PC directly to the router. But since I now have 3 PCs together at one place and plan to add a PS4 for remote gaming, relying on the ports on the router wouldn\u0026rsquo;t allow me to have more than 3 devices connected at the same time. Because of this, I got myself a basic 1 Gbps switch with 8 additional ports back in the summer.\nThe interesting thing about a switch is that not only does it expand your LAN ports, it\u0026rsquo;s also capable of helping the computers talk directly to each other, without having to go through the \u0026ldquo;middle-man\u0026rdquo;—the router. Think of it as a simple roundabout that connects all the local streets, with the big highway being the router. You can go from one street to another easily, and you can also get on the highway if you want to. This offloads the job from the router (which is like a toll booth on the highway), improving internal throughput and performance.\nHowever, with only one LAN port per computer, all my traffic was forced onto the same network. This means the high-speed Proxmox cluster communication (checking if nodes are online), VM traffic, and any storage traffic were all competing on the same, single 1Gbps \u0026ldquo;road.\u0026rdquo; This congestion was the root cause of my problem: critical cluster packets were being dropped, causing my nodes to think their partners were offline. Thanks for this comment from North Idaho Tom Jones on the Proxmox forum, I was able to find the solution:\nIMO - some things to look at your cluster communications :\n- How fast are your physical network interfaces ( 100-Meg , 1-Gig , 10-Gig , something faster ).\nThere is a possibility your interfaces might be busy moving I/O traffic to/from your VMs , and you don\u0026rsquo;t have the network additional I/O capacity for the cluster to communicate to the other cluster(s).\n- Are you performing backups when the cluster(s) drops ?\n- Do you have interface errors and/or packet drops on your physical \u0026amp; virtual ethernet interfaces ( and your external switch(es) ).\n- What is your current I/O bandwidth rate when you drop a node in your cluster ?\nI have a Proxmox network with 14-Clusters and 6-external NFS systems for my VM hard disk storage. I have 10 \u0026amp; 40 Gig network cards. My cluster IPs and my NFS IPs do not share any IP address space with my VMs \u0026mdash; My VMs \u0026amp; my Cluster IPs \u0026amp; my NFS IPs are on unique IP networks. I do this to keep unwanted/un-needed network chatter down to a minimum. I have never had a node drop out of the cluster , even with pushing 20+ Gig on multiple nodes in the cluster at the same time while all nodes are doing a backup at the same time.\nFrom this, I decided to upgrade my network infrastructure again, finding new network cards to expand the LAN ports on my PCs. I initially looked at 10 Gig, but after seeing the price of both the network cards and the switch, I immediately retreated. Although my switch is only 1 Gig, I didn\u0026rsquo;t plan to replace it right now because even 2.5 Gig switches are so expensive. So I opted for 2.5 Gig network cards for each computer, with the plan to upgrade the switch later.\nAt first, I was just looking for the cheapest network card. After searching on Amazon, I found this one, which is only $8 for a 2.5 Gig PCIe network card. I immediately bought 3 of them. When they arrived, I found out that the PCIe connection here is an M+B key, which is a totally different connector than an NVME drive or a standard PCI x1 slot. It\u0026rsquo;s a rare port for a PC to have, as it\u0026rsquo;s usually used for WiFi cards or Coral TPU accelerators.\nNeedless to say, only the HP Prodesk had this port, and it didn\u0026rsquo;t even look nice when plugged in (see the extra cord that\u0026rsquo;s totally detached from the case in the image above). So I had to return two of the network cards and buy two more. This time, I learned that I needed to find a specific PCI x1 network card, so I filtered carefully and bought this one. After they arrived, they fit nicely into the cases.\nThe next step, after wiring everything and connecting it to the switch, was to go into Proxmox to verify the PCs recognized the cards. Then I created a new subnet (192.168.69.0/24) just for these 3 PCs so they could talk to each other directly for cluster communication. After that, I changed the /etc/hosts file on each node so they would use these new IPs to communicate with each other. And after everything was done, voila! The cluster is now stable, and each node has two different addresses: one for the main network (via the router) and one for direct, high-speed communication with other nodes.\nWhat\u0026rsquo;s Next: Building the Software Stack (NAS, Kubernetes, and ArgoCD) # And with that, the hardware journey is complete. It\u0026rsquo;s been a fantastic learning experience, evolving from a single, hand-me-down HP Prodesk to a powerful 3-node Proxmox cluster.\nThis process taught me some valuable lessons:\nStart with what you have: That first SFF PC was the perfect, low-cost entry point into self-hosting.\nPlan for your bottlenecks: I quickly hit the physical limits of SFF (power, cooling, and PCIe slots). Moving to a full-tower workstation like the Lenovo P520 was the right call, giving me room for a real GPU and more storage.\nNetworking is critical for clusters: A \u0026ldquo;stable\u0026rdquo; cluster isn\u0026rsquo;t just about the PCs; it\u0026rsquo;s about the network connecting them. My nodes kept dropping offline due to network congestion on a single 1Gbps link. Adding dedicated 2.5GbE cards for a private cluster subnet (192.168.69.0/24) made the cluster perfectly stable.\nNow that the physical foundation is laid, it\u0026rsquo;s time to build on top of it. In the next part of this series, I\u0026rsquo;ll dive into the software that brings this hardware to life. We\u0026rsquo;ll cover:\nCreating a ZFS-based NAS: How I\u0026rsquo;m using TrueNAS (or Proxmox\u0026rsquo;s built-in ZFS) to manage my 6TB of storage.\nSpinning up the Kubernetes Cluster: We\u0026rsquo;ll provision the VMs that will form our 3-node k8s cluster using Terraform and Ansible.\nAutomating Everything with GitOps: Finally, I\u0026rsquo;ll show you how I use ArgoCD to automatically deploy and manage all my applications (Jellyfin, Frigate, and more) on Kubernetes.\nStay tuned!\n","date":"28 October 2025","externalUrl":null,"permalink":"/posts/on-premise-hardware/","section":"Posts","summary":"\u003cp\u003eYou might have heard about the recent AWS outage that caused many services to go down. To think that half of the internet relies on a single service is crazy, even if it might be the best in its field. The engineers at AWS have done many things to prevent this, such as high availability, multi-region, zones, etc., but if a critical part of it still goes down, the whole thing goes down too. Therefore, if you don\u0026rsquo;t want to give all your money and trust to a single company, you might want to explore the on-premise option, which is self-hosting.\u003c/p\u003e","title":"On-Premise 101 (Part 1): Building a 3-Node Cluster","type":"posts"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/tags/self-hosted/","section":"Tags","summary":"","title":"Self-Hosted","type":"tags"},{"content":"A long time ago, my house only had 1 router, located on the third floor. So whenever we needed to access the internet while having dinner on the first floor, we needed to enable 4G or walk near the stairs just to receive a fraction of the WiFi signal. My father decided to give me an important task, which was to figure out how to solve this issue. I initially bought a simple WiFi extender located on the second floor, but after using it for a month, even though we could receive a strong signal from it, we couldn\u0026rsquo;t access the internet because the signal from the router on the 3rd floor to the WiFi extender on the 2nd floor was still really weak.\nThat\u0026rsquo;s when I decided to ask my dad to invest in a powerful second router located in my room on the 2nd floor, that could connect to the master router via LAN cable and advertise a strong WiFi signal. Researching multiple options, I opted for the Xiaomi CR6608 as it had impressive specs (WiFi 6, 4 antennas, gigabit) and most importantly, it could install OpenWRT. This opened a whole new world for me, because after 3 years, it has taught me many things about networking and the internet, such as subnets, IPv4, DHCP, DNS, and VPNs.\nThis post is a tour of my home network, built on that OpenWRT router. I\u0026rsquo;ll walk through how I designed my network layout, solved DNS and ad-blocking, and built a secure, globally-accessible lab using Tailscale and a cheap VPS.\nPlanning the Network # Currently, this router is configured as a cascaded router behind another router (double NAT). So it is also a DHCP server, which subnet is 10.69.0.0/16, to give IPs to the client devices. It has the total capacity of giving 256*256=65536 IPs:\nPhysical devices (fixed): 10.69.0.1 to 10.69.0.255 (255 ips) Proxmox VMs: 10.69.1.1 to 10.69.1.255 (255 ips) Kubernetes load balancers: 10.69.2.1 to 10.69.2.255 (255 ips) Other WLAN client (dynamic): 10.69.254.1 to 10.69.255.255 (510 ips) I also plan to learn how to set up VLANs for additional security and a separate environment, but that requires a managed switch, which is kinda expensive. For now, it has done an awesome job of being a router, giving IPs and helping the clients connect to the internet. But it also needs to provide DNS for the client devices too.\nBlocking Ads and Local DNS # On the DNS side, I initially used an AdGuard local server hosted directly on the Xiaomi CR6608 to provide DNS for clients. But after running it for a while for my family, I found out that my device couldn\u0026rsquo;t hold up long when multiple devices trying to connect; it kept restarting after experiencing too much load. And also, when I wanted to go out, if I wanted to get the same DNS setup as when I was at home, I needed to configure a lot more to get it working.\nSo in the end, I chose to use NextDNS as my primary DNS solution. With luci-app-nextdns in the OpenWRT packages, my router gets the profile ID for my NextDNS profile and then advertises it so that every client can also have it. What\u0026rsquo;s more, I can still configure my iPhone to have an HTTPS DNS Profile for NextDNS when I go outside.\nAdditionally, it also has the capacity to block ads, parental controls, and blacklist or whitelist any domain you want. This feature is extremely useful if you want to limit yourself from doom-scrolling Facebook, YouTube, or any other services while you\u0026rsquo;re at work, and you can get access back when you\u0026rsquo;re not. It can also provide analytics and logs, which give you more monitoring options.\nAccessing My Network from Anywhere # But using DNS for my local server would not give me immediate access to my server at home. I needed a way to access my local IP while I am outside. Moreover, since my internet modem is also behind a NAT, opening port forwarding would have no effect. After researching, I found these were the options that could help me:\nFeature WireGuard Cloudflare Tunnel Tailscale How it Works Direct Client-to-Server VPN. Your server at home listens for connections. An agent at home sends an outbound connection to Cloudflare. All devices connect to a virtual mesh network, brokered by Tailscale. Setup Difficulty Difficult Medium Very Easy Needs Open Ports? Yes No No Client App Required? Yes (on client devices) No (for web access) Yes (on all devices) Key Pros High performance, self-hosted, full network control. Simple for web access, no client app for browsers, hides your home IP. \u0026ldquo;It just works\u0026rdquo; simplicity, great for CG-NAT, powerful features (subnet routes). Key Cons Hard to set up, requires port forwarding, fails with CG-NAT. Requires a domain, traffic limits (not for video), less flexible for non-web. Requires their app on all devices, relies on a 3rd-party service. Needless to say, I have chosen Tailscale to work with. I couldn\u0026rsquo;t believe it is a free solution because it gives me so much management and many features. The ability to act as both a subnet route (advertise local IP for devices that don\u0026rsquo;t have the app) and an exit node (forward all the traffic through one device) is just super powerful. I did try using Cloudflare Tunnel at the beginning, but it required me to configure every DNS record each time I wanted to access a service, and it also did not allow me to stream video, so I migrated all of it to Tailscale a year ago.\nCurrently, I have Tailscale installed on my OpenWRT router, which advertises a subnet route of 10.69.0.0/16 and also acts as an exit node. If I go outside, I just simply open the Tailscale app on my phone or my laptop, authenticate, and bang, connected. If I carry a device that can\u0026rsquo;t install Tailscale, I can simply connect it to my travel router (which is the gl-ar300m device in the image above), which I have wrote about here.\nDon\u0026rsquo;t get me wrong, Cloudflare Tunnel is still a solid option. It has one feature that Tailscale couldn\u0026rsquo;t provide, which is the ability to allow guest users to connect to my home services without them needing to install an app or authenticate anything. This is extremely useful for a web blog when I want to showcase something. But for now, I have figured out a way to combine Tailscale and an additional VPS (a virtual private server, which is the racknerd-e54f406 device in the image above) to achieve this.\nSecurely Exposing Services to the Public # One day, I was tinkering with the idea of finding the best VPS provider to host some of my services in case my homelab was on maintenance but I still wanted to access critical services such as Bitwarden (a password manager) and Actual-budget (a budgeting service). Then I found this site called RackNerd. They offered an option that shocked me with its price and specs:\n11$ per year 1 vCPU Core 24 GB Pure SSD Storage 1 GB RAM 2 TB Monthly Transfer 1 Gbps Network Port Full Root Admin Access 1 Dedicated IPv4 Address What\u0026rsquo;s more, I realized I could also connect to services at home using Tailscale + Nginx, which is a reverse proxy. With 1 GB RAM, I could easily install Nginx Proxy Manager in a Docker container. I also needed to configure DNS on Cloudflare to route all DNS requests beginning with *.vps.phuchoang.sbs and vps.phuchoang.sbs to match the public static IP of the VPS. The final step was to install Tailscale on it and then type the home IPs on the destination, and I was good to go.\nHowever, I realized the above method was scary because it exposed the services publicly. If an application doesn\u0026rsquo;t have an authentication method or 2FA built-in, I was afraid hackers might attack my services at home. Therefore, I decided to spin up Authelia, an open-source authentication and authorization server. It allowed me to create a side-car authentication service for every \u0026ldquo;proxy host\u0026rdquo; in Nginx Proxy Manager above. All I needed to do was to add myself as a user and adjust the custom Nginx Configuration on the Advanced tab of each \u0026ldquo;proxy host\u0026rdquo;. Now each time you visit, for example nginx.vps.phuchoang.sbs, it will prompt you for a username and password. Even if you get it right, you have to provide the 2FA codes that are set up for each user. Pretty secure, right?\nSummary # It\u0026rsquo;s amazing to think this all started with a simple WiFi problem on the first floor. What began as a need for a second router (the Xiaomi CR6608) quickly became the \u0026ldquo;brain\u0026rdquo; of my entire home network.\nBy leveraging the power of OpenWRT, I built a custom network from the ground up. NextDNS handles clean and secure DNS (plus ad-blocking) for my whole family, both at home and on the go. Tailscale completely solves the challenge of being behind my ISP\u0026rsquo;s NAT, creating a secure mesh network that lets me access my 10.69.0.0/16 subnet from anywhere.\nFinally, adding a cheap RackNerd VPS with Nginx Proxy Manager and Authelia gives me the best of both worlds: a way to securely expose public services (like a blog) or private services (like Actual-budget) to the internet, all while being protected by a strong 2FA authentication portal. This setup is the backbone of all my projects and has been a powerful, hands-on learning experience.\n","date":"27 October 2025","externalUrl":null,"permalink":"/posts/self-hosted-network-design/","section":"Posts","summary":"\u003cp\u003eA long time ago, my house only had 1 router, located on the third floor. So whenever we needed to access the internet while having dinner on the first floor, we needed to enable 4G or walk near the stairs just to receive a fraction of the WiFi signal. My father decided to give me an important task, which was to figure out how to solve this issue. I initially bought a simple WiFi extender located on the second floor, but after using it for a month, even though we could receive a strong signal from it, we couldn\u0026rsquo;t access the internet because the signal from the router on the 3rd floor to the WiFi extender on the 2nd floor was still really weak.\u003c/p\u003e","title":"Building a Secure Network for Homelab with OpenWRT, NextDNS, Tailscale, and a VPS","type":"posts"},{"content":"","date":"27 October 2025","externalUrl":null,"permalink":"/tags/openwrt/","section":"Tags","summary":"","title":"Openwrt","type":"tags"},{"content":"","date":"27 October 2025","externalUrl":null,"permalink":"/tags/tailscale/","section":"Tags","summary":"","title":"Tailscale","type":"tags"},{"content":"A week ago, my laptop died. When I booted it up, it said something like, fTPM is corrupted, press Y to reset or N to do nothing. Neither of them helped me boot into my machine. So I had a severe mental attack back then, because that machine was the primary one that helped me do my work for school, my lab, and my personal projects. I was lucky to have my friend lend me an old laptop of his, but it was pretty bulky, so I figured I couldn\u0026rsquo;t bring it to school. Therefore, I had to go to the school library to do my work on its public computers.\nLuckily, I have a Microsoft Edge sync account, so all of my tabs, history, extensions, and passwords were replicated exactly like my old setup. However, as it is a school computer, it\u0026rsquo;s managed by the organization, so I couldn\u0026rsquo;t install any programs, [[replicate an exact programming experience on any machine]], or browse my personal home network. Lucky me again, there was a VirtualBox program on the school computer, so I could install Arch on it and run the script to download and configure all of my dotfiles and programs that I frequently use.\nHowever, there was still one big problem: I couldn\u0026rsquo;t access the resources that I host with my homelab, so I still couldn\u0026rsquo;t 100% replicate the network side of my home laptop. Previously, even if I was at school, I could connect via Tailscale to my home router running OpenWRT that advertises a subnet route of 10.69.0.0/16, so I could still access those addresses without having to physically be there. Because the public computer can\u0026rsquo;t install any programs, it can\u0026rsquo;t install Tailscale. But I saw there was a LAN cable connected to it, so I thought maybe I could buy some old router that could also run OpenWRT to hijack the school\u0026rsquo;s internet. Enter the GL.iNet Travel Router.\nThis post is the story of how I turned a tiny, $10 travel router into a powerful networking tool that gave me full, secure access to my entire homelab from a locked-down public computer.\nThe plan # After a long consideration and research, I settled on the GL-AR300M16. It was a tiny, cute little router that was actually (not so much) powerful. It\u0026rsquo;s specs were:\n100 Mbps Ethernet ports 16MB NOR ROM flash (The problem child) 128MB RAM (The lifesaver) Two configurable WAN/LAN ports My goal was to \u0026ldquo;insert\u0026rdquo; my own router between the school\u0026rsquo;s network and the public computer. The computer wouldn\u0026rsquo;t know the difference, but all its traffic would first go through my device, which would be running Tailscale. That way, I could fool the school into thinking I was connected via the school network, but I was actually routing all my packets to my home router. Here\u0026rsquo;s the data flow:\nThe price was really affordable too, only 33 bucks. But as a cheap-ass kid, I was looking to buy something old and used on eBay. Even more surprising, I actually found one that was new/in an unopened box and listed for bidding at 5 bucks. After staying up all night to watch the bid, I successfully got it for 10 + 8 bucks for shipping.\nFirmware reinstallation # After I received the package, the first thing I wanted to do was to uninstall the stock firmware and install the original OpenWRT to free up some space. I don\u0026rsquo;t hold any hatred toward the stock firmware, but with the router only having 16MB of ROM storage, and the firmware size being 15 MB, that left me with less than 1MB to do anything. So, no.\nSearching the internet, I found this site to download the clean version firmware of the router. But when I installed it, it only had OpenWRT version 17, which was so outdated that I couldn\u0026rsquo;t even update the packages properly because the SSL was no longer working. So I hit the internet again to find a guide and, luckily again, I even found the original OpenWRT guide dedicated to this machine.\nThe suggested way of installing OpenWrt on the GL-AR300M is using the u-boot bootloader. It can be accessed by holding the reset button, powering the device and waiting for 5 flashes of the LED. Instructions on how to access u-boot can also be found in the debricking instructions: https://docs.gl-inet.com/en/3/tutorials/debrick/.\nThen one can release the button, and access u-boot on http://192.168.1.1 which requires a PC with a static IP of 192.168.1.2 and netmask 255.255.255.0 .\nAssuming your u-boot is working correctly:\nboot into u-boot (debrick mode) by holding the reset button, powering the device and waiting for the led to flash 5 times. Browse to [http://192.168.1.1](http://192.168.1.1/ \u0026quot;http://192.168.1.1\u0026quot;) Upload glinet_gl-ar300m-nand-initramfs-kernel.bin file to router Wait for it to reboot Telnet to 192.168.1.1 and set a root password, or browse to http://192.168.1.1 if LuCI is installed. You will notice that the router is running in safemode. In order to leave it, one must now flash a regular image factory or sysupgrade (e.g. glinet_gl-ar300m-nand-squashfs-factory.img) from shell or luci. After this second flash, the router will boot normally.\nThe only thing I needed to adjust to access u-boot was that I needed to unplug all the external antennas and the WAN port, leaving only the LAN cable connected to it. Also, I needed to remember to download two files: one initramfs-kernel.bin file for the initial u-boot installation, and the other squashfs-sysupgrade.bin for the sysupgrade installation afterward.\nAlso, as I\u0026rsquo;ll explain later in this article, I was stuck with the issue of Tailscale not working without iptables because OpenWRT decided to use nftables in OpenWRT 22.04 and later. So I needed to install the 21.02 version. Here are the links to the two firmware files:\nopenwrt-21.02.0-ath79-generic-glinet_gl-ar300m16-initramfs-kernel.bin (for the initial u-boot flash) openwrt-21.02.0-ath79-generic-glinet_gl-ar300m16-squashfs-sysupgrade.bin (for the real installation afterward). Install Tailscale # Now for the fun part. I found and tried using this repo but realized the device only had 16MB of NOR ROM, of which 6MB had already been reserved for the firmware. So when I ran the script, it warned me I didn\u0026rsquo;t have enough storage, as it required 15MB.\nTherefore, I decided to follow this repo. Instead of relying on the 16MB NOR ROM storage, it stored the tailscaled and tailscale binaries in /tmp (i.e., the 128 MB RAM) instead. But it still stored the init daemon (which is used for downloading those binaries every time the router boots up) and the state file to keep the login and authentication persistent.\nHowever, with that said, I still had to downgrade the Tailscale version and remove the \u0026ldquo;fetch latest release\u0026rdquo; logic in the usr/bin/tailscale and usr/bin/tailscaled files because the latest version also didn\u0026rsquo;t fit in the available RAM.\ntailscale_version=\u0026#34;1.36.1\u0026#34; # latest_version=`wget -O- https://pkgs.tailscale.com/stable/ | grep tailscale_ | head -1 | cut -d\u0026#39;_\u0026#39; -f 2` # if [ \u0026#34;$tailscale_version\u0026#34; != \u0026#34;$latest_version\u0026#34; ]; then # tailscale_version=$latest_version #fi For Tailscale versions before 1.58.2-1, the init script may need to be modified to force Tailscale to assign an IP to the tailscale0 interface. So I had to modify the init daemon at /etc/init.d/tailscale. After the last procd_append_param, I added: procd_append_param command --tun tailscale0\nAs mentioned previously, Tailscale has trouble running on OpenWRT version 22.04 and later. I tried to follow this guide to fix it, but no luck, so I decided to reinstall firmware 21.02 again.\nwgengine.NewUserspaceEngine(tun \u0026#34;tailscale0\u0026#34;) error: router.Up: setting netfilter mode: exec: \u0026#34;iptables\u0026#34;: executable file not found in $PATH flushing log. logger closing down getLocalBackend error: createEngine: router.Up: setting netfilter mode: exec: \u0026#34;iptables\u0026#34;: executable file not found in $PATH After the re-installation and following the exact guide above, I was finally able to authenticate with Tailscale on the router.\nroot@OpenWrt:~# /etc/init.d/tailscale start root@OpenWrt:~# tailscale up To authenticate, visit: https://login.tailscale.com/a/d7ec72e01e7d4 Success. Some peers are advertising routes but --accept-routes is false # Reboot the machine root@OpenWrt:~# tailscale status Downloading Tailscale 1.36.1_mips .. Downloading \u0026#39;https://pkgs.tailscale.com/stable/tailscale_1.36.1_mips.tgz\u0026#39; Connecting to 199.38.181.239:443 Writing to stdout tailscale_1.36.1_mips/tailscale - 100% |*******************************| 20505k 0:00:00 ETA Download completed (20997485 bytes) Done! 100.98.192.109 openwrt-1 mrphuc26032006@ linux - 100.121.251.90 archlinux mrphuc26032006@ linux offline 100.72.74.109 ipad161 mrphuc26032006@ iOS offline 100.111.147.16 iphone-13 mrphuc26032006@ iOS offline 100.110.49.37 openwrt mrphuc26032006@ linux idle; offers exit node 100.68.207.111 racknerd-e54f406 mrphuc26032006@ linux idle; offers exit node The next step was to route all the traffic that connects to this router to my home network. This was easily done by following this guide.\nCreate a new unmanaged interface via LuCI: Network → Interfaces → Add new interface\nName: tailscale Protocol: Unmanaged Device: tailscale0 Create a new firewall zone via LuCI: Network → Firewall → Zones → Add\nName: tailscale Input: ACCEPT (default) Output: ACCEPT (default) Forward: ACCEPT Masquerading: on MSS Clamping: on Covered networks: tailscale Allow forward to destination zones: Select your LAN (and/or other internal zones or WAN if you plan on using this device as an exit node) Allow forward from source zones: Select your LAN (and/or other internal zones or leave it blank if you do not want to route LAN traffic to other tailscale hosts) Click Save \u0026amp; Apply\n(Optional) Connect to WPA2-EAP network such as eduroam # Although my router has 2 WAN/LAN ports, it also has the capability to act as a WiFi repeater. This can be extremely useful if I\u0026rsquo;m wandering around the school and can\u0026rsquo;t find any LAN ports on the wall; I can still connect to the eduroam network and advertise it back to my phone or tablet to access my home network.\nHowever, eduroam is kinda tricky to set up, even on a computer client. Lucky me again, I found this guide to help me set it up. Albeit, I had to install the wpad-wolfssl package instead of wpad, since I was using wolfssl as a backend service. The normal wpad package didn\u0026rsquo;t allow me to verify the SSL certificate after connecting to it.\n\u0026ldquo;eduroam\u0026rdquo; is a WWPA2-EAP network that allows members of higher education and other institution around the world to use each others WiFi networks with their home credentials. As such, the setup is slightly more complicated than that of other WiFi clients. Especially, the wpad package needs to be upgraded:\nInstall the full version of wpad: opkg update; opkg remove wpad-mini; opkg remove wpad-basic; opkg install wpad; reboot (via SSH, but the web UI works as well). Click \u0026ldquo;Scan\u0026rdquo; on either WiFi interface (but doing this for both seems to create problems with DHCP client) on http://192.168.8.1/cgi-bin/luci/admin/network/wireless, select \u0026ldquo;Join network\u0026rdquo; for any \u0026ldquo;eduroam\u0026rdquo; connection/AP. Select the few possible settings as appropriate, enter anything as password for now, \u0026ldquo;submit\u0026rdquo;. Under \u0026ldquo;Wireless Security\u0026rdquo;, select WWPA2-EAP as \u0026ldquo;Encryption\u0026rdquo;, and set everything else according to your institutions eduroam configuration. Here\u0026rsquo;s my school\u0026rsquo;s configuration: (Optional) Install NextDNS to have local DNS resolution and Ads Blocking on the go # If you want more detail on how I set up NextDNS and Tailscale, you can read it here. But for now, it is my main DNS server to resolve my Kubernetes Traefik resolutions, for ad-blocking services, and for parental controls to limit myself from doom-scrolling too much. Installing it was easier than Tailscale. Just get the NextDNS configuration profile ID, paste it in after you install the package, and you\u0026rsquo;re good to go.\nGo to System -\u0026gt; Software Click the \u0026ldquo;Update lists\u0026rdquo; button Enter \u0026ldquo;luci-app-nextdns\u0026rdquo; in the \u0026ldquo;Download and install package\u0026rdquo; field and click \u0026ldquo;OK\u0026rdquo; Go to Services -\u0026gt; NextDNS and configure NextDNS Summary # For $18, even though it\u0026rsquo;s a weak router, I couldn\u0026rsquo;t ask more of it. It can run Tailscale, act as a WiFi repeater, and handle NextDNS resolution, and with all of that, it still has 28MB left in RAM. What\u0026rsquo;s not to like? Definitely a lifesaver at a time when my laptop was broken. Maybe with this, I will begin to prefer going to school to study and code more than staying at home.\n","date":"20 October 2025","externalUrl":null,"permalink":"/posts/self-hosted-travel-router/","section":"Posts","summary":"\u003cp\u003eA week ago, my laptop died. When I booted it up, it said something like, \u003ccode\u003efTPM is corrupted, press Y to reset or N to do nothing\u003c/code\u003e. Neither of them helped me boot into my machine. So I had a severe mental attack back then, because that machine was the primary one that helped me do my work for school, my lab, and my personal projects. I was lucky to have my friend lend me an old laptop of his, but it was pretty bulky, so I figured I couldn\u0026rsquo;t bring it to school. Therefore, I had to go to the school library to do my work on its public computers.\u003c/p\u003e","title":"Accessing a Homelab from a Locked-Down PC with a Travel Router","type":"posts"},{"content":"","date":"31 May 2025","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"","date":"31 May 2025","externalUrl":"https://github.com/phuchoang2603/kubernetes-proxmox","permalink":"/projects/kubernetes-proxmox/","section":"Projects","summary":"This project automates the provisioning and configuration of a RKE2 Kubernetes cluster on Proxmox using Terraform and Ansible.","title":"Automate provisioning Kubernetes cluster on Proxmox with Terraform + Ansible","type":"projects"},{"content":" How I got here # Hey! I\u0026rsquo;m Hoang Xuan Phuc, but everyone just calls me Felix.\nI\u0026rsquo;m a CompSci student at the University of South Florida (USF), hailing from Vietnam. Growing up, my life was basically a script: Wake up at 6 a.m., school at 7, extra school at 5, eat, sleep, repeat. That was my loop until 5th grade, when my mom gifted me an old laptop.\nMy first thought? Gaming on it all day long. But that dream died fast. I quickly realized I had inherited a \u0026ldquo;potato laptop\u0026rdquo; after trying to run some random first-person shooter game (you know, back when they still used Adobe Flash Player). Sure, it was playable, but the lag was so bad I\u0026rsquo;d have to be a psychic to get a shot in before getting blasted.\nBeing a problem-solving nerd, I got hooked on a new question: How do I make this thing not suck?\nSo, thanks to Google and QuanTriMang.com, I went down the rabbit hole of 20-step optimization guides. I was editing startup programs in msconfig.exe, killing services in services.msc, and even foolishly trying to \u0026ldquo;download more RAM\u0026rdquo; by messing with pagefile.sys (protip: don\u0026rsquo;t do this). Honestly, every megabyte of RAM I freed up was a bigger rush than actually playing the game.\nThis tinkering addiction escalated. Before I knew it, I\u0026rsquo;d installed Linux, ditching games entirely for the promise of it running faster. And man, the learning curve wasn\u0026rsquo;t a curve, it was a cliff. I spent more time fixing the OS than actually using it.\nTouch-pad scrolls too fast? Have fun digging in random config files in the /etc directory; you need to manually change the file content, because there isn\u0026rsquo;t any proper GUI to fix this. How to change it? Oh, the internet said I should use the vim program to edit the file. Okay, it opened, but why can\u0026rsquo;t I type anything? You have to press i to enter Insert Mode? Okay, copied and pasted my line there\u0026hellip; now how the hell do I save and quit??? Yeah, it was annoying. But it was also so much fun. I realized I love digging into the guts of a computer and making it my own. Now, my life script is: wake up at 12 am, school till 6, eat, fake sleep, and then run random commands till 4am, repeat. And then, later on, I found myself not just playing around with my own laptop but playing around with other things like blocking ads on the router, self-hosted a Netflix and MangaDex at home, running Switch emulation on an iPad, hacking a PS4 to install unlimited games, \u0026hellip;\nThe problem is, my brain isn\u0026rsquo;t big enough to remember all these commands and weird tricks, and they\u0026rsquo;re all so cool I just want to talk about it all day long. So that\u0026rsquo;s where this blog comes in. Think of it as my public brain-dump - a place to store my \u0026lsquo;hidden recipes\u0026rsquo; and all the cool (or weird) things I find. It\u0026rsquo;s my personal notebook, a record of my journey, and maybe, just maybe, it\u0026rsquo;ll help or inspire you to break\u0026hellip; er, I mean, tinker with your own computer, too. :wq\nMy \u0026ldquo;On-Paper\u0026rdquo; Stats # Alright, enough stories. If you\u0026rsquo;re interested in my professional journey, my skills, and all that \u0026ldquo;hire me\u0026rdquo; stuff, you can find the details in the resume I\u0026rsquo;ve embedded below.\n","date":"4 April 2025","externalUrl":null,"permalink":"/about/","section":"/home/fel1x","summary":"\u003ch2 class=\"relative group\"\u003eHow I got here\n    \u003cdiv id=\"how-i-got-here\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#how-i-got-here\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eHey! I\u0026rsquo;m \u003cstrong\u003eHoang Xuan Phuc\u003c/strong\u003e, but everyone just calls me \u003cstrong\u003eFelix\u003c/strong\u003e.\u003c/p\u003e","title":"Welcome to my blog!","type":"page"},{"content":"","date":"3 April 2025","externalUrl":null,"permalink":"/tags/entertainment/","section":"Tags","summary":"","title":"Entertainment","type":"tags"},{"content":"","date":"3 April 2025","externalUrl":null,"permalink":"/tags/gaming/","section":"Tags","summary":"","title":"Gaming","type":"tags"},{"content":"If you thought the biggest gaming news was the upcoming Switch 2, think again. The emulation community has made a breakthrough: you can now run Nintendo Switch games on your iPhone or iPad at buttery-smooth 60FPS—all offline, with no jailbreak, no shady VPNs, and zero sketchiness.\nBackground # What is JIT? Why is it important for emulation? # Just-In-Time (JIT) compilation is a method used by emulators to dramatically improve performance by converting code into machine instructions while the program is running. Normally, an emulator has to interpret every single instruction one-by-one, which is slow. JIT speeds things up by compiling larger chunks of code in real time, allowing games to run closer to native speeds. However, Apple has strict security policies that prevent unauthorized JIT execution on iOS devices. There are some workarounds like JITStreamer or SideJITServer but they all require an internet connection. StikJIT and StosVPN now bypass this restriction by enabling JIT locally, without external servers, making offline Nintendo Switch emulation possible on iPhones. Here\u0026rsquo;s the difference between no JIT vs JIT-enabled when running emulator on iOS.\nPPSSPP On iOS 17 (NO JIT vs JIT) iPhone XR\nStosVPN – Internal VPN for enabling JIT without internet # Unlike commercial VPNs that often collect user data, StosVPN simply creates a virtual VPN on the device itself, tricking the system into thinking a “trusted connection” is being made. This lets iOS enable JIT without needing an actual internet connection—boosting privacy and eliminating security risks.\nThis is a breakthrough that allows Switch emulators to run fully offline with high performance.\n🔐 No internet connection needed – No data sent out – No unusual battery drain.\nEssential Tips Before You Begin # Recommended hardware - IOS 17+, Iphone/Ipad with more than 4 GB RAM. Disable Low Power Mode – Emulation requires full performance. If iOS throttles the CPU, the game will lag. Don’t close important background apps – Some apps like SideStore and LiveContainer must keep running for stable emulation. Prepare a Windows or macOS computer. You’ll need one to set up SideStore initially and to generate a pairing file. It\u0026rsquo;s best to put all the needed apps into one folder. Use that folder to transfer games – You can create a Windows shared folder and mount it in the iOS Files app for easier game management. Installing SideStore # SideStore is a sideloading tool (lets you install apps outside the App Store) that doesn’t require jailbreaking and is regularly updated by the community.\nInstallation steps:\nRead the official guide for more in-depth tutorials: SideStore Docs\nGenerate a pairing file:\nDownload jitterbugpair.exe Run it and transfer the *.mobiledevicepairing file to your phone Install SideStore:\nInstall AltServer Windows Guide Hold Shift + Click the AltServer tray icon and sideload the SideStore IPA Final Tweaks:\nInstall StosVPN from the App Store and enable it Open SideStore, pair it with the pairing file Refresh the app and remove previous AltServer certificates Why we need to refresh Sidestore (you may ask)? Normally, Apple limits sideloaded apps (using a free developer account) to 7 days of usage before you have to “refresh” (i.e., re-sign the IPA with a new certificate). When you install SideStore using AltServer, it stops using AltServer’s certificate and instead creates its own certificate, managed within the app itself. This means you won’t need a computer later—just open SideStore on your phone and tap “Refresh” to handle everything automatically. To do that, SideStore needs a way to simulate a local server environment to trick iOS into thinking the app signing process is legitimate. The trick here is using an internal VPN (StosVPN), which creates a loopback server directly on the device.\nInstalling LiveContainer # Apple allows only 3 sideloaded apps to run at once. LiveContainer bypasses this by running apps “inside” it, like a virtual machine. Here’s how to install it:\nDownload the LiveContainer IPA from HugeBlack’s fork Actions tab (requires GitHub account) Open SideStore with StosVPN enabled and add LiveContainer IPA. In LiveContainer Settings: Tap \u0026ldquo;Patch SideStore/AltStore\u0026rdquo; to reinstall it with tweaks. After installation: Reopen SideStore/AltStore. Return to LiveContainer: Tap \u0026ldquo;Test JIT-Less Mode\u0026rdquo;—if it says \u0026ldquo;Test Passed,\u0026rdquo; you’re good to go. Install a second instance of LiveContainer via the main LiveContainer app. In LiveContainer Settings: Set JIT Enabler to StikJIT (Another LiveContainer). Installing MeloNX \u0026amp; Enable StikJIT \u0026amp; Increasing RAM Limits # Apple limits apps to using only half of the device’s RAM, but GetMoreMemory by HugeBlack bypasses this restriction.\nDownload MeloNX \u0026amp; memory entitlement: MeloNX Repo and StikKIT IPA from StikJIT GitHub.\nAdd all three apps to the Apps section in LiveContainer. For each app: long-press the icon → Settings → Convert to Shared App.\nEnable file picker \u0026amp; local notifications in MeloNX settings. Run memory entitlement and log into your account to enable the entitlement for:\nLiveContainer LiveContainer2 MeloNX If errors occur, clean up Keychain and try again. Reinstall LiveContainer \u0026amp; LiveContainer2 to apply the configuration.\nReinstall SideStore from the app (do not just refresh it).\nUpload the pairing file in StikJIT on LiveContainer2 and enable \u0026ldquo;Auto Quit After Enabling JIT.\u0026rdquo; Run MeloNX via LiveContainer1.\nAdding keys, firmware, games on MeloNX # First-time setup:\nLaunch MeloNX via LiveContainer Choose your prod.keys and title.keys files Select your Switch firmware .zip Go to settings and check to see if it has JIT and extended RAM enabled You\u0026rsquo;re done! Just tap MeloNX from LiveContainer whenever you want to play, even offline. Add games (.NSP or .XCI) by tapping the ➕ button.\nI won’t go into detail on how to acquired keys, firmware, and games since this involves piracy. As far as I know, aside from downloading illegally, you can extract your own keys and firmware from your own Switch. For, uh, testing purposes, here’s a little base64:\nS2V5cyAmIEZpcm13YXJlOiBodHRwczovL3Byb2RrZXlzLm5ldC8NCkdhbWVzOiBodHRwczovL25zd2dhbWUuY29tLw== Some useful tips I’ve come across:\nAlways download both the game and its update file for the best performance. Large files may get stuck during transfers due to MeloNX’s lack of a proper file transfer UI. MeloNX Settings for best performance or least ram usage # Use the following settings to get the best possible performance # Shader Cache: On (may causes games to use much more ram which can cause crashes on devices with not enough ram tho) Disable VSync: Off (Enable if you want more than 30/60fps if your device can handle it) Texture Recompression: On MacroHLE: On Docked Mode: Off Resolution Scale: Use the lowest resolution you still find good where it doesn’t crash Memory Manager: Sometimes \u0026#34;Host Unchecked (fast, unstable / unsafe\u0026#34; or \u0026#34;Host (fast)\u0026#34; has better performance Ignore Missing Services: On Debug Logs: Off Trace Logs: Off MVK: Pre-fill Metal Command Buffers: Off Use the following settings to get the least amount of ram usage in a game # Shader Cache: Off Texture Recompression: On MacroHLE: On Docked Mode: Off Resolution Scale: The lower, the better Expand Guest Ram: Off Ignore Missing Services: On Debug Logs: Off Trace Logs: Off Memory Manager Mode: Host (fast) Disable PTC: On List of games that MeloNX currently supports # Compatibility | MeloNX # 3gb+ Devices # Minecraft: Nintendo Switch Edition (Bedrock requires a 8gb+ ram) Sonic Mania Captain Toad: Treasure Tracker Sniper Rescue Helltaker (Homebrew) VVVVVV Cheez it the game One shot world machine 4gb+ Devices (May work on 3gb devices if on ios 18.2.x) # Hue (probably 3gb devices too) Celeste Mario vs Donkey Kong (requires 2-3 tries the first time) Undertale (probably 3gb devices too) Star Wars: The Force Unleashed Carrion Trombone Champ (requires gyro controls which aren’t implemented yet) Dead Cells Thumper Farming Simulator 20 Super Meat Boy Nintendo Switch Online(All) Devil May Cry 3: Special Edition 6gb+ Devices (May work on 4gb devices if on ios 18.2 - 18.2.x) # Untitled Goose Game SCHiM Oceanhorn Super Mario Maker 2 Unravel 2 Super Mario Bros. Wonder Sonic Superstars Cult of Lamb Super Mario 3D World Portal ANTONBLAST Mario Kart 8 Deluxe Super Mario Bros. U Deluxe Outlast Links awakening Pokémon Legends Arceus PayDay 2 Pokémon Sword Oceanhorn (Some flickering but not unplayable) Farming Simulator 23 Goat Simulator Super Mario 3D World Animal Crossing: New Horizons Diablo 3 Eternal Edition Persona 5 Royal Persona 4 Golden Ni No Kuni 2 8gb+ devices (May work on 6gb devices if on ios 18.2.x) # Outer Wilds (set CPU Mode to Software) Skyrim Super Mario Odyssey Cuphead Arkham City Call of Juarez: Gunslinger Thief Simulator Asterix \u0026amp; Obelix XXL 3 Super Mario Party: Superstars Mario \u0026amp; Sonic: At The Olympic Games Super Mario 3D All-Stars (very slow performance) Need for Speed: Hot Pursuit Breath of the Wild Minecraft (Bedrock, Legacy works down to 3gb devices) Burnout Paradise Remastered Echoes of Wisdom Super Mario RPG Splatoon 2 Mario Rabbids: Kingdom Battle Ori and the Blind Forest Splatoon 3 Xenoblade Chronicles 3 Xenoblade Chronicles: Definite Edition 16gb iPads (also works on 8gb devices if on iOS 18.2.x) # Kirby and the Forgotten Land Lego Starwars: The Skywalker Saga (takes a few tries) Tears of the Kingdom Mario Tennis Aces Games which don’t boot/Boots then crash: # Stray Red Dead Redemption (works on Pomelo) Hogwarts Legacy The Witcher 3 Yoshi’s Crafted World DOOM Eternal Terraria Pikuniku (works on Pomelo) Mario Strikers No man’s sky Wolfenstein 2 Outlast 2 Dying Light Boots but too many graphical glitches # Octopath Travaler 2 Credits \u0026amp; Sources # This guide was compiled from various sources and contributions:\nHugeBlack: Developer of LiveContainer and GetMoreMemory 0-Blu: Developer of StikJIT MeloNX Team: Creators of the MeloNX emulator. You can also join the community here. SideStore Team: For enabling sideloading on iOS Various GitHub Repositories \u0026amp; Documentation: SideStore Docs LiveContainer GitHub StikJIT GitHub MeloNX Repo r/EmulationOniOS community, especially this post How to install MeloNX and get it working with fully offline JIT activation. A step by step guide. : r/EmulationOniOS Final Thoughts # Running Tears of the Kingdom on an iPhone sounds like science fiction—but it’s real. Once it’s all set up, you’ll be gaming at full speed, offline, on hardware that was should be meant for this. Even crazier? This lays the groundwork for full-blown VM emulation. Some users are already running macOS Sonoma on iPad using UTM.\n","date":"3 April 2025","externalUrl":null,"permalink":"/posts/emulation-switch-ipad/","section":"Posts","summary":"\u003cp\u003eIf you thought the biggest gaming news was the upcoming Switch 2, think again. The emulation community has made a breakthrough: you can now run Nintendo Switch games on your iPhone or iPad at buttery-smooth 60FPS—all offline, with no jailbreak, no shady VPNs, and zero sketchiness.\u003c/p\u003e","title":"Running Nintendo Switch Games on iPhone at 60FPS – Free, No Jailbreak Needed","type":"posts"},{"content":" Video demo: # The Struggle: Why I Needed This # Have you ever stumbled upon a manga so niche and obscure that it never gets an official English release? That was me—desperately trying to read intimate and underrated series that weren’t even on MangaDex, likely due to copyright issues.\nAt first, I took the long road: hunting for raw Japanese scans, screenshotting pages, and running them through image translation tools. It worked\u0026hellip; barely. The process was painfully slow and completely broke my immersion. Instead of enjoying the story, I spent more time waiting on awkward machine translations. It was, in a word, disruptive.\nThen it hit me: I already had Suwayomi, a powerful manga server that supports multiple sources and local file management. Why not build my own translation pipeline?\nFinding a Better Way # My first attempt was to look for an existing solution. I found a closed-source iOS app that seemed promising, but when I saw their $15/month paywall, I immediately bailed. (In case you\u0026rsquo;re curious, here\u0026rsquo;s the link.)\nThat led me to GitHub, where I stumbled upon this amazing project. The demo results blew me away, and I knew I had to dive in and make it work for my setup.\nPrerequisite # To follow along, you’ll need Suwayomi Server installed. You can check out the official Docker setup here:\nSuwayomi-Server-docker GitHub\nAfter installation, set up the download folder and local source location. Reference:\nSuwayomi Local Source Wiki\nHere’s my folder structure for Suwayomi:\n. ├── data #suwayomi folder │ ├── downloads # default download folder │ │ ├── mangas │ │ │ └── Rawkuma (JA) │ │ │ ├── Batsu Hare │ │ │ │ ├── Chapter 100 ... │ │ │ ├── Grapara! │ │ │ │ ├── Chapter 69 ... │ │ │ ├── Guilty Circle │ │ │ │ ├── Chapter 1 ... │ │ │ ├── Isekai Saikouhou no Guild Leader │ │ │ │ ├── Chapter 1 ... │ │ │ └── Spy X Family │ │ │ ├── Chapter 105.5 ... │ │ └── thumbnails │ ├── translated # local source location │ │ ├── Batsu Hare │ │ │ ├── Chapter 100 ... │ │ ├── Grapara! │ │ │ ├── Chapter 69 ... │ │ ├── Guilty Circle │ │ │ ├── Chapter 1 ... │ │ ├── Isekai Saikouhou no Guild Leader │ │ │ ├── Chapter 1 .... │ │ └── Spy X Family │ │ ├── Chapter 105.5 .... └── manga-image-translator ├── * ... ├── batch-script.py # additional batch script file that I wrote to automate the translation. The First Steps: Getting the Basics Running # The manga-image-translator repo seems pretty solid at first try. It provide me the default script that can translate multiple chapters at once.\nScan the INPUT_FOLDER (.i.e. the downloads folder I set up above) for new chapters. Run the manga translator command. Save the output in a matching OUTPUT_FOLDER. (.i.e the translated folder) Example set up:\nsudo apt install cython3 conda create -n manga-trans python=3.12 pip python -m manga_translator local -v -i ../suwayomi/data/downloads/mangas/Rawkuma\\ \\(JA\\)/Grapara\\!\\ Raw/Chapter\\ 13/ --output ../suwayomi/data/translated/Grapara\\!\\ Raw\\ translated/Chapter\\ 13/ --use-gpu --config-file examples/config-example.json This worked, but there was one big problem: it wasn’t fully automated and has a lot of command flags that I need to remember. I had to manually re-run the script every time a new chapter arrived. Clearly, there had to be a better way.\nAutomating the Pipeline: Watching for New Manga Chapters # I started exploring file-watching solutions and tested various Linux tools like inotify and watchdog. However, they didn’t track entire directories the way I needed. I wanted a script that:\nMonitors INPUT_FOLDER (and subfolders) for new content. Triggers translation automatically when a new chapter appears. Maintains the same folder structure in OUTPUT_FOLDER. Enter Python watchdog library - a game-changer. With it, I wrote a script that:\nWatches the manga download directory. Detects when a new chapter folder appears. Extracts the relative path and mirrors it in the output directory. Here’s the script:\nimport os import time import subprocess from watchdog.observers import Observer from watchdog.events import FileSystemEventHandler # ===== Configuration ===== INPUT_ROOT = \u0026#34;data/downloads/mangas/Rawkuma (JA)\u0026#34; OUTPUT_ROOT = \u0026#34;data/translated\u0026#34; CONFIG_FILE = \u0026#34;examples/config-example.json\u0026#34; # ========================== class TranslationHandler(FileSystemEventHandler): def on_created(self, event): if event.is_directory: print(f\u0026#34;Detected new folder: {event.src_path}\u0026#34;) self.process_new_folder(event.src_path) def process_new_folder(self, input_path): # Get relative path from input root relative_path = os.path.relpath(input_path, INPUT_ROOT) dest_path = os.path.join(OUTPUT_ROOT, relative_path) # Create output directory if it doesn\u0026#39;t exist os.makedirs(dest_path, exist_ok=True) command = [ \u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;manga_translator\u0026#34;, \u0026#34;local\u0026#34;, \u0026#34;-v\u0026#34;, \u0026#34;-i\u0026#34;, input_path, \u0026#34;--dest\u0026#34;, dest_path, \u0026#34;--config-file\u0026#34;, CONFIG_FILE, \u0026#34;--use-gpu\u0026#34;, ] try: print(f\u0026#34;Starting translation for: {relative_path}\u0026#34;) subprocess.run(command, check=True) print(f\u0026#34;Successfully translated: {relative_path}\\n\u0026#34;) except subprocess.CalledProcessError as e: print(f\u0026#34;Error translating {relative_path}:\u0026#34;) print(f\u0026#34;The command failed with return code {e.returncode}\u0026#34;) print(\u0026#34;Command executed:\u0026#34;, \u0026#34; \u0026#34;.join(command)) def main(): # Validate paths if not os.path.isdir(INPUT_ROOT): raise ValueError(f\u0026#34;Input folder does not exist: {INPUT_ROOT}\u0026#34;) os.makedirs(OUTPUT_ROOT, exist_ok=True) # Set up folder observer event_handler = TranslationHandler() observer = Observer() observer.schedule(event_handler, INPUT_ROOT, recursive=True) observer.start() try: print(f\u0026#34;Watching directory tree: {INPUT_ROOT}\u0026#34;) print(f\u0026#34;Mirroring structure to: {OUTPUT_ROOT}\u0026#34;) print(\u0026#34;Press Ctrl+C to stop monitoring...\u0026#34;) while True: time.sleep(1) except KeyboardInterrupt: observer.stop() observer.join() if __name__ == \u0026#34;__main__\u0026#34;: main() Running It as a Background Service or a Docker container # I wanted the script to run 24/7 without manually starting it each time. There are two ways to achieve this:\nRun the Python script as a systemd service (requires tinkering with conda). Run it as a Docker container (recommended).\u0026quot;* Option 1: systemd Service (for local installs, not recommended) # Created a new service file: sudo nano /etc/systemd/system/manga-trans.service Added this configuration: [Unit] Description=Manga translation for Suwayomi After=network.target [Service] User=1000 Group=1000 WorkingDirectory=/DATA/AppData/manga-image-translator ExecStart=/bin/bash -c \u0026#34;source ~/miniforge3/etc/profile.d/conda.sh \u0026amp;\u0026amp; conda activate manga-trans \u0026amp;\u0026amp; python batch-script.py\u0026#34; [Install] WantedBy=multi-user.target Enabled and started the service: sudo systemctl daemon-reload sudo systemctl enable manga-trans.service sudo systemctl start manga-trans.service Option 2: Docker Container (recommended) # Rather than relying on conda, I containerized the entire application and push it to my own registry. To run it, you must first create a .env file to specify where the data gonna stored.\nMANGA_FOLDER=/mnt/storage/media/manga APPDATA=/mnt/storage/appdata/suwayomi TZ=America/New_York You also need to change the default location where the new manga will be download to in the Suwayomi web interface.\nHere\u0026rsquo;s my docker-compose.yml setup for the full stack deployment.\nname: suwayomi services: suwayomi: container_name: suwayomi environment: - EXTENSION_REPOS=[\u0026#34;https://raw.githubusercontent.com/keiyoushi/extensions/repo/index.min.json\u0026#34;] - FLARESOLVERR_ENABLED=true - FLARESOLVERR_URL=http://flaresolverr:8191 - TZ=${TZ} hostname: suwayomi image: ghcr.io/suwayomi/suwayomi-server:preview ports: - 4567:4567 restart: always volumes: - ${MANGA_FOLDER}:/home/suwayomi/data - ${APPDATA}/data:/home/suwayomi/.local/share/Tachidesk flaresolverr: container_name: flaresolverr environment: - TZ=${TZ} hostname: flaresolverr image: ghcr.io/flaresolverr/flaresolverr:latest ports: - 8191:8191 restart: unless-stopped manga-image-translator: # build: # context: ${APPDATA}/manga-image-translator image: ghcr.io/phuchoang2603/manga-image-translator:v0.1.0 container_name: manga-image-translator command: batch-script.py volumes: - ${APPDATA}/manga-image-translator/:/app/ - ${APPDATA}/facehuggingcache:/root/.cache/huggingface/ - ${MANGA_FOLDER}:/app/data ipc: host # For GPU deploy: resources: reservations: devices: - capabilities: [gpu] Now, it runs in the background automatically whenever I download a new manga chapter. No more waiting, no more manual intervention—just seamless reading.\nFinal Thoughts # This project completely transformed how I read untranslated manga. No more screenshots, slow translators, or endless searching. Everything is automated, seamless, and runs quietly in the background.\nIf you\u0026rsquo;re someone who digs through obscure manga titles like me, I highly recommend trying this out.\n","date":"3 March 2025","externalUrl":null,"permalink":"/posts/self-hosted-manga-trans/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eVideo demo:\n    \u003cdiv id=\"video-demo\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#video-demo\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003clite-youtube videoid=\"RgkC246Ul44\" playlabel=\"RgkC246Ul44\" params=\"\"\u003e\u003c/lite-youtube\u003e\n\n\u003chr\u003e\n\n\u003ch2 class=\"relative group\"\u003eThe Struggle: Why I Needed This\n    \u003cdiv id=\"the-struggle-why-i-needed-this\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#the-struggle-why-i-needed-this\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eHave you ever stumbled upon a manga so niche and obscure that it never gets an official English release? That was me—desperately trying to read intimate and underrated series that weren’t even on MangaDex, likely due to copyright issues.\u003c/p\u003e","title":"Automating Manga Translations: My Journey to a Seamless Reading Experience","type":"posts"},{"content":"We all love the idea of being super organized, right? We make these grand plans, give it our all for a few weeks or months, and then, bam! We\u0026rsquo;re back to the good old \u0026ldquo;throw it in the downloads folder with everything else\u0026rdquo; routine. It happens to the best of us, and I\u0026rsquo;m guilty too. But hey, even though I\u0026rsquo;ve still got loads of room for improvement, I can proudly say I\u0026rsquo;ve gotten way faster at finding my files.\nPhase One (2014-2019): The Awakening # I stored all my files in the initial phase in the download folder. At that time, with my first laptop and just one attached hard drive, I wasn\u0026rsquo;t concerned about efficiently organizing my files. Typically, I heavily rely on browsers while using our PCs. Most of my files originated from internet downloads—games, program installation, and office files from Gmail, Facebook, and Messenger. As you may know, none of these were particularly crucial: once I downloaded them, I never looked back. However, my awareness of this inefficient system grew as I began experiencing difficulties locating specific files and noticed duplications due to multiple downloads. The tipping point was when I spent nearly an hour searching for a homework document, resulting in a late submission to my teacher. This prompted me to reconsider and change my approach.\nImage courtesy of Folder Tidy\nPhase Two (2019-2021): Aesthetic Revolution # I initiated using folders to categorize file types and employed Internet Download Manager to organize downloaded files based on filename extensions automatically. As someone who values aesthetics, I went further by customizing folder icons for each category, replacing the default \u0026ldquo;folder\u0026rdquo; icon on Windows. Placing these folders in Quick Access significantly enhanced the efficiency of file retrieval.\nHowever, I found the process inefficient when collaborating online with others for studying or project presentations. Organizing files downloaded from shared Drive folders and re-uploading them became time-consuming. Consequently, I realized the need to alter my file organization methods for both offline and online activities.\nPhase Three (2021-2023): Google Drive Renaissance # I dedicated time to thoroughly reorganizing my Google Drive, which had been messy from its use as temporary storage, before transferring files to my offline storage. Despite long-standing plans to address this, the chaotic state of my Google Drive had deterred me. The turning point came when I had to switch from my laptop to a new PC. In the past, I often neglected the backup process: I would have unplugged my hard drive and connected it to the new PC, risking the loss of essential files. But the thing is, there were many important files on it, and I was told by many sources that I should back it up to several Cloud Storage before executing the change. That also allowed me to rearrange things, e.g., filtering out unnecessary files, moving big files onto the USB, and uploading essential files to Google Drive.\nHere\u0026rsquo;s my Drive after fixing it. The two folders, \u0026ldquo;Application\u0026rdquo; \u0026amp; \u0026ldquo;College,\u0026rdquo; are synced with my offline files\nAdditionally, I discovered the Google Drive for Desktop App. It mirrors my Drive files, mounting them to the chosen location\u0026rsquo;s virtual disk. This means all the changes I made on the offline files will be automatically synced with the files on the cloud, reducing the significant time of waiting for uploading.\nCurrent \u0026amp; Future: Expanding Horizons # Recently, I obtained a 5TB OneDrive from the Microsoft 365 free E5 Developer Pack, expanding my file storage capacity. This includes game installation files, video editing projects, and downloaded online courses. Files for collaboration with friends and teachers, such as homework assignments, IELTS, and SAT papers, are stored on Google Drive. Moreover, with the arrival of a refurbished computer that I turned into a Synology NAS, I now have more options for file storage. This NAS serves as my torrent server for streaming films, shows, audiobooks, etc., through platforms like Jellyfin and Audiobookshelf. I plan to share my journey of building self-hosting in the near future, but for now, I am satisfied with my current setup.\nIn the future, I may adopt new techniques to organize my files more efficiently. As of now, I have switched to macOS, and with the wonderful search app called Raycast (Spotlight alternative), I guess I may return to the era where I would store my files a bit more freely, as opposed to the strict tree structure now. Because with the tree structure method, it still takes me several steps to navigate to the files I want. But with the file search method, I just need to learn how to name the files mnemonic, and I can have my files right in the second.\n","date":"16 November 2023","externalUrl":null,"permalink":"/posts/productitvity-file-management/","section":"Posts","summary":"\u003cp\u003eWe all love the idea of being super organized, right? We make these grand plans, give it our all for a few weeks or months, and then, bam! We\u0026rsquo;re back to the good old \u0026ldquo;throw it in the downloads folder with everything else\u0026rdquo; routine. It happens to the best of us, and I\u0026rsquo;m guilty too. But hey, even though I\u0026rsquo;ve still got loads of room for improvement, I can proudly say I\u0026rsquo;ve gotten way faster at finding my files.\u003c/p\u003e","title":"Organizing My Digital Life - A Nerdy Journey Across Drives, File Management","type":"posts"},{"content":"","date":"16 November 2023","externalUrl":null,"permalink":"/tags/productivity/","section":"Tags","summary":"","title":"Productivity","type":"tags"},{"content":"Today, I would like to introduce you to the journey of optimizing my workspace environment, or in simpler terms, my desk setup. In fact, I am not someone who strictly adheres to principles in life. However, I really enjoy working in a disciplined environment and always strive to optimize it for effective studying and working. To achieve the setup I have today, I had to tear down and rebuild this workstation more than 10 times. Finally, I am truly satisfied with it. It not only satisfies me aesthetically but also makes me eager to work as soon as I step into this room (well, not quite true because nowadays most of the time I feel most productive when working from a coffee shop\u0026hellip;).\nInitial set-up from 2021, when I was just a 9th-grade middle schooler # Looking at the picture, you would probably guess that this is an extremely simple setup. It shows a Lenovo 14-inch Intel Celeron old school laptop that I inherited from my mom when I was just a 7th-grader. It\u0026rsquo;s a typical low-end Windows laptop from before 2015, and I couldn\u0026rsquo;t even take it anywhere because the connector between the monitor and the keyboard was broken. I remember trying to install some games on it, but the only one that ran smoothly was Halflife 2, which is from the 2000s. Despite its limitations, I feel positive towards this laptop because it served my educational needs and taught me how to \u0026ldquo;optimize\u0026rdquo; things\u0026hellip;\nOver the years, although there were countless times when this low-end laptop frustrated me with its 2GB RAM and inability to open more than three Google Chrome tabs, it still fulfilled my educational requirements. Adding a second monitor that my mom had left over from her office was a game changer for me, especially during the COVID-19 pandemic when I had to attend Zoom classes. This additional monitor played a crucial role in helping me get accepted into three top high schools in my country before I eventually got a new PC. This laptop also taught me the initial steps of exploring the Internet effectively and opened up a world of computer science knowledge.\nTo this day, even though I couldn\u0026rsquo;t keep the laptop because I donated it to my cousin, I still use the second monitor as it has become an integral part of my workflow. Since having two screens, my work productivity has significantly increased as I can multitask and access information more easily. In fact, if given the choice between an extremely high-end laptop with only one screen or a mediocre one with an additional screen, I would not hesitate to choose the latter.\nAfter I got into Amsterdam High School, here\u0026rsquo;s my mum\u0026rsquo;s present. # Boom, a big upgrade coming right there. Like, literally from a 50 USD budget laptop to a 700 USD high-end PC. Not to mention this is also the first time I have had the courage to ask my mom for a present, as I usually shy away from asking my parents to spend money on me. However, this time was different because for the first time in my life, my parents were ready to spend this much for my attendance at Amsterdam High School.\nHowever, as much of a tragedy it may seem, this is also the time when hardware components became so expensive due to the rise of Bitcoin and Cryptocurrency mining. So if I had the opportunity to go back, I would probably wait until November 2021 to buy a new one because with the same specifications, I could have spent only around $400 on this one. Still, I am so grateful for this because it was a big transition from my last laptop and it serves my needs way better than its capabilities.\nEverything got replaced except for the DELL second monitor mentioned above. I also took the old hard disk from my laptop to serve as the second hard drive for my computer. Truth be told, initially, I was very excited to set up my workstation. I often visited Facebook groups and watched videos about workspaces, cable management, decorative accessories, and more from various individuals. After countless times hiding under the table trying to make the cables look organized, I finally got the final setup shown above. However, I was too focused on aesthetics without actually realizing what was most important: productivity.\nAfter about three months of attending online Zoom classes where my bed and chair were so close together, I started developing bad habits. Whenever I felt tired or sleepy during class, it was too easy for me to just fall back onto that bed and have a sleepy \u0026ldquo;day\u0026rdquo;. It was good for my health but definitely not good for my studies, so I once again had to make a change.\nSeparate my \u0026ldquo;working me\u0026rdquo; apart from my \u0026ldquo;sleepy me\u0026rdquo; # The picture above that you see right here shows the setup I had been using for nearly two years. As you can see, there is a table serving as a divider between the chair and the bed. This setup prevents me from \u0026ldquo;sleeping\u0026rdquo; when I have to attend a Zoom class or pull an all-nighter. The table not only serves as a place to put additional things on but also separates my \u0026ldquo;work\u0026rdquo; mode from my \u0026ldquo;entertainment\u0026rdquo; mode.\nAdditionally, the second old monitor has now been turned into a vertical one since the space on the table has become narrower. However, the main reason for this change is because there is some content that I really love to view vertically, such as articles, coding documentation, or Phuong Ly\u0026rsquo;s videos -)). Not only can you view more content at once, but it also resembles a big smartphone, allowing you to view optimized social media sites just like when surfing on phones like Medium, Facebook, Reddit,\u0026hellip;\nHowever, it\u0026rsquo;s easy to have an Icarus feeling - never being satisfied with what you currently have. After using two monitors for a while, I have the urge to have three monitors because when I am learning Coursera MOOC courses or watching YouTube reference materials while coding together, the vertical ones really irritate me because they can\u0026rsquo;t scale fullscreen. But I still have to suppress this urge because I know that I don\u0026rsquo;t have enough money to purchase another one and my VGA doesn\u0026rsquo;t have three ports to export screens. Until then,\u0026hellip;\nI once again changed because now I have 3 monitors and a Hackintosh # This is probably the last update before I leave my home to attend university. As you may have anticipated, after 2 years, I finally got a \u0026ldquo;new\u0026rdquo; monitor from my mom\u0026rsquo;s office leftovers (I\u0026rsquo;m really grateful for the whole office there, you know).\nI also switched my operating system from Windows to MacOS, but instead of buying a brand new Mac, I \u0026ldquo;Hackintoshed\u0026rdquo; my computer. Additionally, I purchased a refurbished RX580 VGA from a Facebook marketplace because my old GT1030 had some compatibility issues with MacOS.\nAnd that\u0026rsquo;s the end of my workspace optimization journey. As you probably expected, having a more modern and optimized workspace can lead to increased productivity, right? Well, that\u0026rsquo;s not entirely true. I\u0026rsquo;ve come to realize that true productivity comes from within and is not solely dependent on the hardware or number of monitors you have.\nThe funny thing is that even though my laptop setup when I was in 9th grade had the lowest specifications, I felt second most productive compared to my different setups mentioned above. I guess it\u0026rsquo;s because there were fewer distractions with that setup; all I could do was study. On the other hand, with the high-end PC setup, it\u0026rsquo;s easy to get distracted and procrastinate with games and social media sites. (I\u0026rsquo;ve put in a lot of effort to counteract this distraction and someday I\u0026rsquo;ll write a blog sharing how I\u0026rsquo;ve fought against it). So here\u0026rsquo;s my current most productive setup, which you may not be ready for yet.\n","date":"4 November 2023","externalUrl":null,"permalink":"/posts/productivity-workspace/","section":"Posts","summary":"\u003cp\u003eToday, I would like to introduce you to the journey of optimizing my workspace environment, or in simpler terms, my desk setup. In fact, I am not someone who strictly adheres to principles in life. However, I really enjoy working in a disciplined environment and always strive to optimize it for effective studying and working. To achieve the setup I have today, I had to tear down and rebuild this workstation more than 10 times. Finally, I am truly satisfied with it. It not only satisfies me aesthetically but also makes me eager to work as soon as I step into this room (well, not quite true because nowadays most of the time I feel most productive when working from a coffee shop\u0026hellip;).\u003c/p\u003e","title":"Organizing My Digital Life - How My Workspace Evolved from Chaos to Hackintosh","type":"posts"},{"content":"","date":"20 October 2023","externalUrl":null,"permalink":"/tags/flutter/","section":"Tags","summary":"","title":"Flutter","type":"tags"},{"content":"","date":"20 October 2023","externalUrl":null,"permalink":"/tags/hackathon/","section":"Tags","summary":"","title":"Hackathon","type":"tags"},{"content":"Today, I want to share my experience with you about the project I worked on during the Steam Hacks 2023 competition. Although my team did not win any major awards for this project, it was an incredible learning opportunity for me, particularly in terms of coding. Moreover, I also gained valuable insights into the collaborative process through working with my amazing new friend from Son La - Minh Chau.\nBackground information about STEAM Hacks # STEAM Hacks is a national hackathon organized by STEAMS for Vietnam in collaboration with Ha Noi University of Science and Technology. It offers two tracks, Hipster and Hacker, catering to UX/UI designers and developers respectively.\nAlthough I initially had confidence in joining the Hacker Track, I ultimately decided to participate in the Hipster Track. This choice was driven by my desire to have better overall project management skills and to further enhance my UX/UI abilities. However, it turned out that I still had to write code for the team despite being in the Hipster Track. (you would know the reason later on this article)\nThe hackathon consists of three rounds. The first round (Breaker Challenge) is an individual challenge with separate tracks: Hipster and Hacker. Each track would receive support by various workshops, covering the knowledge required to participate to the competition. At the end of the first round, participants from each track are required to submit an assignment that showcases their newly acquired knowledge. For Hipsters, this involves designing an e-commerce landing page with a high-fidelity wireframe created on Figma, along with user personas and user journey details related to their product. Hackers must submit a functional website developed using Flask framework and incorporating some form of \u0026ldquo;AI\u0026rdquo; feature.\n150 participants from both tracks are then chosen for Round 2: Innovation Spirit, where they form teams of 2 Hipsters and 2 Hackers and choose from four tracks: Sustainability, Education, Mental Health, or Community. Only 15 teams advance to the final round, The Hacking Day, with 7 team awards and 2 individual awards. It seems like I came close to being recognized as one of the best Hipsters since I was invited for an interview among the Top 5 Best Hipsters, but I couldn\u0026rsquo;t make it in the end though 😢 (interestingly, the recipient of the Best Hipster Awards was none other than Minh Chau, a fellow member of my team)\nMy initial idea for both first and the next two round # After placing as a Top 10 Finalist in last year\u0026rsquo;s Samsung competition, I couldn\u0026rsquo;t help but feel a sense of dissatisfaction with my product. It lacked certain features I wanted to include, and there were numerous unresolved issues with the app\u0026rsquo;s development. This drove me to participate in this competition, with a focus on creating another application to finish an overall optimized system for reducing e-waste. In the initial round, where I had to design a landing page for an e-commerce site, I immediately decided to develop a marketplace for trading refurbished devices. This solidified my determination to craft a comprehensive product idea for the subsequent rounds, even though I wasn\u0026rsquo;t certain of advancing beyond round 1.\nRecognizing the importance of further developing this idea, I dedicated significant time to meticulously designing wireframes for the project. However, due to personal circumstances, I found myself dangerously close to missing the round 1 deadline. Thankfully, only by submitting the final version just 30 minutes before the cutoff did I manage to make it on time (Pheww). You can access my completed assignment here. This also qualified me for entry into the next round. Additionally, I had the honor of being recognized as one of the top five Best Hipster participants in the competition because of my performance on this round.\nRound 2 - The bond and conflict arise # Upon receiving the email notifying me about the results and next steps, I also discovered that my old friend - Tu Linh - had also participated in the competition and qualified for the second round. The only difference was that he had chosen to join the Hacker track. After a brief discussion, I decided to join him along with two others: his friend Huy, and a randomly recruited girl named Minh Chau. While the three of us were from Hanoi and Minh Chau hailed from Son La, it was her who took the initiative throughout our collaboration (and also contributed the most). That was also the reason why she became a leader for my team. Well, she is really good at project management and business analytics, but underperforming in software development. That also the time I became the Software Development Lead\nThe initial meeting went smoothly as we all introduced ourselves and comfortably shared our ideas from the first round for brainstorming purposes. While I don\u0026rsquo;t want to come across as boastful, it became apparent to me that my assignment was the only one with the potential to win major awards. Huy and Linh from the Hacker Track supported my viewpoint. They agreed that my already beautiful interface and adherence to guidelines would significantly reduce the time spent on front-end development. However, Minh Chau had some reservations. She did like my idea, but she stated that it just so \u0026ldquo;ordinary\u0026rdquo; and \u0026ldquo;unsafe\u0026rdquo;. After several more meetings filled with debates and discussions, we eventually reached a consensus on what we should create for rounds 2 and 3: a counterfeit product verification system.\nYou might be wondering, is that all? Why were you so easily compromised? Well, I did attempt to negotiate with her. However, as mentioned earlier, choosing the idea of creating a marketplace for refurbished devices would result in my team being disqualified from the STEAM Hacks competition if they discovered any correlation or signs of idea copying from my Samsung product last year. On another note, my primary motivation for participating in this competition was not solely to complete my Samsung product but rather to acquire and expand my knowledge and skills.\nBuilding the entire application myself # I don\u0026rsquo;t want to point fingers or be too hard on myself, but it\u0026rsquo;s clear that none of the Hacker members on our team were pulling their weight. It\u0026rsquo;s not entirely their fault though; they excel in competitive coding, but they weren\u0026rsquo;t adequately prepared for web development or a hackathon like this. So, I found myself having to join them in tackling the code development.\nFurthermore, our team was currently facing a conflicting idea with our leader regarding the counterfeit product verification system. Unlike other student projects that typically involve education or mental health, this idea feels challenging for us to create something meaningful. Initially, our leader proposed using AI to detect fake patterns or incorrect labels, but our team lacks the competence in developing AI-related features. Additionally, implementing such a system seemed impossible and unproductive due to the vast variety of products and their unique fake versions. Moreover, there are also concerns about implementation costs and potential returns.\nWell, turns out, every cloud has a silver lining. Firstly, despite my limited technical knowledge at that time, I embraced the opportunity to learn backend development on my own, driven by a strong determination and purpose. This made me feel alive and empowered. Specifically, I was fortunate to have access to the FlutterFlow Education program, which enabled me to quickly learn and utilize their user-friendly software for creating meaningful applications. This greatly reduced our front-end development time and expanded my knowledge of this powerful tool. Furthermore, I acquired proficiency in Flask to develop an API backend application that automatically serves ML models trained through the Teachable Machine platform. This saved me the tedious task of manually researching and collecting data for training models.\nHowever, it is easier planning than implementing.\nExploring the Implementation Process # During the initial stages, I explored various potential implementations, each presenting its own set of hurdles. I want to make an Android application which have the Computer Vision feature on it. Initially, I came up with the idea of hosting a .Tflite model offline using Android Studio, which seemed promising, but the dependency management and debugging complexities proved daunting. Despite investing countless hours to resolve library errors, achieving a successful attempt remained elusive. Similarly, attempting to host a Transformer model on HuggingFace appeared overly complex, demanding a deep understanding of Transformer and Tensorflow intricacies.\nHowever, the breakthrough emerged with the decision to develop a Flask backend for a REST API server, specifically tailored to handle Teachable Machine exported models. This decision, inspired by a helpful tutorial I stumbled upon, paved the way for a more feasible and practical approach.\nLink of the tutorial: https://sogalanbat.medium.com/custom-api-for-keras-model-using-cloud-run-9d367a2ea5e8\nOvercoming Challenges and Constraints # Finding a way to accomplish my goal was a challenge, but I persevered. However, when the time came to put my plan into action, I realized it was much more difficult than anticipated. At the outset, my lack of prior knowledge in Machine Learning Model and Python Flask REST API posed a steep learning curve. Existing tutorials primarily focused on deploying entire Flask web apps or Tensorflow models, which didn\u0026rsquo;t directly address the Teachable Machine\u0026rsquo;s unique requirements.\nFurthermore, wrestling with outdated Python packages presented a considerable setback. Efforts to convert image URLs directly to tensor files proved unreliable, often yielding inaccurate results due to the limitations in handling diverse image file types. However, persistence and patience ultimately led me to discover a workaround. I learned how to download image URLs directly and convert them to the compatible Keras format, effectively resolving the earlier challenges.\nThe deployment phase, while initially daunting, became more manageable with the aid of a comprehensible Dockerfile tutorial. Thanks to this newfound understanding, I successfully deployed the application on Google Cloud Platform (GCP).\nDespite the successful implementation, certain limitations persist. The machine learning model\u0026rsquo;s inability to recognize products with 100% accuracy remains a challenge. Additionally, scaling the model to encompass a wider range of products proves challenging due to constraints in data resources. Although there were still flaws and room for improvement, we managed to qualify for the next round - the Hacking Day\u0026hellip;\nThe application cannot detect these physical patterns on real and fake products.\nAlthough it achieved a 70% success rate after conducting 400 tests, relying solely on this mechanism may not be deemed reliable.\nOverview of our app before the final round\nFinal revision and New trajectory # Entering the final round was unexpected for all of us. Still, we have to figure something out to improve the process of verifying counterfeit products.\nTo address these limitations and optimize the user experience, a novel approach has been conceived. Instead of relying solely on the machine learning model to identify differences between fake and authentic products, the application will facilitate user interaction with informative content. Users will receive guidance on how to physically examine the product, potentially by scanning barcodes or utilizing Optical Character Recognition (OCR) to decipher product numbers. The extracted data will then be passed through ChatGPT (LangChain) to source relevant internet pages based on credibility and popularity. A summarized report will then be relayed back to the user, empowering them to make informed decisions.\nThe day before the final, we had a meeting scheduled with our counselor and advisor. It was embarrassing because our team had barely finished the final version of our product. However, this gave us an opportunity to ask our advisor important questions regarding the challenges we were currently facing. As I delved deeper into NLP and specifically LangChain, I discovered that it was poorly developed and relied heavily on tokenization. After a lengthy discussion, we also learned some valuable techniques for optimizing it. And in the same day, we were able to complete our underperforming product, making it ready for the presentation scheduled for the following day.\nOn the day of our presentation, nothing out of the ordinary occurred. However, one memorable moment stands out in my mind. Our team made the decision to demonstrate the live usage of our application. Unfortunately, it didn\u0026rsquo;t go as planned and resulted in uproarious laughter from the audience. Despite this setback, we were able to complete the presentation without further complications (although our question and answer session left much to be desired). Although our team did not receive any awards in the competition, we still managed to capture a photo with smiles all around.\nHere\u0026rsquo;s a video demonstration of the app.\n","date":"20 October 2023","externalUrl":null,"permalink":"/posts/hackathon-steam-hacks-2023/","section":"Posts","summary":"\u003cp\u003eToday, I want to share my experience with you about the project I worked on during the Steam Hacks 2023 competition. Although my team did not win any major awards for this project, it was an incredible learning opportunity for me, particularly in terms of coding. Moreover, I also gained valuable insights into the collaborative process through working with my amazing new friend from Son La - Minh Chau.\u003c/p\u003e","title":"My STEAM Hacks 2023 experience","type":"posts"},{"content":"","date":"20 October 2023","externalUrl":"https://github.com/phuchoang2603/techburst-counterfeit","permalink":"/projects/steam-hacks-2023-techburst-counterfeit/","section":"Projects","summary":"This project aims to provide users with a reliable solution to identify counterfeit products. Originally designed as a C2C Ecommerce Platform, the project has evolved to focus solely on counterfeit detection. The system offers two methods for users to verify product authenticity: utilizing a Teachable Machine model and employing a barcode scanner.","title":"TechBurst – Revolutionary AI Solution for Identifying Counterfeit Products","type":"projects"},{"content":"It all started with a computer my mom saved from the trash heap at her office. It was a standard HP Prodesk 600 G4—nothing special, and since I already had a PC and a laptop, I had no idea what to do with it. Just installing Windows on it felt like a waste. I knew it could be something more than just another desktop collecting dust.\nThat simple idea kicked off a journey that completely changed what I thought I was passionate about and what I wanted to do with my life.\nMy First, Clumsy Steps # My first goal was to create my own private server to back up all my photos. I stumbled upon something called Xpenology, which is a clever way to run the software from those fancy, expensive Synology servers on any old computer. I spent a whole night fighting with it. It was a huge pain. The whole thing relied on a specific USB stick that had to be plugged in all the time to trick the software into working. It felt messy and unreliable.\nhttps://voz.vn/t/huong-dan-build-case-nas-xpenlogy-dsm-7-2-full-license-synology-surveillance-9-1-2-tan-dung-pc-cu.845346/\nFrustrated, I kept searching and found Proxmox. This was the \u0026ldquo;aha!\u0026rdquo; moment. Proxmox is an operating system built specifically to run other computers inside of it. Think of it like a digital nesting doll. It was stable, powerful, and let me create a virtual \u0026ldquo;computer\u0026rdquo; just for my photo backups. I could finally experiment and mess around without the fear of crashing everything. The beast was finally tamed.\nFinding a Real Purpose (With a Little Help from a GPU) # The server was running, but I knew it could do more. Things got really interesting when I found an old, barely-working GT 1030 graphics card. It was the kind of thing most people would throw out, but I stuck it in the PC, and suddenly, a whole new world of projects opened up.\nFirst, I built my own personal Netflix. I used a free program called Jellyfin to organize all my movies and TV shows into a slick interface. To get the media, I set up a few helper tools that people call the \u0026ldquo;arr stack\u0026rdquo; (Sonarr for TV, Radarr for movies) that automatically find and download stuff for me. That little graphics card did all the heavy lifting, converting video files on the fly so I could watch anything on my TV, laptop, or phone without any stuttering. If you want more detailed on how to properly set this up, here\u0026rsquo;s the link https://trash-guides.info/.\nNext, I got into AI before ChatGPT was cool. I found a program called Stable Diffusion that can create images from just text descriptions. I had a pretty juvenile idea for a Telegram bot: you send it a picture, which then trigger a web-hook to a script I wrote to identify people clothes and send it to the Stable Diffusion server, which then \u0026hellip; well, let\u0026rsquo;s just say \u0026ldquo;creatively redraw\u0026rdquo; the clothing. It was a ridiculous project born from a 16-year-old\u0026rsquo;s hentai brain, but it taught me a ton about programming and how to connect different services together. Here\u0026rsquo;s the repository if anyone interested https://github.com/phuchoang2603/telegram-sdwebui.\nBut the most meaningful thing I built was for my family. I connected our existing security cameras to a program called Frigate. Using the graphics card, Frigate is smart enough to tell the difference between a person walking up to our door and just a tree swaying in the wind. I then hooked Frigate into Home Assistant, which is like a central brain for smart home gadgets. Now, if someone is walking around our house late at night, my entire family gets an alert on their phones with a picture of what’s happening. It’s given us some real peace of mind, all powered by that old office PC.\nThe Obsession and the Path Forward # I was officially hooked. I found online communities like the r/selfhosted and r/homelab subreddits and discovered I wasn\u0026rsquo;t alone. There are tons of people out there building amazing things in their own homes.\nAt first, all my projects only worked inside my house network. But what if I wanted to watch a movie while I was out, or share my photo server with a friend? I learned how to use something called a Cloudflare Tunnel. It sounds complicated, but it\u0026rsquo;s a super secure way to let my projects be seen by the wider internet without opening up my home network to hackers.\nLooking back, it’s wild. This whole journey started with a free computer that was about to be scrapped. In the process of tinkering, I accidentally taught myself about networking, Linux servers, and coding. I wasn\u0026rsquo;t just building a server; I was discovering something I truly loved to do. It made me realize I didn\u0026rsquo;t want to be a UX/UI designer anymore. I wanted to be the person who builds the stuff that works behind the scenes.\nThat old HP Prodesk didn\u0026rsquo;t just find a new purpose; it gave me one, too.\n","date":"27 August 2023","externalUrl":null,"permalink":"/posts/self-hosted-initial-story/","section":"Posts","summary":"\u003cp\u003eIt all started with a computer my mom saved from the trash heap at her office. It was a standard HP Prodesk 600 G4—nothing special, and since I already had a PC and a laptop, I had no idea what to do with it. Just installing Windows on it felt like a waste. I knew it could be something more than just another desktop collecting dust.\u003c/p\u003e","title":"My First Server Was Office Trash: A Self-Hosting Story","type":"posts"},{"content":"","date":"9 November 2022","externalUrl":null,"permalink":"/tags/figma/","section":"Tags","summary":"","title":"Figma","type":"tags"},{"content":"This is my product created while participating in Samsung Solve for Tomorrow 2022, a STEM contest organized by Samsung Electronic Company that encourages problem-solving skills and showcases the positive impact of young minds on society, with over 76,000 participants nationwide\nDemo # User experience case study # Website # Built during the first round of STEAM Hacks 2023, Reware Website is a C2C marketplace inspired by platforms like eBay and Shopee. It enables users to buy and sell refurbished electronics and features self-serve kiosks for convenient device trade-ins. Reware promotes sustainable tech use by giving old devices a second life while offering users cash incentives.\n","date":"9 November 2022","externalUrl":null,"permalink":"/projects/sft2022-reware-app/","section":"Projects","summary":"\u003cp\u003eThis is my product created while participating in Samsung Solve for Tomorrow 2022, a STEM contest organized by Samsung Electronic Company that encourages problem-solving skills and showcases the positive impact of young minds on society, with over 76,000 participants nationwide\u003c/p\u003e","title":"Reware App – Ewaste Management Solutions","type":"projects"},{"content":"This is my diary about the trip to Sapa Hope Center with 10 colleagues from Summit Education - a renowned English language teaching center in Hanoi. Therefore, this trip is aimed at fostering cultural exchange and teaching English and basic skills to students from Grade 1 to Grade 2 in Sapa, Lao Cai.\nDay 1 # After many days of anticipation for new tasks and challenges – my first time as a cameraman – on the charity trip to Sapa with Summit, today finally arrived. I woke up quite early, having only slept for 3 hours the night before. I should have slept more, but I woke up at 3:40 am. After brushing my teeth, I fumbled on my computer to download videos on photography tutorials from YouTube to watch and learn on the way (I was assigned to take photos even though I\u0026rsquo;ve never held a camera before -))). I downloaded the videos and finished preparing my gear by 4:40 am, then called my dad to drive me.\nUpon arriving at the gathering point, I realized I was early af -)) However, that period was also when I started to play around and learn more about the camera. It was quite fascinating, to be honest, because like everything else, it has its own rules and standards that we can learn in a simple and understandable way. Having said that, after getting on the bus and starting to watch the videos I downloaded earlier, I only got a basic grasp of concepts like lens mm, aperture, shutter speed, ISO, \u0026hellip; And I haven\u0026rsquo;t learned anything about framing angles and positioning yet.\nAt Sapa Hope Center, the most surprising thing for me was the extremely warm welcome from the kids. Although many of them couldn\u0026rsquo;t speak Vietnamese (Sapa is a land of convergence for 5 different ethnic groups), they were enthusiastic and innocent. As soon as I arrived, the kids gave each of us a flower and a welcoming card, which they had drawn themselves. The drawings looked simple, but I believe they came from the sincere and innocent hearts of these little ones.\nDuring the self-introduction, I was quite surprised when the owner of the center turned out to be a young man (seemingly stepping out of \u0026ldquo;Silent Sapa\u0026rdquo;) who had studied abroad in Singapore. After visiting this place, he fell in love with it, despite numerous opportunities in the city. Even his relatives thought he was \u0026ldquo;charmed\u0026rdquo; (there is a local belief in enchantment being quite common here). He shared many stories about Sapa – the specially nutritious herbs, the underprivileged children lacking sufficient food, the uphill journey from home to school taking 2-3 hours each day, and more. Peter Thương, along with the kids at the center, made me feel moved every time I heard those genuine and simple stories.\nAfter having lunch and a break, we spent the entire afternoon getting to know each child individually to better understand their personalities. This helped us plan and manage the teaching program more effectively for the upcoming days. Despite having only known each other for a few hours, we engaged in drawing exercises, played games, and sang together, creating a strong bond. The room that day was filled with laughter.\nThe entire group introducing themselves and playing games\nThe older members teaching the children how to draw\nSharing and chatting together\nPlaying the \u0026ldquo;musical chairs\u0026rdquo; game together\nAfter finishing the activities, the sky had darkened, and we were quite tired from the day of travel. We decided to return to the hotel to rest.\nTaken at Praha Hotel, 86 Violet Street, Sapa\nDay 2 # On that morning, instead of going to the large activity hall like yesterday, our group had to go to a dining place to set up a projector to assist in teaching the children. The reason for this was that our goal for the day was to teach the kids English through themed movies and songs, requiring a dark room to make it easier for the children to see.\nDuring the morning teaching session, we played songs and shared stories about the movie Frozen, then guided the children to fill in blanks or answer questions about the movie content. However, it seemed that some of the children had already seen the movie before, and for some, it was a challenging task. Despite our efforts to maintain their interest in the first 1-2 hours, we couldn\u0026rsquo;t make that teaching session as effective as we hoped.\nAfter lunch, Chau Anh (from our group) decided to show the kids a famous Disney movie about three squirrels to regain their interest. However, after 15 minutes of screening, the kids seemed bored with the movie and began to leave the room. Faced with this situation, our group decided to stop showing movies and return to the large activity hall, directly interacting and teaching the kids how to create and play games.\nIndeed, the children here were too accustomed to watching movies whenever there was a foreign charity group visiting, so they had become disinterested in watching another movie. Instead, they enjoyed running and playing with everyone – something they rarely get to do when charity groups visit. Therefore, when given the opportunity to play freely that day, the adorable kids at Sapa Hope Center were very happy and cooperated completely with the older members, creating laughter for both sides.\nThe older members playing games designed by the children\nAfter the play session, we presented books, notebooks, and clothes to the kids Upon returning to the hotel, we held a meeting with the staff to address the issue of the \u0026ldquo;lesson plan burnout\u0026rdquo; that occurred that day. However, what I remember most from that evening was the conversation between me, Ms. Mai, and Chị Minh Hà\u0026rsquo;s mother. Ms. Mai shared a lot about the difficulties and experiences in understanding the psychology of teaching preschool and elementary school children. (There was one saying from her that I found very insightful: \u0026ldquo;It takes losing nine toes to finally realize we need to protect the remaining one.\u0026rdquo;) Chị Minh Hà\u0026rsquo;s mother also shared a lot about life in Russia.\nDay 3 # On that morning, we continued our teaching sessions for the children at Sapa Hope Center. The goal for our group on this day was to educate the children about personal hygiene skills through lively videos and practical exercises.\nHowever, unlike the previous day, this time, we decided to use the large activity hall (rather than the dining area, which was too dark and dirty by evening) for the lecture and for the convenience of teaching in a more dynamic manner (as discussed and planned at the hotel after reflecting on the previous teaching session). Indeed, moving to a spacious and clean room, as shown in the picture, significantly improved the comfort for both the teachers and the children.\nAdditionally, we must acknowledge the enthusiasm of the two \u0026ldquo;teachers,\u0026rdquo; Nhi and Linh, who brought interesting lessons on personal hygiene to the children. The two of them, along with the entire group, collaborated to guide the children in dancing to songs, creating artwork, and participating in related games. Later, the children practiced by taking fun quizzes, such as coloring objects corresponding to their daily hygiene items.\nWhile I wasn\u0026rsquo;t the main photographer for the morning session, I felt delighted to participate in a different role as a support person, helping the children complete their tasks in a joyful and easy manner.\nAfter completing the teaching session at the center, in the afternoon, our group planned to visit each child\u0026rsquo;s home to better understand their family situations and daily lives. At each home, we spent some time listening to the children\u0026rsquo;s stories and the family\u0026rsquo;s struggles. It was an opportunity to gain insight into the hardships they face and learn about the various professions within each household.\nReflection # It can be said that, although this trip is not too short or too long, it is sufficient to allow me to experience, try new things, and learn. There were many firsts for me on this journey: the first time participating in a charity trip without my parents, the first time having to plan and adapt to everything on my own, the first time being the one behind the camera capturing moments and actions throughout the trip, and more. In addition to the experiential lessons from these firsts, I also met and learned a lot from the children, older members, and staff. Everyone helped me engage and listen to many unique and diverse stories. It can be said that this trip will probably leave a deep memory during my high school years.\n","date":"30 June 2022","externalUrl":null,"permalink":"/posts/volunteer-sapa-hope-center/","section":"Posts","summary":"\u003cp\u003e\u003cem\u003eThis is my diary about the trip to Sapa Hope Center with 10 colleagues from Summit Education - a renowned English language teaching center in Hanoi. Therefore, this trip is aimed at fostering cultural exchange and teaching English and basic skills to students from Grade 1 to Grade 2 in Sapa, Lao Cai.\u003c/em\u003e\u003c/p\u003e","title":"Lesson Learned in Three-day Trip to Sapa Hope Center","type":"posts"},{"content":"This product emerged from my participation in the BASF Innovation Challenge, a national competition that empowers students to devise sustainability solutions, where we secured the 3rd Runner Up prize. While our aim was not the highest award, this competition established a robust foundation for me and our team to persist in researching and developing solutions to address the serious issue of e-waste\nPitching # Applications User Interface # ","date":"9 June 2022","externalUrl":null,"permalink":"/projects/basf-green-express/","section":"Projects","summary":"\u003cp\u003eThis product emerged from my participation in the BASF Innovation Challenge, a national competition that empowers students to devise sustainability solutions, where we secured the 3rd Runner Up prize. While our aim was not the highest award, this competition established a robust foundation for me and our team to persist in researching and developing solutions to address the serious issue of e-waste\u003c/p\u003e","title":"The Green Express – Initial Business Concept of Fostering Ewaste Reduction","type":"projects"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"}]