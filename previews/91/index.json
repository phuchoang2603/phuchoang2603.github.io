[{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/","section":"/home/fel1x","summary":"","title":"/home/fel1x","type":"page"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/arxiv/","section":"Tags","summary":"","title":"Arxiv","type":"tags"},{"content":"This is my story of how I attempt to build an AI-powered pipeline for ArXiv papers. It was a journey that started with a cool idea about AI agents and ended with me wrestling Docker, n8n, and Python into submission.\nphuchoang2603/sis-arxiv-vad-papers hosts arxiv papers on the topic \u0026ldquo;video anomaly detection\u0026rdquo; Python 2 0 Part 1: The AI Agent Dream (and Subsequent Nightmare) # My first idea was to build a smart AI agent using n8n that could talk to ArXiv. I wanted an LLM that could actually search for papers, download them, and even read them.\nThe Docker Model Context Protocol (MCP) looked like the perfect tool for this. I found an arxiv-mcp-server on GitHub and my first task was just getting the thing to build.\nBuilding the ArXiv Server # blazickjp/arxiv-mcp-server A Model Context Protocol server for searching and analyzing arXiv papers Python 1836 132 I found the repo above, which had already configured an mcp server just as I wanted. However, when I attempted to get it running, I found the original Dockerfile was\u0026hellip; a bit much. I figured I could simplify it using uv, since its official base images are clean and come with it pre-installed.\nMy new Dockerfile was way simpler:\n# Use a single Python base image with \u0026#39;uv\u0026#39; pre-installed FROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim WORKDIR /app COPY . . # Install the project and all its deps RUN uv pip install . --system # Run the server ENTRYPOINT [\u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;arxiv_mcp_server\u0026#34;] The \u0026ldquo;Stateless\u0026rdquo; Problem # With the server built, I set up a custom catalog.yaml above to define my tools and hooked it all into docker-compose.yml. A custom catalog is essentially your own personal, self-contained list of MCP servers. Instead of relying on a public registry, you define everything about the servers you want to use in a single catalog.yaml file.\nThe process is straightforward:\nCreate catalog.yaml: You define one or more servers in a YAML file following the specified format. Mount the Catalog: In your docker-compose.yml file, you use a volume mount to make your local catalog.yaml file available inside the gateway container. volumes: - ./catalog.yaml:/mcp/catalog.yaml Tell the Gateway to Use It: You use command arguments to point the gateway to your mounted catalog file and specify which server(s) from that file you want to activate. command: - --catalog=/mcp/catalog.yaml - --servers=duckduckgo registry: arxiv-mcp-server: title: \u0026#34;ArXiv MCP Server\u0026#34; description: \u0026#34;An MCP server that enables AI assistants to search, download, and read papers from the arXiv research repository.\u0026#34; type: \u0026#34;server\u0026#34; image: \u0026#34;arxiv-mcp-server:latest\u0026#34; tools: - name: \u0026#34;search_papers\u0026#34; - name: \u0026#34;download_paper\u0026#34; - name: \u0026#34;list_papers\u0026#34; - name: \u0026#34;read_paper\u0026#34; - name: \u0026#34;deep-paper-analysis\u0026#34; env: - name: \u0026#34;ARXIV_STORAGE_PATH\u0026#34; value: \u0026#34;/data\u0026#34; volumes: - \u0026#34;/mnt/storage/media/docs:/data\u0026#34; docker-compose file\nservices: n8n: image: docker.n8n.io/n8nio/n8n hostname: n8n ports: - \u0026#34;5678:5678\u0026#34; environment: - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true - N8N_HOST=${SUBDOMAIN}.${DOMAIN_NAME} - N8N_PORT=5678 - N8N_PROTOCOL=http - N8N_RUNNERS_ENABLED=true - WEBHOOK_URL=https://${SUBDOMAIN}.${DOMAIN_NAME}/ - OLLAMA_HOST=${OLLAMA_HOST:-ollama:11434} - TZ=${GENERIC_TIMEZONE} volumes: - ${APPDATA}/n8n/storage:/home/node/.n8n - ${SHARED_FOLDER}:/files restart: always mcp-gateway: image: docker/mcp-gateway hostname: mcp-gateway ports: - \u0026#34;8811:8811\u0026#34; command: - --servers=arxiv-mcp-server - --catalog=/mcp/catalog.yaml - --transport=sse - --port=8811 volumes: - /var/run/docker.sock:/var/run/docker.sock - ./catalog.yaml:/mcp/catalog.yaml I then wired it up in n8n with an AI Agent node, first trying a local qwen3 model (way too slow) and then switching to gpt-4o-mini (much faster).\nIt worked! The agent called the search_papers tool.\nThen, I hit the first major wall. The agent would download_paper, and on the very next step, the read_paper tool would fail.\nThe Problem: The MCP gateway, by default, is stateless. It spins up a brand new container for every single tool call. The container that downloaded the paper was instantly destroyed, so the new container for read_paper had no idea the file existed. Bruh.\nThe \u0026ldquo;Static\u0026rdquo; Mode Saga # The fix was static=true mode. This tells the gateway to connect to an already-running server container. I dutifully refactored my docker-compose.yml to have mcp-gateway depend on a long-running mcp-arxiv-server.\nmcp-gateway: image: docker/mcp-gateway hostname: mcp-gateway ports: - \u0026#34;8811:8811\u0026#34; command: - --servers=arxiv-mcp-server,duckduckgo - --static=true - --transport=streaming - --port=8811 depends_on: - mcp-arxiv-server mcp-arxiv-server: image: mcp/arxiv-mcp-server entrypoint: [\u0026#34;/docker-mcp/misc/docker-mcp-bridge\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;arxiv_mcp_server\u0026#34;] init: true labels: - docker-mcp=true - docker-mcp-tool-type=mcp - docker-mcp-name=arxiv-mcp-server - docker-mcp-transport=stdio volumes: - type: image source: docker/mcp-gateway target: /docker-mcp - ${SHARED_FOLDER}:/app/papers It failed.\nI tried the exact same setup with the official duckduckgo server, and it worked perfectly. My ArXiv server? Nothing. Just network errors.\nI was losing my mind, so I filed a GitHub issue. A collaborator saved me. Turns out, the gateway auto-prefixes the server name with mcp- to find the service.\nMy service was named mcp-arxiv-server. The gateway was looking for mcp-mcp-arxiv-server.\nI renamed my service to arxiv-mcp-server (so the gateway would find it at mcp-arxiv-mcp-server) and just like that, it worked. The agent could finally search, download, and read papers in one session.\nPart 2: The New Task - Batch Processing 300 PDFs # Right after that win, I had a meeting with Lokman Belkit. We shifted gears. I now had a folder of 300 PDFs he\u0026rsquo;d sent me. The priority was no longer the live agent, but batch-processing this existing data.\nI needed a good PDF-to-Markdown converter. LlamaParse looked amazing, but you have to negotiate a license. No thanks. I settled on Docling because it was open-source and had a docling-serve image.\nConfiguring docling-serve # This took way longer than I expected. docling-serve is great, but it needs to download all its models before it can run. I ended up creating a two-stage setup in my docker-compose.yml:\ndocling-serve-initial: A service that runs once (restart: \u0026quot;no\u0026quot;) and just runs the download command, saving the models to a shared Docker volume. docling-serve: The main server. It depends_on the initial service, mounts that same volume, and reads the pre-downloaded models. services: # To download the required models before first run docling-serve-initial: image: ghcr.io/docling-project/docling-serve-cu126:main command: - docling-tools - models - download - --all volumes: - ${APPDATA}/n8n/docling_artifacts:/opt/app-root/src/.cache/docling/models restart: \u0026#34;no\u0026#34; # For document parsing docling-serve: image: ghcr.io/docling-project/docling-serve-cu126:main hostname: docling-serve ports: - \u0026#34;5001:5001\u0026#34; environment: DOCLING_SERVE_ENABLE_UI: \u0026#34;true\u0026#34; NVIDIA_VISIBLE_DEVICES: \u0026#34;all\u0026#34; DOCLING_SERVE_ARTIFACTS_PATH: \u0026#34;/models\u0026#34; DOCLING_SERVE_ENABLE_REMOTE_SERVICES: \u0026#34;true\u0026#34; DOCLING_SERVE_ALLOW_EXTERNAL_PLUGINS: \u0026#34;true\u0026#34; deploy: # This section is for compatibility with Swarm resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - ${APPDATA}/n8n/docling_artifacts:/models runtime: nvidia restart: always depends_on: docling-serve-initial: condition: service_completed_successfully First, I tried to get n8n to load all the papers from the arxiv-existing folder Lokman sent me. It contained 56 items, and it took a really long time to load. Next, I started poking around the docling API. Before unleashing it on all 56 papers, I tested it out with just two to see what would actually happen.\nI first tried all of the features that docling offers:\npipeline (str). The choice of which pipeline to use. Allowed values are standard and vlm. Defaults to standard. do_table_structure (bool): If enabled, the table structure will be extracted. Defaults to true. do_code_enrichment (bool): If enabled, perform OCR code enrichment. Defaults to false. do_formula_enrichment (bool): If enabled, perform formula OCR, return LaTeX code. Defaults to false. do_picture_classification (bool): If enabled, classify pictures in documents. Defaults to false. do_picture_description (bool): If enabled, describe pictures in documents. Defaults to false. However, it took a ridiculous amount of time and memory to run, and it was prone to crashing. After digging around, I found out that other people were hitting this same issue. The formula and code awareness models, while small, will apparently eat your entire GPU VRAM. So I had to turn off all the enrichments.\nPart 3: The n8n Orchestration Nightmare # This was my first time really using n8n for a complex batch job, and honestly, it was frustrating.\nProblem 1: The ZIP File docling-serve returns a ZIP file with paper.md and an artifacts/ folder. n8n\u0026rsquo;s \u0026ldquo;Decompress\u0026rdquo; node doesn\u0026rsquo;t output a nice array you can loop over. It\u0026rsquo;s\u0026hellip; not an array. It\u0026rsquo;s a single item with multiple binary properties. After digging through forums, I found you have to use a custom Code node to manually split it:\n// This code is required to split a single item with multiple binary files // into multiple items, each with one binary file. let results = []; for (item of items) { for (key of Object.keys(item.binary)) { results.push({ json: { fileName: item.binary[key].fileName }, binary: { data: item.binary[key] } }); } } return results; Problem 2: Nested Loops are Buggy My next instinct was one giant workflow:\nLoop over all 50 PDFs. (Nested) Call Docling API. (Nested) Get the ZIP, run the code above. (Nested) Loop over the resulting files to save them. This failed miserably. Data from the first paper\u0026rsquo;s loop would \u0026ldquo;bleed\u0026rdquo; into the second paper\u0026rsquo;s execution. It would just process the first paper over and over.\nThe Solution: Master/Child Workflows The only stable solution was to refactor:\nMaster Workflow: Its only job is to loop through the 50 PDFs and call a \u0026ldquo;Child\u0026rdquo; workflow once per paper. Child Workflow: Receives one paper, calls Docling, saves the files, and finishes. This isolated the execution and finally worked. I also created a merge node to filter out previously processed items, in case I wanted to run the workflow again in the future.\nThis process might look easy, but it took me several hours. At first, I didn\u0026rsquo;t know the Edit Fields node existed, so I was comparing the filenames directly, without realizing the file extensions were different (silly me). I had to modify the items in the output node, using an expression to cut the .md extension and replace it with .pdf just to get the filter to work.\nIn the end, it processed 39 papers in 11 minutes (with table enrichment on). A success\u0026hellip; but it felt way too complicated.\nPart 4: The Final Pivot - From n8n Hell to Python # The next step was extracting metadata from these new Markdown files.\nI briefly tried pdfVector because of its JSON schema feature. Then I saw the price: 2 credits per page. A 25-page paper would cost 75 credits. It was a complete non-starter. The results was actually good tho. At least I can utilize the JSON schema of it.\n{ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Title of the research paper\u0026#34; }, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Type of the paper\u0026#34;, \u0026#34;enum\u0026#34;: [ \u0026#34;method\u0026#34;, \u0026#34;benchmark\u0026#34;, \u0026#34;dataset\u0026#34;, \u0026#34;application\u0026#34;, \u0026#34;survey\u0026#34;, \u0026#34;other\u0026#34; ] }, \u0026#34;categories\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A list of the paper\u0026#39;s methodology.\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [ \u0026#34;Weakly Supervised\u0026#34;, \u0026#34;Semi Supervised\u0026#34;, \u0026#34;Training Free\u0026#34;, \u0026#34;Instruction Tuning\u0026#34;, \u0026#34;Unsupervised\u0026#34;, \u0026#34;Hybrid\u0026#34; ] } }, \u0026#34;github_link\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Link to the GitHub repository (if available)\u0026#34;, \u0026#34;nullable\u0026#34;: true }, \u0026#34;summary\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Description of the novelty of the paper\u0026#34; }, \u0026#34;benchmarks\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A list of benchmarks used in the paper\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [ \u0026#34;cuhk-avenue\u0026#34;, \u0026#34;shanghaitech\u0026#34;, \u0026#34;xd-violence\u0026#34;, \u0026#34;ubnormal\u0026#34;, \u0026#34;ucf-crime\u0026#34;, \u0026#34;ucsd-ped\u0026#34;, \u0026#34;other\u0026#34; ] }, \u0026#34;nullable\u0026#34;: true }, \u0026#34;authors\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A list of the paper\u0026#39;s authors\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Publication date of the paper (YYYY-MM-DD)\u0026#34; } }, \u0026#34;required\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;type\u0026#34;, \u0026#34;categories\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;summary\u0026#34; ], \u0026#34;additionalProperties\u0026#34;: false } So I decided to roll my own json extractor on n8n. I added an \u0026ldquo;Information Extractor\u0026rdquo; node to my n8n workflow, using the JSON schema I\u0026rsquo;d built. Again, the local model disappointed me with inferior results.\nLocal qwen3-4b: Failed to call the tool. OpenRouter gpt-4.1-nano: Worked perfectly. The final nail in the n8n-coffin came when I tried to set up my Hugo site. To make images work, Hugo needs a \u0026ldquo;Page Bundle\u0026rdquo; structure:\n- content/ ----papers/ -------\u0026lt;paper-name\u0026gt;/ -----------index.md -----------artifacts/ ---------------image.png Trying to make n8n create this dynamic directory (\u0026lt;paper-name\u0026gt;) and rename the file to index.md was a joke. I was writing crazy expressions, using Execute Command nodes, Edit Fields nodes\u0026hellip; it was just a mess.\nI was so frustrated with how n8n handles basic file and path manipulation that I just gave up on it for orchestration.\nThe Final Architecture: Python as the Orchestrator # I threw away the complex \u0026ldquo;Master\u0026rdquo; n8n workflow and replaced it with a single Python script.\nThis new hybrid architecture is the best of all worlds:\nPython (main.py) is the \u0026ldquo;Master.\u0026rdquo; It\u0026rsquo;s simple, I can debug it, and it handles file I/O perfectly. Docker runs my services (docling-serve, n8n, mcp-server). n8n is now just a simple \u0026ldquo;serverless function.\u0026rdquo; It\u0026rsquo;s a single webhook that receives a file path, extracts the JSON, and sends it back. Here\u0026rsquo;s the new flow:\nPython script loops through all PDFs. Calls docling-serve (running in Docker) to get the ZIP. Unzips the files locally into the correct Hugo Page Bundle structure (\u0026lt;paper-name\u0026gt;/index.md). Calls the n8n webhook with the absolute path to the new index.md. The n8n workflow reads the file, extracts the JSON using the LLM, and sends the JSON back as the webhook response. Python receives the JSON, formats it with ruamel.yaml, and writes it as front matter to the top of the index.md. The script moves the original PDF to a done folder. This journey was\u0026hellip; a lot. But the final system is clean, robust, and a perfect example of using the right tool for the right job—even if it takes a few frustrating detours to find it.\n","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/arxiv-n8n-docling/","section":"Posts","summary":"\u003cp\u003eThis is my story of how I attempt to build an AI-powered pipeline for ArXiv papers. It was a journey that started with a cool idea about AI agents and ended with me wrestling Docker, n8n, and Python into submission.\u003c/p\u003e","title":"Building an AI-Powered ArXiv Pipeline: Thought n8n was the future, but not yet","type":"posts"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/mcp/","section":"Tags","summary":"","title":"Mcp","type":"tags"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/n8n/","section":"Tags","summary":"","title":"N8n","type":"tags"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"This repository contains the source code for the \u0026ldquo;SIS ArXiv VAD Papers\u0026rdquo; website, a Hugo static site using the Blowfish theme.\nThis project is a comprehensive platform for managing, processing, and displaying ArXiv research papers. It combines a Hugo static site with a powerful backend of containerized services for AI-driven PDF processing, metadata extraction, and ArXiv interaction.\nFeatures # ArXiv AI Agent: Includes an mcp-arxiv-mcp-server, which allows AI assistants to search, download, and read papers directly from the ArXiv repository. Automated PDF-to-Markdown: Uses the GPU-accelerated docling-serve to convert complex PDFs into clean Markdown. AI Metadata Extraction: A Python script orchestrates a pipeline that calls an n8n workflow to extract structured JSON metadata (title, authors, date, etc.) from converted text. YAML Front Matter: Automatically writes the extracted JSON back into the Markdown files as clean YAML front matter, making them ready to publish. Hugo Static Site: A clean, modern, and fast website built with Hugo and the Blowfish theme. Architecture \u0026amp; Services # The project\u0026rsquo;s backend is defined in the docker/compose.yml file and includes several key services:\nn8n: The workflow automation service. It is used here as an API endpoint (via Webhook) to run the AI metadata extraction pipeline. It is also used to connect the mcp-arxiv-mcp-server to integrate with an LLM model for searching and downloading the latest papers. docling-serve: A powerful, GPU-enabled service that handles the core PDF-to-Markdown conversion. It is pre-loaded with models via the docling-serve-initial service. mcp-gateway \u0026amp; mcp-arxiv-mcp-server: A service that provides an AI-readable interface to the ArXiv repository, allowing for programmatic searching, downloading, and reading of papers. Python Pipeline (scripts/): This is the \u0026ldquo;glue\u0026rdquo; that connects everything. It is a host-run script that: Finds new PDFs in an input directory. Calls docling-serve to convert the PDF to Markdown. Renames the output to index.md in a new content/papers/ bundle. Calls the n8n webhook with the path to the new index.md. Receives the extracted JSON metadata back from n8n. Writes this JSON as YAML front matter into the index.md file. File Structure # . ├── archetypes/ # Hugo new content templates ├── assets/ # Site assets (images, etc.) ├── config/ # Hugo configuration ├── content/ # The Markdown content for the site │ └── papers/ # \u0026lt;-- Processed, AI-enhanced articles land here ├── docker/ # Docker service definitions │ ├── compose.yml # The main Docker Compose file for all services │ └── catalog.yaml # Describes the ArXiv MCP service ├── scripts/ # The Python automation pipeline │ ├── config.py # Holds paths and API configs │ ├── main.py # Main script to run the pipeline │ ├── .env # (Not shown) Stores secret keys │ ├── pyproject.toml # Python project definition │ └── uv.lock # Python dependencies ├── static/ # Static files (favicons, etc.) ├── themes/ # Hugo themes │ └── blowfish/ └── hugo.toml # Main Hugo configuration file Setup \u0026amp; Installation # Clone the Repository:\ngit clone https://github.com/phuchoang2603/sis-arxiv-vad-papers.git cd sis-arxiv-vad-papers Configure Docker Environment: Create a .env file in the project\u0026rsquo;s root directory (next to docker/). This will provide environment variables to your compose.yml.\n# ./.env # -- Docker Services -- # MUST be an absolute path to your shared data folder SHARED_FOLDER=/path/to/your/shared/data # MUST be an absolute path for persistent Docker data APPDATA=/path/to/your/appdata/sis-arxiv # -- n8n -- SUBDOMAIN=n8n DOMAIN_NAME=your-domain.com GENERIC_TIMEZONE=America/New_York Configure n8n Workflow:\nStart your n8n instance and create your metadata extraction workflow. Start Node: Use a Webhook node. Authentication: Set to Header Auth and create a secure, random API key. Response Mode: Set to Respond at End of Workflow. This is critical for getting the JSON response back. Workflow: Add a Read Binary File from Disk node (using the path from the webhook), an Extract from File node, and your Information Extractor node. Activate: Click the \u0026ldquo;Active\u0026rdquo; toggle in the top-right. Copy: Copy the Production URL. Configure Python Pipeline: Create a separate .env file inside the scripts/ directory for the Python script.\n# scripts/.env N8N_WEBHOOK_URL=\u0026#34;https://n8n.your-domain.com/webhook/...\u0026#34; # \u0026lt;-- Your n8n PRODUCTION URL N8N_API_KEY=\u0026#34;your-secret-n8n-header-auth-key\u0026#34; Run Docker Services: Run this command from the project\u0026rsquo;s root directory:\ndocker-compose -f docker/compose.yml up --build -d This will build and start n8n, docling-serve, and the other services.\nInstall Python Dependencies: Navigate to the scripts directory and use uv to install:\ncd scripts uv sync How to Use the Pipeline # Add PDFs: Place your .pdf files into the input directory defined in scripts/config.py. (By default, this points to ../../arxiv_existing/test, which is a directory sibling to your project folder). Run Pipeline: cd scripts python main.py Check Output: Watch the terminal as the script processes each file. Your new content bundles, complete with index.md and YAML front matter, will appear in content/papers/. Preview Site: cd .. # Return to the Hugo root hugo server Your site will be available at http://localhost:1313. License # This project is licensed under the MIT License.\n","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/projects/sis-arxiv-vad-papers/","section":"Projects","summary":"\u003cp\u003eThis repository contains the source code for the \u0026ldquo;SIS ArXiv VAD Papers\u0026rdquo; website, a \u003ca\n  href=\"https://gohugo.io/\"\n    target=\"_blank\"\n  \u003eHugo\u003c/a\u003e static site using the \u003ca\n  href=\"https://blowfish.page/\"\n    target=\"_blank\"\n  \u003eBlowfish\u003c/a\u003e theme.\u003c/p\u003e","title":"SIS ArXiv VAD Papers","type":"projects"},{"content":"","date":"2 November 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"30 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/series/on-premise-101/","section":"Series","summary":"","title":"On-Premise 101","type":"series"},{"content":"In the previous part, we covered how I started playing around with my hand-me-down computer and then escalated to building a whole 3-node cluster. It\u0026rsquo;s been a 3-year journey, and there have been so many changes to the software stack I host. But no matter what, there\u0026rsquo;s one thing I\u0026rsquo;ve used consistently from beginning to end: Proxmox (Hypervisor). So before we dive into how I deploy my applications on Docker or Kubernetes, I\u0026rsquo;ll show you the glue that connects the hardware we just built to all the software we\u0026rsquo;re going to deploy.\nWhy Proxmox? The Magic of Type 1 Hypervisors # Many of you have probably heard the term Virtual Machines (VMs), the ones that programs like VirtualBox and VMWare create. Those programs, which are often called a Type 2 Hypervisor, let you basically run a whole Guest OS (i.e., Windows, Linux, \u0026hellip;) on top of your current one (which is called the Host OS). Because it doesn\u0026rsquo;t actually use the machine\u0026rsquo;s hardware resources directly - it goes through a translation layer in the Host OS - it lets you do anything inside without worrying about the machine being compromised. Therefore, it\u0026rsquo;s frequently used for testing out dangerous programs like Honorlock online exam proctoring (oops, don\u0026rsquo;t arrest me prof Eggers). It runs fine with basic, lightweight programs, sure. But if you want to push it harder to run heavier ones, it might not utilize all the potential performance the machine has.\nThat\u0026rsquo;s where the Type 1 Hypervisor comes in. It can still create Virtual machines like above. But right now, notice it no longer has the Host OS between it and the Hardware resources. That means it can now directly access the full performance of the machine, without relying on the Host OS\u0026rsquo;s translation layer, making the Virtual machine running on top of it much more efficient. Maybe you don\u0026rsquo;t know this, but when you enable Windows Subsystem for Linux 2 (WSL2), Windows re-configures itself to run on top of its own Type 1 Hypervisor called Hyper-V. WSL2 then runs as a separate, lightweight VM directly on that same hypervisor, side-by-side with Windows, providing full system call compatibility. Other examples include VMWare ESX or ESXi, and notably, Proxmox.\nProxmox is an open-source virtualization management solution that uses KVM to manage virtual machines and LXC to manage containers. It is a Type 1 Hypervisor that built on a Debian-based Linux distribution, and is managed through a central web interface for tasks like clustering, high availability, storage, and networking.\nSo that explains Proxmox. For me, the biggest benefit of using Proxmox was to easily pass through computer resources to bypass even the Hypervisor layer itself, eliminating significant overhead and allowing the VM to receive near-native hardware performance. This allows me to pass through a GPU into a VM to run inference on LLM models at the highest VRAM usage and computation; or pass through HDDs to a VM to use as a NAS (more on this in the next part). Additionally, as mentioned above, you get a beautiful web interface to manage the VMs and other useful features, so it has the potential to replace even Kubernetes or TrueNAS. However, I prefer specialization over convenience. I\u0026rsquo;d rather use a dedicated tool that does one job perfectly than a single, all-in-one tool that does multiple jobs just \u0026lsquo;okay\u0026rsquo;.\nTo demonstrate the ability of Proxmox, I will show you a guide to create a VM that is capable of directly inheriting a Nvidia GPU from the host machine, allowing the applications running inside the VM to utilize its fullest performance. However, note that this is just one of many examples you can do with Proxmox. In the next part, I will show you how to even virtualize a NAS device on it, or provision multiple VMs in one command.\nGetting Started: My Proxmox Post-Install Toolkit # I\u0026rsquo;m not going to cover the in-depth guide to install Proxmox as it\u0026rsquo;s similar to installing any other OS: just get the ISO file, burn it to a USB, turn off the computer, boot from the USB, and follow the installation guide (the default options are pretty solid). The settings they ask you to configure from the beginning can also be changed later after installation. So no need to be afraid of anything.\nNext, I will show you commands that I actually use after installing Proxmox, as I believe they are really useful.\nThis script provides options for managing Proxmox repositories, including disabling the Enterprise Repo, adding or correcting PVE sources, enabling the No-Subscription Repo, adding the test Repo, disabling the subscription nag, updating Proxmox VE, and rebooting the system.\nbash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/community-scripts/`Proxmox`VE/main/tools/pve/post-pve-install.sh)\u0026#34; To create a new Proxmox VE Ubuntu 24.04 VM, run the command below in the Proxmox Shell. Note that if you want to use pass-through features like a GPU, you must select q35 as the machine type, host as the CPU type, and make sure not to start the machine after installation. In the cloud-init tab, change the username, password, network address, and SSH public key so you can SSH into it later.\nbash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/community-scripts/`Proxmox`VE/main/vm/ubuntu2404-vm.sh)\u0026#34; This script is fantastic because it creates a cloud-init template, which is much faster than manually installing from an ISO. It lets you pre-configure your username, password, and networking, so the VM is ready to go in seconds. It\u0026rsquo;s the perfect way to spin up new VM s quickly, especially when you plan to make more for a Kubernetes cluster later. If you want more useful scripts like this, feel free to hop over to here.\nPassing a GPU to a VM # Firstly, we don\u0026rsquo;t want the Proxmox host system utilizing our GPU(s), so we need to blacklist the drivers. I\u0026rsquo;m not gonna provide the whole full guide, as it\u0026rsquo;s a really long process and requires you to know your system to adjust accordingly. But here\u0026rsquo;s the ultimate guide that provides detailed explanations and steps for doing so.\nSecondly, after we verify the GPU was passed through successfully, we need to set up the VM to receive it:\nAfter Ubuntu has finished installing and is reachable by SSH on your network, shut down the `VM and go to the \u0026ldquo;Hardware\u0026rdquo; tab. Click \u0026ldquo;Add\u0026rdquo; \u0026gt; \u0026ldquo;PCI Device\u0026rdquo;. Select \u0026ldquo;Raw Device\u0026rdquo; and find your GPU. Click the \u0026ldquo;Advanced\u0026rdquo; checkbox, \u0026ldquo;All Functions\u0026rdquo; checkbox, and \u0026ldquo;PCI-Express\u0026rdquo; checkbox, then hit Add. Start the VM again and type lspci in the console. Search for your GPU. If you see it, you\u0026rsquo;re good to go! Thirdly, we need to install the drivers inside the VM so that the software can utilize the GPU correctly. There are multiple ways to do this, but I always refer to this site to install.\nsudo apt update \u0026amp;\u0026amp; sudo apt install ubuntu-drivers-common # List drivers sudo ubuntu-drivers list --gpgpu # Let’s assume we want to install the `570-server` driver (listed as `nvidia-driver-570-server`): sudo ubuntu-drivers install --gpgpu nvidia:570-server #You will also want to install the following additional components: sudo apt install nvidia-utils-570-server # After that, reboot your system sudo reboot # Once the machine is back up, check to be sure your drivers are functioning properly nvidia-smi Installing Docker (And Why I Use a VM, Not LXC) # Note that Proxmox also has LXC, as an alternative to Docker. While LXC offers better performance, the software support and ease of use aren\u0026rsquo;t as good as Docker. Not to mention it is harder to pass through a full physical device (like a GPU) to an LXC container, because by design, it\u0026rsquo;s tightly integrated with the host kernel.\nTherefore, I only use LXC for critical software that needs to stay active alongside the machine, such as Tailscale or Cloudflared. For most other things, I use a VM with Docker to experiment and test. If you want to install Docker and Nvidia Container Toolkit on the VM to help containers leverage your GPU, you\u0026rsquo;ll want to run these commands:\n# Add Docker\u0026#39;s repostiory to apt sources sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \\ \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null ## Add Nvidia Container Toolkit\u0026#39;s repostiory to apt sources curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list ## Install sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin sudo apt-get install -y nvidia-container-toolkit # Use docker without sudo, need to log out then back in to apply this sudo usermod -aG docker $USER # Configure the container runtime by using the nvidia-ctk command and restart docker sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker After successful installation, this will test to make sure that the NVIDIA container toolkit can access the GPU correctly.\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi # You should see the same output as running nvidia-smi without Docker. Summary \u0026amp; What\u0026rsquo;s Next: Prepping for TrueNAS # And that finishes the whole step of setting up a VM on top of a Type 1 Hypervisor to use an Nvidia GPU with near-native performance. Now you can go ahead and install any Docker containers you want.\nBut I\u0026rsquo;m not doing that right away, because I want those containers to use NFS storage. That way, even if the VM stops working correctly, I can still spin up a new one without losing my persistent application data. This approach is also helpful if you want to migrate from Docker to Kubernetes, as it allows you to reuse the data by just pointing to the correct NFS path. To do this, we have to set up TrueNAS, a network-attached storage platform.\n","date":"30 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/on-premise-hypervisor/","section":"Posts","summary":"\u003cp\u003eIn the previous part, we covered how I started playing around with my hand-me-down computer and then escalated to building a whole 3-node cluster. It\u0026rsquo;s been a 3-year journey, and there have been so many changes to the software stack I host. But no matter what, there\u0026rsquo;s one thing I\u0026rsquo;ve used consistently from beginning to end: \u003ccode\u003eProxmox\u003c/code\u003e (Hypervisor). So before we dive into how I deploy my applications on Docker or Kubernetes, I\u0026rsquo;ll show you the glue that connects the hardware we just built to all the software we\u0026rsquo;re going to deploy.\u003c/p\u003e","title":"On-Premise 101 (part 2): Proxmox, the 'Glue' for My Homelab","type":"posts"},{"content":"","date":"30 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/proxmox/","section":"Tags","summary":"","title":"Proxmox","type":"tags"},{"content":"","date":"30 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"You might have heard about the recent AWS outage that caused many services to go down. To think that half of the internet relies on a single service is crazy, even if it might be the best in its field. The engineers at AWS have done many things to prevent this, such as high availability, multi-region, zones, etc., but if a critical part of it still goes down, the whole thing goes down too. Therefore, if you don\u0026rsquo;t want to give all your money and trust to a single company, you might want to explore the on-premise option, which is self-hosting.\nBut deploying on-premise isn\u0026rsquo;t that easy, though. You have to spend time, effort, and money to configure the system for yourself—from PC specs, network performance, and security, to virtualization. All of that is not a one-day setup. But it also gives you the power to manage everything yourself, too.\nSo if you want to start exploring and build an infrastructure like the one I\u0026rsquo;m currently hosting, here\u0026rsquo;s the guide for you to do so. This is the first part of the series On-Premise 101. In this part, I\u0026rsquo;ll share my whole journey from the beginning: how I acquired the hardware, the decisions I made, and finally, how I wired it all together with networking to get it ready for the software.\nIn this part, you\u0026rsquo;ll see me refer to Proxmox a lot and might ask, \u0026ldquo;What the hell is it?\u0026rdquo; For now, just think of it as the platform that allows me to create virtual machines. I will cover it in-depth in the next part.\nThe Beginning: My First SFF Server and Its Limits # Looking back, I started this journey 3 years ago. At the time, I had an HP Prodesk 600 G4 SFF that my mother gave me; she picked it up from the warehouse at her office. It was an old computer, so I didn\u0026rsquo;t know what to do with it at the time, since I already had a PC that I could do anything with. But when I was browsing my YouTube feed, I saw a video talking about how a creator used his Synology NAS (Network Attached Storage) to replace Google Drive and help him collaborate with his video editors easily.\nI thought it was so cool that you could transfer a file that big from one computer to another, without any wires, almost instantly. I checked the specs of the NAS and found out it was nothing powerful—just a bunch of HDDs and storage. That\u0026rsquo;s when I realized I could probably do all of that with my old computer, too.\nIf you want to read what I was doing with my old computer at that time, feel free to read my initial self-hosting story.\nThe specs of my PC weren\u0026rsquo;t really powerful, but its iGPU was capable enough to run Frigate (an NVR camera detection service) and Jellyfin (for video transcoding) pretty well. Later on, I tried to upgrade the PC specs , such as plugging in a graphics card for LLM models and upgrading the RAM to 16GB. But since it was a Small-Form-Factor (SFF) PC, it had limited upgrade options.\nFirst, I tried plugging in a GTX 1030, and although it worked, I had to remove the side panel to make room for it. Second, I tried to modify the HDD holder to fit two SATA HDDs in the case, but after running it for a while, I noticed it got really hot because the two HDDs were stuck so close together. So, I was tempted to look for another PC.\nLesson Learned: Small Form Factor (SFF) PCs are a great, cheap way to start, but their limited space for upgrades (especially for GPUs and storage) means you might outgrow them fast.\u0026quot;\nThe First Big Upgrade: The Lenovo P520 Workstation # After hitting the bottlenecks of my computer and having to move to the US for my undergrad studies, I had to leave my entire homelab back in Vietnam. I could still access it via Tailscale (a VPN service, which I\u0026rsquo;ll share more about later), but the latency was really high, so I also wanted to buy a new one. Actually, at that time, ChatGPT and LLMs were also getting popular, so I wanted to get a GPU to help me \u0026ldquo;study\u0026rdquo; more in that field. Since I\u0026rsquo;m also a student, I set my budget to under $500 total.\nBrowsing through eBay, YouTube, and Reddit for suggestions, I settled on the Lenovo P520 as the PC. Initially, I wanted to build my own PC, but I realized that if I bought all new hardware, it would cost much more than $500, and the specs wouldn\u0026rsquo;t be powerful enough. So I wanted to choose pre-built enterprise hardware, similar to my HP Prodesk 600 G4 above. Instead of an SFF, I wanted a Full-Tower this time so that I could upgrade it more easily later on. There were options like the Dell Optiplex, HP Prodesk, HP Elitedesk, etc., but I decided to choose Lenovo because of its price-to-performance ratio. My favorite YouTuber, Hardware Haven, also released a video on this PC.\nWith only $274, I got:\nCPU: Intel Xeon W-2123 (4 cores, 8 threads, 3.60 GHz)\nRAM: 32 GB of 2666 MHz ECC DDR4\nStorage: 1TB SATA SSD and 4TB HDD\nAnd a bunch of ports and upgrade options on the motherboard\nAs for the GPU, there were also lots of options to choose from, but I felt the Nvidia RTX 3060 and Nvidia Tesla P40 were the strongest candidates. On one hand, the Nvidia Tesla P40 attracted me because it has 24GB of VRAM, and as you may know, VRAM capacity is king in the LLM world. But I was afraid the outdated software support would cost me in the future since it\u0026rsquo;s a card that was released nearly 10 years ago and uses the Pascal architecture. The RTX 3060, on the other hand, has 12GB of VRAM, was launched recently, and has active CUDA support, although I can\u0026rsquo;t run some of the largest LLM models on it. But with a price of only $200, and considering the fact that I might occasionally game on it, I decided to settle on the 3060.\nI also upgraded it to 64GB of ECC RAM with two purchases of 16GB sticks on eBay, because I also planned to use this machine as a NAS server (TrueNAS loves ECC RAM !). Later in the year, I also 3D-printed an HDD bay (the green part in the bottom right corner of the image below). This allowed me to install two additional 1TB HDDs, bringing the total NAS storage to 6TB.\nThe \u0026ldquo;Free PC\u0026rdquo; and My Dream of a 3-Node Cluster # A year later, which is just recently, I also received a new PC (notice I said \u0026ldquo;received\u0026rdquo;). The story of how I got this PC is really surprising. In the summer of 2025, I needed to go back to Vietnam to visit my family. Since it was a short trip, I decided not to bring the Lenovo P520 with me because it was really clunky, and at home, I still had the HP Prodesk 600 G4. But that summer was also the first time I learned Kubernetes and I really wanted to experiment with it. So I ran those experiments on the HP Prodesk 600 G4 with multiple virtual machines, and planned to upgrade to multiple physical nodes.\nWhen the summer ended, I planned to bring the HP Prodesk 600 G4 with me to the US. But unfortunately, after the flight, my machine would no longer boot up. Since I didn\u0026rsquo;t bring a monitor or any cables with me, I couldn\u0026rsquo;t troubleshoot the issue myself. I decided to bring it to the IT team at my school to hopefully fix it. After some back-and-forth discussion, they reached the conclusion that my machine had faulty RAM issues, and they only needed to plug out two 4 GB RAM sticks to boot it up normally.\nWhen I went to their office to pick up my device, I saw a bunch of old Dell Optiplex machines lying around, so I just asked what they planned to do with them. They answered that they were replacing them with newer devices for the library and planned to trash or give them away. I asked if I could have one for \u0026ldquo;study reasons,\u0026rdquo; and they happily gave me a Dell Optiplex 7020, and even threw in a Dell monitor. That\u0026rsquo;s the happiest day of my entire month since the HP Prodesk decided to leave me; not only I got it relived but I also welcomed a new one to my family ! The specs of the machine are:\nCPU: 3.6 GHz Intel Core i7-4790 Quad-Core\nRAM: 16GB of 1600 MHz DDR3\nStorage: 1TB HDD\nSo right now, I have 3 PCs, which is perfect for me to set up the Proxmox 3-node cluster I had dreamed of in the summer. But after a month of testing with it, I realized two of the nodes (the HP Prodesk and Dell Optiplex) constantly went offline. After unplugging and replugging the LAN cable, it would connect again, but only for a very short period. I figured this must be a networking bottleneck. And then, the upgrading journey continued.\nThe Bottleneck: Fixing a Failing Cluster with a Dedicated Network # If you want to know more about the background of my network design for my homelab, feel free to read this post.\nFor a long time while working with my homelab, I only had a single PC at one place at the time: the HP Prodesk when I was in Vietnam, and the Lenovo P520 when I was in the US. Therefore, I always connected the LAN cable from the PC directly to the router. But since I now have 3 PCs together at one place and plan to add a PS4 for remote gaming, relying on the ports on the router wouldn\u0026rsquo;t allow me to have more than 3 devices connected at the same time. Because of this, I got myself a basic 1 Gbps switch with 8 additional ports back in the summer.\nThe interesting thing about a switch is that not only does it expand your LAN ports, it\u0026rsquo;s also capable of helping the computers talk directly to each other, without having to go through the \u0026ldquo;middle-man\u0026rdquo;—the router. Think of it as a simple roundabout that connects all the local streets, with the big highway being the router. You can go from one street to another easily, and you can also get on the highway if you want to. This offloads the job from the router (which is like a toll booth on the highway), improving internal throughput and performance.\nHowever, with only one LAN port per computer, all my traffic was forced onto the same network. This means the high-speed Proxmox cluster communication (checking if nodes are online), VM traffic, and any storage traffic were all competing on the same, single 1Gbps \u0026ldquo;road.\u0026rdquo; This congestion was the root cause of my problem: critical cluster packets were being dropped, causing my nodes to think their partners were offline. Thanks for this comment from North Idaho Tom Jones on the Proxmox forum, I was able to find the solution:\nIMO - some things to look at your cluster communications :\n- How fast are your physical network interfaces ( 100-Meg , 1-Gig , 10-Gig , something faster ).\nThere is a possibility your interfaces might be busy moving I/O traffic to/from your VMs , and you don\u0026rsquo;t have the network additional I/O capacity for the cluster to communicate to the other cluster(s).\n- Are you performing backups when the cluster(s) drops ?\n- Do you have interface errors and/or packet drops on your physical \u0026amp; virtual ethernet interfaces ( and your external switch(es) ).\n- What is your current I/O bandwidth rate when you drop a node in your cluster ?\nI have a Proxmox network with 14-Clusters and 6-external NFS systems for my VM hard disk storage. I have 10 \u0026amp; 40 Gig network cards. My cluster IPs and my NFS IPs do not share any IP address space with my VMs \u0026mdash; My VMs \u0026amp; my Cluster IPs \u0026amp; my NFS IPs are on unique IP networks. I do this to keep unwanted/un-needed network chatter down to a minimum. I have never had a node drop out of the cluster , even with pushing 20+ Gig on multiple nodes in the cluster at the same time while all nodes are doing a backup at the same time.\nFrom this, I decided to upgrade my network infrastructure again, finding new network cards to expand the LAN ports on my PCs. I initially looked at 10 Gig, but after seeing the price of both the network cards and the switch, I immediately retreated. Although my switch is only 1 Gig, I didn\u0026rsquo;t plan to replace it right now because even 2.5 Gig switches are so expensive. So I opted for 2.5 Gig network cards for each computer, with the plan to upgrade the switch later.\nAt first, I was just looking for the cheapest network card. After searching on Amazon, I found this one, which is only $8 for a 2.5 Gig PCIe network card. I immediately bought 3 of them. When they arrived, I found out that the PCIe connection here is an M+B key, which is a totally different connector than an NVME drive or a standard PCI x1 slot. It\u0026rsquo;s a rare port for a PC to have, as it\u0026rsquo;s usually used for WiFi cards or Coral TPU accelerators.\nNeedless to say, only the HP Prodesk had this port, and it didn\u0026rsquo;t even look nice when plugged in (see the extra cord that\u0026rsquo;s totally detached from the case in the image above). So I had to return two of the network cards and buy two more. This time, I learned that I needed to find a specific PCI x1 network card, so I filtered carefully and bought this one. After they arrived, they fit nicely into the cases.\nThe next step, after wiring everything and connecting it to the switch, was to go into Proxmox to verify the PCs recognized the cards. Then I created a new subnet (192.168.69.0/24) just for these 3 PCs so they could talk to each other directly for cluster communication. After that, I changed the /etc/hosts file on each node so they would use these new IPs to communicate with each other. And after everything was done, voila! The cluster is now stable, and each node has two different addresses: one for the main network (via the router) and one for direct, high-speed communication with other nodes.\nWhat\u0026rsquo;s Next: Building the Software Stack (NAS, Kubernetes, and ArgoCD) # And with that, the hardware journey is complete. It\u0026rsquo;s been a fantastic learning experience, evolving from a single, hand-me-down HP Prodesk to a powerful 3-node Proxmox cluster.\nThis process taught me some valuable lessons:\nStart with what you have: That first SFF PC was the perfect, low-cost entry point into self-hosting.\nPlan for your bottlenecks: I quickly hit the physical limits of SFF (power, cooling, and PCIe slots). Moving to a full-tower workstation like the Lenovo P520 was the right call, giving me room for a real GPU and more storage.\nNetworking is critical for clusters: A \u0026ldquo;stable\u0026rdquo; cluster isn\u0026rsquo;t just about the PCs; it\u0026rsquo;s about the network connecting them. My nodes kept dropping offline due to network congestion on a single 1Gbps link. Adding dedicated 2.5GbE cards for a private cluster subnet (192.168.69.0/24) made the cluster perfectly stable.\nNow that the physical foundation is laid, it\u0026rsquo;s time to build on top of it. In the next part of this series, I\u0026rsquo;ll dive into the software that brings this hardware to life. We\u0026rsquo;ll cover:\nCreating a ZFS-based NAS: How I\u0026rsquo;m using TrueNAS (or Proxmox\u0026rsquo;s built-in ZFS) to manage my 6TB of storage.\nSpinning up the Kubernetes Cluster: We\u0026rsquo;ll provision the VMs that will form our 3-node k8s cluster using Terraform and Ansible with this project.\nAutomating Everything with GitOps: Finally, I\u0026rsquo;ll show you how I use ArgoCD to automatically deploy and manage all my applications (Jellyfin, Frigate, and more) on Kubernetes.\nStay tuned!\n","date":"28 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/on-premise-hardware/","section":"Posts","summary":"\u003cp\u003eYou might have heard about the recent AWS outage that caused many services to go down. To think that half of the internet relies on a single service is crazy, even if it might be the best in its field. The engineers at AWS have done many things to prevent this, such as high availability, multi-region, zones, etc., but if a critical part of it still goes down, the whole thing goes down too. Therefore, if you don\u0026rsquo;t want to give all your money and trust to a single company, you might want to explore the on-premise option, which is self-hosting.\u003c/p\u003e","title":"On-Premise 101 (part 1): Building a 3-Node Proxmox Cluster","type":"posts"},{"content":"","date":"28 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/self-hosted/","section":"Tags","summary":"","title":"Self-Hosted","type":"tags"},{"content":"A long time ago, my house only had 1 router, located on the third floor. So whenever we needed to access the internet while having dinner on the first floor, we needed to enable 4G or walk near the stairs just to receive a fraction of the WiFi signal. My father decided to give me an important task, which was to figure out how to solve this issue. I initially bought a simple WiFi extender located on the second floor, but after using it for a month, even though we could receive a strong signal from it, we couldn\u0026rsquo;t access the internet because the signal from the router on the 3rd floor to the WiFi extender on the 2nd floor was still really weak.\nThat\u0026rsquo;s when I decided to ask my dad to invest in a powerful second router located in my room on the 2nd floor, that could connect to the master router via LAN cable and advertise a strong WiFi signal. Researching multiple options, I opted for the Xiaomi CR6608 as it had impressive specs (WiFi 6, 4 antennas, gigabit) and most importantly, it could install OpenWRT. This opened a whole new world for me, because after 3 years, it has taught me many things about networking and the internet, such as subnets, IPv4, DHCP, DNS, and VPNs.\nThis post is a tour of my home network, built on that OpenWRT router. I\u0026rsquo;ll walk through how I designed my network layout, solved DNS and ad-blocking, and built a secure, globally-accessible lab using Tailscale and a cheap VPS.\nPlanning the Network # Currently, this router is configured as a cascaded router behind another router (double NAT). So it is also a DHCP server, which subnet is 10.69.0.0/16, to give IPs to the client devices. It has the total capacity of giving 256*256=65536 IPs:\nPhysical devices (fixed): 10.69.0.1 to 10.69.0.255 (255 ips) Proxmox VMs: 10.69.1.1 to 10.69.1.255 (255 ips) Kubernetes load balancers: 10.69.2.1 to 10.69.2.255 (255 ips) Other WLAN client (dynamic): 10.69.254.1 to 10.69.255.255 (510 ips) I also plan to learn how to set up VLANs for additional security and a separate environment, but that requires a managed switch, which is kinda expensive. For now, it has done an awesome job of being a router, giving IPs and helping the clients connect to the internet. But it also needs to provide DNS for the client devices too.\nBlocking Ads and Local DNS # On the DNS side, I initially used an AdGuard local server hosted directly on the Xiaomi CR6608 to provide DNS for clients. But after running it for a while for my family, I found out that my device couldn\u0026rsquo;t hold up long when multiple devices trying to connect; it kept restarting after experiencing too much load. And also, when I wanted to go out, if I wanted to get the same DNS setup as when I was at home, I needed to configure a lot more to get it working.\nSo in the end, I chose to use NextDNS as my primary DNS solution. With luci-app-nextdns in the OpenWRT packages, my router gets the profile ID for my NextDNS profile and then advertises it so that every client can also have it. What\u0026rsquo;s more, I can still configure my iPhone to have an HTTPS DNS Profile for NextDNS when I go outside.\nAdditionally, it also has the capacity to block ads, parental controls, and blacklist or whitelist any domain you want. This feature is extremely useful if you want to limit yourself from doom-scrolling Facebook, YouTube, or any other services while you\u0026rsquo;re at work, and you can get access back when you\u0026rsquo;re not. It can also provide analytics and logs, which give you more monitoring options.\nAccessing My Network from Anywhere # But using DNS for my local server would not give me immediate access to my server at home. I needed a way to access my local IP while I am outside. Moreover, since my internet modem is also behind a NAT, opening port forwarding would have no effect. After researching, I found these were the options that could help me:\nFeature WireGuard Cloudflare Tunnel Tailscale How it Works Direct Client-to-Server VPN. Your server at home listens for connections. An agent at home sends an outbound connection to Cloudflare. All devices connect to a virtual mesh network, brokered by Tailscale. Setup Difficulty Difficult Medium Very Easy Needs Open Ports? Yes No No Client App Required? Yes (on client devices) No (for web access) Yes (on all devices) Key Pros High performance, self-hosted, full network control. Simple for web access, no client app for browsers, hides your home IP. \u0026ldquo;It just works\u0026rdquo; simplicity, great for CG-NAT, powerful features (subnet routes). Key Cons Hard to set up, requires port forwarding, fails with CG-NAT. Requires a domain, traffic limits (not for video), less flexible for non-web. Requires their app on all devices, relies on a 3rd-party service. Needless to say, I have chosen Tailscale to work with. I couldn\u0026rsquo;t believe it is a free solution because it gives me so much management and many features. The ability to act as both a subnet route (advertise local IP for devices that don\u0026rsquo;t have the app) and an exit node (forward all the traffic through one device) is just super powerful. I did try using Cloudflare Tunnel at the beginning, but it required me to configure every DNS record each time I wanted to access a service, and it also did not allow me to stream video, so I migrated all of it to Tailscale a year ago.\nCurrently, I have Tailscale installed on my OpenWRT router, which advertises a subnet route of 10.69.0.0/16 and also acts as an exit node. If I go outside, I just simply open the Tailscale app on my phone or my laptop, authenticate, and bang, connected. If I carry a device that can\u0026rsquo;t install Tailscale, I can simply connect it to my travel router (which is the gl-ar300m device in the image above), which I have wrote about here.\nDon\u0026rsquo;t get me wrong, Cloudflare Tunnel is still a solid option. It has one feature that Tailscale couldn\u0026rsquo;t provide, which is the ability to allow guest users to connect to my home services without them needing to install an app or authenticate anything. This is extremely useful for a web blog when I want to showcase something. But for now, I have figured out a way to combine Tailscale and an additional VPS (a virtual private server, which is the racknerd-e54f406 device in the image above) to achieve this.\nSecurely Exposing Services to the Public # One day, I was tinkering with the idea of finding the best VPS provider to host some of my services in case my homelab was on maintenance but I still wanted to access critical services such as Bitwarden (a password manager) and Actual-budget (a budgeting service). Then I found this site called RackNerd. They offered an option that shocked me with its price and specs:\n11$ per year 1 vCPU Core 24 GB Pure SSD Storage 1 GB RAM 2 TB Monthly Transfer 1 Gbps Network Port Full Root Admin Access 1 Dedicated IPv4 Address What\u0026rsquo;s more, I realized I could also connect to services at home using Tailscale + Nginx, which is a reverse proxy. With 1 GB RAM, I could easily install Nginx Proxy Manager in a Docker container. I also needed to configure DNS on Cloudflare to route all DNS requests beginning with *.vps.phuchoang.sbs and vps.phuchoang.sbs to match the public static IP of the VPS. The final step was to install Tailscale on it and then type the home IPs on the destination, and I was good to go.\nHowever, I realized the above method was scary because it exposed the services publicly. If an application doesn\u0026rsquo;t have an authentication method or 2FA built-in, I was afraid hackers might attack my services at home. Therefore, I decided to spin up Authelia, an open-source authentication and authorization server. It allowed me to create a side-car authentication service for every \u0026ldquo;proxy host\u0026rdquo; in Nginx Proxy Manager above. All I needed to do was to add myself as a user and adjust the custom Nginx Configuration on the Advanced tab of each \u0026ldquo;proxy host\u0026rdquo;. Now each time you visit, for example nginx.vps.phuchoang.sbs, it will prompt you for a username and password. Even if you get it right, you have to provide the 2FA codes that are set up for each user. Pretty secure, right?\nSummary # It\u0026rsquo;s amazing to think this all started with a simple WiFi problem on the first floor. What began as a need for a second router (the Xiaomi CR6608) quickly became the \u0026ldquo;brain\u0026rdquo; of my entire home network.\nBy leveraging the power of OpenWRT, I built a custom network from the ground up. NextDNS handles clean and secure DNS (plus ad-blocking) for my whole family, both at home and on the go. Tailscale completely solves the challenge of being behind my ISP\u0026rsquo;s NAT, creating a secure mesh network that lets me access my 10.69.0.0/16 subnet from anywhere.\nFinally, adding a cheap RackNerd VPS with Nginx Proxy Manager and Authelia gives me the best of both worlds: a way to securely expose public services (like a blog) or private services (like Actual-budget) to the internet, all while being protected by a strong 2FA authentication portal. This setup is the backbone of all my projects and has been a powerful, hands-on learning experience.\n","date":"27 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/self-hosted-network-design/","section":"Posts","summary":"\u003cp\u003eA long time ago, my house only had 1 router, located on the third floor. So whenever we needed to access the internet while having dinner on the first floor, we needed to enable 4G or walk near the stairs just to receive a fraction of the WiFi signal. My father decided to give me an important task, which was to figure out how to solve this issue. I initially bought a simple WiFi extender located on the second floor, but after using it for a month, even though we could receive a strong signal from it, we couldn\u0026rsquo;t access the internet because the signal from the router on the 3rd floor to the WiFi extender on the 2nd floor was still really weak.\u003c/p\u003e","title":"Building a Secure Network for Homelab with OpenWRT, NextDNS, Tailscale, and a VPS","type":"posts"},{"content":"","date":"27 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/openwrt/","section":"Tags","summary":"","title":"Openwrt","type":"tags"},{"content":"","date":"27 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/tailscale/","section":"Tags","summary":"","title":"Tailscale","type":"tags"},{"content":"A week ago, my laptop died. When I booted it up, it said something like, fTPM is corrupted, press Y to reset or N to do nothing. Neither of them helped me boot into my machine. So I had a severe mental attack back then, because that machine was the primary one that helped me do my work for school, my lab, and my personal projects. I was lucky to have my friend lend me an old laptop of his, but it was pretty bulky, so I figured I couldn\u0026rsquo;t bring it to school. Therefore, I had to go to the school library to do my work on its public computers.\nLuckily, I have a Microsoft Edge sync account, so all of my tabs, history, extensions, and passwords were replicated exactly like my old setup. However, as it is a school computer, it\u0026rsquo;s managed by the organization, so I couldn\u0026rsquo;t install any programs, [[replicate an exact programming experience on any machine]], or browse my personal home network. Lucky me again, there was a VirtualBox program on the school computer, so I could install Arch on it and run the script to download and configure all of my dotfiles and programs that I frequently use.\nHowever, there was still one big problem: I couldn\u0026rsquo;t access the resources that I host with my homelab, so I still couldn\u0026rsquo;t 100% replicate the network side of my home laptop. Previously, even if I was at school, I could connect via Tailscale to my home router running OpenWRT that advertises a subnet route of 10.69.0.0/16, so I could still access those addresses without having to physically be there. Because the public computer can\u0026rsquo;t install any programs, it can\u0026rsquo;t install Tailscale. But I saw there was a LAN cable connected to it, so I thought maybe I could buy some old router that could also run OpenWRT to hijack the school\u0026rsquo;s internet. Enter the GL.iNet Travel Router.\nThis post is the story of how I turned a tiny, $10 travel router into a powerful networking tool that gave me full, secure access to my entire homelab from a locked-down public computer.\nThe plan # After a long consideration and research, I settled on the GL-AR300M16. It was a tiny, cute little router that was actually (not so much) powerful. It\u0026rsquo;s specs were:\n100 Mbps Ethernet ports 16MB NOR ROM flash (The problem child) 128MB RAM (The lifesaver) Two configurable WAN/LAN ports My goal was to \u0026ldquo;insert\u0026rdquo; my own router between the school\u0026rsquo;s network and the public computer. The computer wouldn\u0026rsquo;t know the difference, but all its traffic would first go through my device, which would be running Tailscale. That way, I could fool the school into thinking I was connected via the school network, but I was actually routing all my packets to my home router. Here\u0026rsquo;s the data flow:\nThe price was really affordable too, only 33 bucks. But as a cheap-ass kid, I was looking to buy something old and used on eBay. Even more surprising, I actually found one that was new/in an unopened box and listed for bidding at 5 bucks. After staying up all night to watch the bid, I successfully got it for 10 + 8 bucks for shipping.\nFirmware reinstallation # After I received the package, the first thing I wanted to do was to uninstall the stock firmware and install the original OpenWRT to free up some space. I don\u0026rsquo;t hold any hatred toward the stock firmware, but with the router only having 16MB of ROM storage, and the firmware size being 15 MB, that left me with less than 1MB to do anything. So, no.\nSearching the internet, I found this site to download the clean version firmware of the router. But when I installed it, it only had OpenWRT version 17, which was so outdated that I couldn\u0026rsquo;t even update the packages properly because the SSL was no longer working. So I hit the internet again to find a guide and, luckily again, I even found the original OpenWRT guide dedicated to this machine.\nThe suggested way of installing OpenWrt on the GL-AR300M is using the u-boot bootloader. It can be accessed by holding the reset button, powering the device and waiting for 5 flashes of the LED. Instructions on how to access u-boot can also be found in the debricking instructions: https://docs.gl-inet.com/en/3/tutorials/debrick/.\nThen one can release the button, and access u-boot on http://192.168.1.1 which requires a PC with a static IP of 192.168.1.2 and netmask 255.255.255.0 .\nAssuming your u-boot is working correctly:\nboot into u-boot (debrick mode) by holding the reset button, powering the device and waiting for the led to flash 5 times. Browse to [http://192.168.1.1](http://192.168.1.1/ \u0026quot;http://192.168.1.1\u0026quot;) Upload glinet_gl-ar300m-nand-initramfs-kernel.bin file to router Wait for it to reboot Telnet to 192.168.1.1 and set a root password, or browse to http://192.168.1.1 if LuCI is installed. You will notice that the router is running in safemode. In order to leave it, one must now flash a regular image factory or sysupgrade (e.g. glinet_gl-ar300m-nand-squashfs-factory.img) from shell or luci. After this second flash, the router will boot normally.\nThe only thing I needed to adjust to access u-boot was that I needed to unplug all the external antennas and the WAN port, leaving only the LAN cable connected to it. Also, I needed to remember to download two files: one initramfs-kernel.bin file for the initial u-boot installation, and the other squashfs-sysupgrade.bin for the sysupgrade installation afterward.\nAlso, as I\u0026rsquo;ll explain later in this article, I was stuck with the issue of Tailscale not working without iptables because OpenWRT decided to use nftables in OpenWRT 22.04 and later. So I needed to install the 21.02 version. Here are the links to the two firmware files:\nopenwrt-21.02.0-ath79-generic-glinet_gl-ar300m16-initramfs-kernel.bin (for the initial u-boot flash) openwrt-21.02.0-ath79-generic-glinet_gl-ar300m16-squashfs-sysupgrade.bin (for the real installation afterward). Install Tailscale # Now for the fun part. I found and tried using this repo but realized the device only had 16MB of NOR ROM, of which 6MB had already been reserved for the firmware. So when I ran the script, it warned me I didn\u0026rsquo;t have enough storage, as it required 15MB.\nTherefore, I decided to follow this repo. Instead of relying on the 16MB NOR ROM storage, it stored the tailscaled and tailscale binaries in /tmp (i.e., the 128 MB RAM) instead. But it still stored the init daemon (which is used for downloading those binaries every time the router boots up) and the state file to keep the login and authentication persistent.\nHowever, with that said, I still had to downgrade the Tailscale version and remove the \u0026ldquo;fetch latest release\u0026rdquo; logic in the usr/bin/tailscale and usr/bin/tailscaled files because the latest version also didn\u0026rsquo;t fit in the available RAM.\ntailscale_version=\u0026#34;1.36.1\u0026#34; # latest_version=`wget -O- https://pkgs.tailscale.com/stable/ | grep tailscale_ | head -1 | cut -d\u0026#39;_\u0026#39; -f 2` # if [ \u0026#34;$tailscale_version\u0026#34; != \u0026#34;$latest_version\u0026#34; ]; then # tailscale_version=$latest_version #fi For Tailscale versions before 1.58.2-1, the init script may need to be modified to force Tailscale to assign an IP to the tailscale0 interface. So I had to modify the init daemon at /etc/init.d/tailscale. After the last procd_append_param, I added: procd_append_param command --tun tailscale0\nAs mentioned previously, Tailscale has trouble running on OpenWRT version 22.04 and later. I tried to follow this guide to fix it, but no luck, so I decided to reinstall firmware 21.02 again.\nwgengine.NewUserspaceEngine(tun \u0026#34;tailscale0\u0026#34;) error: router.Up: setting netfilter mode: exec: \u0026#34;iptables\u0026#34;: executable file not found in $PATH flushing log. logger closing down getLocalBackend error: createEngine: router.Up: setting netfilter mode: exec: \u0026#34;iptables\u0026#34;: executable file not found in $PATH After the re-installation and following the exact guide above, I was finally able to authenticate with Tailscale on the router.\nroot@OpenWrt:~# /etc/init.d/tailscale start root@OpenWrt:~# tailscale up To authenticate, visit: https://login.tailscale.com/a/d7ec72e01e7d4 Success. Some peers are advertising routes but --accept-routes is false # Reboot the machine root@OpenWrt:~# tailscale status Downloading Tailscale 1.36.1_mips .. Downloading \u0026#39;https://pkgs.tailscale.com/stable/tailscale_1.36.1_mips.tgz\u0026#39; Connecting to 199.38.181.239:443 Writing to stdout tailscale_1.36.1_mips/tailscale - 100% |*******************************| 20505k 0:00:00 ETA Download completed (20997485 bytes) Done! 100.98.192.109 openwrt-1 mrphuc26032006@ linux - 100.121.251.90 archlinux mrphuc26032006@ linux offline 100.72.74.109 ipad161 mrphuc26032006@ iOS offline 100.111.147.16 iphone-13 mrphuc26032006@ iOS offline 100.110.49.37 openwrt mrphuc26032006@ linux idle; offers exit node 100.68.207.111 racknerd-e54f406 mrphuc26032006@ linux idle; offers exit node The next step was to route all the traffic that connects to this router to my home network. This was easily done by following this guide.\nCreate a new unmanaged interface via LuCI: Network → Interfaces → Add new interface\nName: tailscale Protocol: Unmanaged Device: tailscale0 Create a new firewall zone via LuCI: Network → Firewall → Zones → Add\nName: tailscale Input: ACCEPT (default) Output: ACCEPT (default) Forward: ACCEPT Masquerading: on MSS Clamping: on Covered networks: tailscale Allow forward to destination zones: Select your LAN (and/or other internal zones or WAN if you plan on using this device as an exit node) Allow forward from source zones: Select your LAN (and/or other internal zones or leave it blank if you do not want to route LAN traffic to other tailscale hosts) Click Save \u0026amp; Apply\n(Optional) Connect to WPA2-EAP network such as eduroam # Although my router has 2 WAN/LAN ports, it also has the capability to act as a WiFi repeater. This can be extremely useful if I\u0026rsquo;m wandering around the school and can\u0026rsquo;t find any LAN ports on the wall; I can still connect to the eduroam network and advertise it back to my phone or tablet to access my home network.\nHowever, eduroam is kinda tricky to set up, even on a computer client. Lucky me again, I found this guide to help me set it up. Albeit, I had to install the wpad-wolfssl package instead of wpad, since I was using wolfssl as a backend service. The normal wpad package didn\u0026rsquo;t allow me to verify the SSL certificate after connecting to it.\n\u0026ldquo;eduroam\u0026rdquo; is a WWPA2-EAP network that allows members of higher education and other institution around the world to use each others WiFi networks with their home credentials. As such, the setup is slightly more complicated than that of other WiFi clients. Especially, the wpad package needs to be upgraded:\nInstall the full version of wpad: opkg update; opkg remove wpad-mini; opkg remove wpad-basic; opkg install wpad; reboot (via SSH, but the web UI works as well). Click \u0026ldquo;Scan\u0026rdquo; on either WiFi interface (but doing this for both seems to create problems with DHCP client) on http://192.168.8.1/cgi-bin/luci/admin/network/wireless, select \u0026ldquo;Join network\u0026rdquo; for any \u0026ldquo;eduroam\u0026rdquo; connection/AP. Select the few possible settings as appropriate, enter anything as password for now, \u0026ldquo;submit\u0026rdquo;. Under \u0026ldquo;Wireless Security\u0026rdquo;, select WWPA2-EAP as \u0026ldquo;Encryption\u0026rdquo;, and set everything else according to your institutions eduroam configuration. Here\u0026rsquo;s my school\u0026rsquo;s configuration: (Optional) Install NextDNS to have local DNS resolution and Ads Blocking on the go # If you want more detail on how I set up NextDNS and Tailscale, you can read it here. But for now, it is my main DNS server to resolve my Kubernetes Traefik resolutions, for ad-blocking services, and for parental controls to limit myself from doom-scrolling too much. Installing it was easier than Tailscale. Just get the NextDNS configuration profile ID, paste it in after you install the package, and you\u0026rsquo;re good to go.\nGo to System -\u0026gt; Software Click the \u0026ldquo;Update lists\u0026rdquo; button Enter \u0026ldquo;luci-app-nextdns\u0026rdquo; in the \u0026ldquo;Download and install package\u0026rdquo; field and click \u0026ldquo;OK\u0026rdquo; Go to Services -\u0026gt; NextDNS and configure NextDNS Summary # For $18, even though it\u0026rsquo;s a weak router, I couldn\u0026rsquo;t ask more of it. It can run Tailscale, act as a WiFi repeater, and handle NextDNS resolution, and with all of that, it still has 28MB left in RAM. What\u0026rsquo;s not to like? Definitely a lifesaver at a time when my laptop was broken. Maybe with this, I will begin to prefer going to school to study and code more than staying at home.\n","date":"20 October 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/self-hosted-travel-router/","section":"Posts","summary":"\u003cp\u003eA week ago, my laptop died. When I booted it up, it said something like, \u003ccode\u003efTPM is corrupted, press Y to reset or N to do nothing\u003c/code\u003e. Neither of them helped me boot into my machine. So I had a severe mental attack back then, because that machine was the primary one that helped me do my work for school, my lab, and my personal projects. I was lucky to have my friend lend me an old laptop of his, but it was pretty bulky, so I figured I couldn\u0026rsquo;t bring it to school. Therefore, I had to go to the school library to do my work on its public computers.\u003c/p\u003e","title":"Accessing a Homelab from a Locked-Down PC with a Travel Router","type":"posts"},{"content":"This project automates the provisioning and configuration of a RKE2 Kubernetes on Proxmox using Terraform and Ansible, with following features:\nAWS S3 for Terraform remote state\nSeparate dev and prod environment variables\nMultiple nodes Proxmox cluster\nkube-vip for high availability virtual IP\nSSL via cert-manager with Cloudflare DNS\nLonghorn for persistent storage\nArgoCD for GitOps deployment\nGithub repo\nBlog posts\nVideo demo:\nGetting Started # 1. Clone the Repository # git clone https://github.com/phuchoang2603/kubernetes-proxmox cd kubernetes-proxmox 2. Set Up Environment Variables for Terraform # cp .env.example .env Then edit .env to reflect your Proxmox IP, credentials, Cloudflare token, etc. You also need to customize your hostnames and IPs in config/k8s_nodes.json and config/longhorn_nodes.json.\nIf you want to use S3 for Terraform state, set the relevant variables in config/dev.s3.tfbackend as well.\n3. Set Up Ansible # You need to have your ssh public key in the keys/ directory for Ansible to use for SSH access to the nodes. You might also want to use uv to manage the Python virtual environment. If not, simply ensure you have Ansible and the required collections installed in your Python environment.\ncp ~/.ssh/id_ed25519.pub keys/ uv venv source .venv/bin/activate uv sync 4. Run the Master Script # cd scripts ./master.sh # If you want to skip Longhorn, SSL, or kube-vip setup, you can use the flags: ./master.sh --skip-longhorn --skip-ssl --skip-kube_vip What the Master Script Does # The master.sh script orchestrates everything:\nPhase 1: Terraform – Provisioning # Configure backend state to use Amazon S3 or not Downloads the base cloud-init image Provisions Kubernetes and (optionally) Longhorn VMs on Proxmox Phase 2: Ansible – Cluster Bootstrap # Installs RKE2 (Kubernetes) Configures kube-vip, Longhorn, and cert-manager + Cloudflare if enabled Credits # Inspired by JimsGarage RKE2 Ansible Playbooks Built with bpg Proxmox Terraform Provider ","date":"31 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/projects/kubernetes-proxmox/","section":"Projects","summary":"\u003cp\u003eThis project automates the provisioning and configuration of a RKE2 Kubernetes on \u003cstrong\u003eProxmox\u003c/strong\u003e using \u003cstrong\u003eTerraform\u003c/strong\u003e and \u003cstrong\u003eAnsible\u003c/strong\u003e, with following features:\u003c/p\u003e","title":"Automate provisioning Kubernetes cluster on Proxmox with Terraform + Ansible","type":"projects"},{"content":"","date":"31 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/devops/","section":"Tags","summary":"","title":"Devops","type":"tags"},{"content":"","date":"31 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"21 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"Over the past few days, I tried deploying an RKE2 Kubernetes cluster on my home Proxmox setup. My initial setup worked, but it quickly became apparent that things weren’t stable or sustainable. RKE2’s auto-deployment behavior, the embedded etcd database, and service load balancing all caused performance and manageability issues. It wasn’t “buggy” in the literal sense, but definitely felt rigid and inefficient — not something I would consider a best-practice deployment for a long-term homelab setup.\nSo I decided to start refactoring the cluster.\nSwitched to a 3-Node Control Plane # Originally, I had 2 RKE2 server nodes. I replaced this with 3 to ensure quorum and enable proper leader election and failover. Kubernetes\u0026rsquo; control plane components like etcd and the scheduler require a majority of nodes to reach consensus, and an odd number ensures this can be calculated unambiguously. With 3 nodes, the cluster can tolerate the failure of one control-plane node while maintaining availability.\nChanged IP Scheme from /24 to /16 # The original setup used the 192.168.69.0/24 subnet. It worked, but quickly felt limiting when I started scaling VMs and reserving address blocks for DHCP, load balancers, and Proxmox infrastructure.\nSo I restructured the network to use the 10.69.0.0/16 subnet, divided as:\n10.69.1.0/24: Static IPs for Proxmox, K8s nodes, internal services\n10.69.2.0/24: DHCP range (start at .2.512, limit 256)\n10.69.3.0/24: Reserved for Kubernetes LoadBalancer services\nProxmox itself was reconfigured to use 10.69.0.1 as its IP and DNS server. Broadcast and netmask were updated accordingly to reflect the expanded range. This gives me much more flexibility to scale and isolate various components without running into IP conflicts.\nGave Up on External Database (Kine) # At one point I considered replacing etcd with an external database using Kine, hoping it would be lighter on disk I/O. After some reading — this post was helpful — I decided it wasn’t worth the added complexity. Kine is a workaround for etcd, not a full solution, and introduces its own overhead and caveats.\nMigrated from MetalLB to kube-vip # I wanted to simplify and consolidate IP assignment across control plane and service load balancers, so I switched from MetalLB to kube-vip. It’s now handling both control plane VIP and LoadBalancer services.\nTo do this:\nI disabled MetalLB entirely (by commenting out its deployment in the Ansible roles I use).\nEnabled svc_enable: true in the kube-vip manifest to allow service load balancing.\nFollowed the official kube-vip cloud provider installation guide to deploy its controller.\nSo far, it works more reliably and integrates cleanly with RKE2’s native load balancer behavior.\nIngress Controller + TLS: nginx vs traefik # I initially tried deploying nginx as my ingress controller with cert-manager for TLS. The plan was to route external access through HTTPS and use ingress rules for reverse proxy and load balancing.\nThat didn’t go smoothly:\nI first attempted to use Traefik (RKE2’s default), but configuring it through HelmChart CRDs was too opaque and hard to control. Then I switched to nginx via a HelmChart manifest — but forgot to rename the .j2 file to .yaml, which caused the Helm controller to silently fail. When I finally got the HelmChart to load, cert-manager couldn’t be fetched via remote repo URLs. I had to download the .tgz, base64 encode it, and inject it via the chartContent field manually. Even after that, cert-manager DNS-01 validation failed until I updated my DNS nameservers to use Cloudflare explicitly. The default nameserver (/etc/resolv.conf) pointed to an internal resolver, which tried to resolve external domains like acme-staging-v02.api.letsencrypt.org.my-domain.local — resulting in failed lookups. In the end, I found out that I need to specify the DNS server when initialize in Terraform Eventually, I circled back to Traefik — mainly because I’m planning to integrate Authentik for application authentication, and Traefik makes that easier to manage with native middlewares and plugin support. I found this issue showing how to re-enable Traefik in RKE2 by simply setting the appropriate value in the HelmChart manifest — something that’s still not properly documented.\nFor cert-manager, I’m likely reverting to basic manifest deployments rather than using HelmCharts. It’s simpler and more transparent.\nLonghorn for Storage # I also deployed Longhorn for persistent storage. I added:\nA new Terraform module to provision extra VMs for storage\nA mirrored Ansible role similar to my existing add-agent role, but with Longhorn=true labels\nInstalled iscsi and nfs-common on each Longhorn node\nI tested Longhorn access through Traefik and cert-manager to confirm that HTTPS access worked as expected.\nWhat’s Next # I’m now looking into deploying Argo CD for GitOps-style CI/CD. With kube-vip, Longhorn, cert-manager, and Ingress all functional, the core of the cluster is ready. Automating deployments with Git is the next step.\n","date":"21 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/terraform-ansible-proxmox-k8s-2/","section":"Posts","summary":"\u003cp\u003eOver the past few days, I tried deploying an RKE2 Kubernetes cluster on my home Proxmox setup. My initial setup worked, but it quickly became apparent that things weren’t stable or sustainable. RKE2’s auto-deployment behavior, the embedded etcd database, and service load balancing all caused performance and manageability issues. It wasn’t “buggy” in the literal sense, but definitely felt rigid and inefficient — not something I would consider a best-practice deployment for a long-term homelab setup.\u003c/p\u003e","title":"Migrating and Rebuilding My RKE2 Kubernetes Cluster on Proxmox","type":"posts"},{"content":"","date":"21 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/series/provision-k8s-on-proxmox/","section":"Series","summary":"","title":"Provision K8s on Proxmox","type":"series"},{"content":"","date":"21 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"Hi everyone! Have you ever tried using AWS, GCP, or Azure? It’s an amazing experience, right? The ability to spin up a couple of VMs, a Kubernetes (K8s) cluster, set up load balancers, high availability — all in just a few clicks. You can focus entirely on developing your product. I mean, yeah, why bother with the hassle of learning how all of that works under the hood?\nWell\u0026hellip; until the cost of cloud services hits you hard. If you just need to run small stuff, or deploy a simple demo web app for showcase purposes, no big deal — as long as you still have free credits from the cloud provider. But if you want to keep using those services, you either have to pay or keep creating fake emails or credit cards to hunt for whatever free credits are left.\nOr\u0026hellip; if you’re a student like me — someone who loves free stuff and enjoys recycling old electronics — why not set up your own cloud provider at home? It’s not as hard or expensive as you might think. You might only need one office desktop PC (i5 8th gen, Pentium, doesn’t really matter), and some RAM and storage. If you have even more? Why not try learning Kubernetes and set up your own cluster?\nThat’s the path I chose. Setting up Kubernetes sounded straightforward — but once I started reading, it became a confusing mess. There’s a lot of stuff to remember: master nodes, worker nodes, high availability, load balancers\u0026hellip; And the setup requirements were not trivial. But since I had just learned Ansible and Terraform a few weeks ago, I decided: Why not make this a triple learning experience?\nAnd so, over the past couple of days, I dove deep into trying to set up my own bare-metal Kubernetes cluster on Proxmox using Terraform and Ansible. In this post, I want to share my experience — not as a fancy tutorial, but more like a overconfident student who struggled, googled like crazy, and finally made it work.\nIf you want the detailed documentation, check this instead:\n👉 https://github.com/phuchoang2603/kubernetes-proxmox\nDemo video:\nMy Goal # Use Terraform to create VMs on Proxmox. Bootstrap the Kubernetes (RKE2) cluster using Ansible. Use Cloud-Init to set up VMs properly. Make everything as clean and automated as possible (so I don\u0026rsquo;t have to do it again manually). Terraform Struggles: FML, I Didn’t Read the Docs # 1. Figuring Out the Connection Between My Machine and Proxmox # Maybe it’s just me, but I found the bpg/proxmox provider documentation incredibly detailed yet still confusing.\nFirst, I had to figure out how to create a terraform user and assign the appropriate role in Proxmox. Then, I needed to generate SSH keys, configure Proxmox to accept them, and figure out how to reference that private key correctly in my Terraform files.\nOn top of that, I needed to separate the SSH connection between the Proxmox API and the VMs themselves. The SSH key I generated above was only for Terraform to manage Proxmox. For the VMs (which are provisioned using Cloud-Init), I needed to inject a separate SSH key for Ansible and personal access.\n2. Okay, Now I Can Connect. But How Do I Do the Magic? # Reading the docs felt all over the place. I ended up cloning the entire bpg/proxmox repo just to dig into the examples and understand how it works under the hood. I figured out how to download the Ubuntu cloud image directly onto Proxmox. Great, one task done.\nBut how do I spin up multiple VMs? I learned to create a VM template using the cloud image, configure the RAM, CPU, and network — so I could clone it later.\nresource \u0026#34;proxmox_virtual_environment_vm\u0026#34; \u0026#34;ubuntu_template\u0026#34; { name = \u0026#34;ubuntu-template\u0026#34; node_name = \u0026#34;pve\u0026#34; template = true started = false machine = \u0026#34;q35\u0026#34; bios = \u0026#34;ovmf\u0026#34; description = \u0026#34;Managed by Terraform\u0026#34; cpu { cores = 2 } memory { dedicated = 2048 } efi_disk { datastore_id = \u0026#34;local\u0026#34; type = \u0026#34;4m\u0026#34; } disk { datastore_id = \u0026#34;local-lvm\u0026#34; file_id = proxmox_virtual_environment_download_file.ubuntu_cloud_image.id interface = \u0026#34;virtio0\u0026#34; iothread = true discard = \u0026#34;on\u0026#34; size = 20 } initialization { ip_config { ipv4 { address = \u0026#34;dhcp\u0026#34; } } user_data_file_id = proxmox_virtual_environment_file.user_data_cloud_config.id } network_device { bridge = \u0026#34;vmbr0\u0026#34; } } resource \u0026#34;proxmox_virtual_environment_download_file\u0026#34; \u0026#34;ubuntu_cloud_image\u0026#34; { content_type = \u0026#34;iso\u0026#34; datastore_id = \u0026#34;local\u0026#34; node_name = \u0026#34;pve\u0026#34; url = \u0026#34;https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img\u0026#34; } 3. Cloud-Init Snippets Were Tricky # After setting up the VM template, I thought spinning up multiple VMs using Cloud-Init would be easy with Terraform\u0026hellip;Until it wasn’t.\nI wanted each VM to have its own hostname (master-1, worker-1, etc.). Because if they don\u0026rsquo;t, after deploying Ansible and ran kubectl get nodes, Kubernetes would only show one node. Turns out, they all had the same default hostname (ubuntu), so Kubernetes thought they were the same.\nAlso, I needed to configure qemu-guest-agent on all VMs so Terraform could detect their state (shutdown, power on, etc.). Without it, Terraform would just freeze and fail the task, even though the VMs were already created.\nFix? Use Cloud-Init snippets to set custom qemu-guest-agent and local-hostname properly for each nodes. Once I did that, all nodes showed up just fine and I can see the qemu-guest-agent on each nodes.\ndata \u0026#34;local_file\u0026#34; \u0026#34;ssh_public_key\u0026#34; { filename = \u0026#34;./id_rsa.pub\u0026#34; } resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;user_data_cloud_config\u0026#34; { content_type = \u0026#34;snippets\u0026#34; datastore_id = \u0026#34;local\u0026#34; node_name = \u0026#34;pve\u0026#34; source_raw { data = \u0026lt;\u0026lt;-EOF #cloud-config hostname: test-ubuntu timezone: America/Toronto users: - default - name: ubuntu groups: - sudo shell: /bin/bash ssh_authorized_keys: - ${trimspace(data.local_file.ssh_public_key.content)} sudo: ALL=(ALL) NOPASSWD:ALL package_update: true packages: - qemu-guest-agent - net-tools - curl runcmd: - systemctl enable qemu-guest-agent - systemctl start qemu-guest-agent - echo \u0026#34;done\u0026#34; \u0026gt; /tmp/cloud-config.done EOF file_name = \u0026#34;user-data-cloud-config.yaml\u0026#34; } } resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;meta_data_cloud_config\u0026#34; { content_type = \u0026#34;snippets\u0026#34; datastore_id = \u0026#34;local\u0026#34; node_name = \u0026#34;pve\u0026#34; source_raw { data = \u0026lt;\u0026lt;-EOF #cloud-config local-hostname: test-ubuntu EOF file_name = \u0026#34;meta-data-cloud-config.yaml\u0026#34; } } resource \u0026#34;proxmox_virtual_environment_vm\u0026#34; \u0026#34;ubuntu_vm\u0026#34; { # ... initialization { # ... user_data_file_id = proxmox_virtual_environment_file.user_data_cloud_config.id meta_data_file_id = proxmox_virtual_environment_file.meta_data_cloud_config.id } # ... } 4. What’s the Right File Structure and Execution Order? # One thing that really tripped me up was whether to keep everything in one big Terraform folder or split things into proper modules. At first, I just dumped all the .tf files together, but it quickly became messy and hard to manage. After some thinking (and breaking things multiple times), I decided to separate the VM template and the Kubernetes cluster into two clear modules. To make things even smoother, I created a Makefile that sources my .env variables automatically and handles terraform init and apply for each module, so I don’t have to type long commands every time. This made my workflow much cleaner and less error-prone.\nAnsible: Why Bother? # That was my initial thought — why not just use Bash scripts? But after watching videos from Jim’s Garage, I realized Ansible might actually make things cleaner, especially with its inventory and roles system.\nhttps://www.youtube.com/watch?v=AnYmetq_Ekc\u0026list=PLXHMZDvOn5sW-EXm2Ur5TroSatW-t0Vz_\u0026index=11\n1. Choosing which method to follow # Initially, I didn’t try the RKE2 script from Jim\u0026rsquo;s Garage because I saw a lot of issues reported on his GitHub repo. So, I switched to Techno Tim’s script (K3s version). But I immediately saw red flags with this project as I encountered an issue where the installation froze and eventually failed, becoming unexecutable afterward.\nI even SSHed into the master nodes to check the logs, but honestly, I couldn’t understand much of it (lmao). I tried to look through his GitHub issues, but it turned out most of the older issues were automatically moved to the discussions section and closed without any replies or troubleshooting provided.\nTo be clear, I’m not trying to speak negatively about his project (it’s open-source, free, and a great effort, by the way). I did try to dive deeper and fix the bugs myself, but it was too complex. So, I ended up going back to the Jim\u0026rsquo;s Garage script.\n2. Initial Success, but Needed Some Modifications # It was definitely a smoother experience diving into Jim\u0026rsquo;s Garage script, but there were still a lot of hard-coded values and links that I needed to adjust.\nFirst, the script didn’t support a dynamic number of master nodes — it only worked with exactly three nodes. So, I had to modify the rke2-prepare and add-server roles to work with just two nodes, which is what I’m using.\nSecond, the script had a hardcoded URL when applying the L2Advertisement manifest, which caused a 502 Bad Gateway error. I had to download the file, make it a local template, and reference it locally.\nLastly, I had to correct the MetalLB load balancer IP range (this was my fault for forgetting basic networking lessons 😅). I had mistakenly set an IP range that exceeded 255, so I fixed it by changing the range back to 230-250.\n3. Python Scripts to Save My Sanity # I also got tired of updating the hosts.ini and group_vars manually.\nSo, I wrote Python scripts to auto-generate them from my k8s_nodes.json and .env files.\nNow I can just update the JSON, run the script, and my Ansible files are updated.\nFinal Thoughts # This project wasn’t just about spinning up a Kubernetes cluster —\nIt taught me about the importance of automation, details, timing, and clean workflows.\nI made a lot of mistakes along the way, but now I have a fully automated setup that I can reuse anytime.\nIt felt like a long journey, but I’m glad I kept pushing through.\nNext time, I’ll remember:\n\u0026ldquo;The magic is in the small details.\u0026rdquo;\n","date":"15 May 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/terraform-ansible-proxmox-k8s/","section":"Posts","summary":"\u003cp\u003eHi everyone! Have you ever tried using AWS, GCP, or Azure? It’s an amazing experience, right? The ability to spin up a couple of VMs, a Kubernetes (K8s) cluster, set up load balancers, high availability — all in just a few clicks. You can focus entirely on developing your product. I mean, yeah, why bother with the hassle of learning how all of that works under the hood?\u003c/p\u003e","title":"How I Struggled (and Learned) to Deploy a Kubernetes Cluster on Proxmox with Terraform and Ansible","type":"posts"},{"content":" How I got here # Hey! I\u0026rsquo;m Hoang Xuan Phuc, but everyone just calls me Felix.\nI\u0026rsquo;m a CompSci student at the University of South Florida (USF), hailing from Vietnam. Growing up, my life was basically a script: Wake up at 6 a.m., school at 7, extra school at 5, eat, sleep, repeat. That was my loop until 5th grade, when my mom gifted me an old laptop.\nMy first thought? Gaming on it all day long. But that dream died fast. I quickly realized I had inherited a \u0026ldquo;potato laptop\u0026rdquo; after trying to run some random first-person shooter game (you know, back when they still used Adobe Flash Player). Sure, it was playable, but the lag was so bad I\u0026rsquo;d have to be a psychic to get a shot in before getting blasted.\nBeing a problem-solving nerd, I got hooked on a new question: How do I make this thing not suck?\nSo, thanks to Google and QuanTriMang.com, I went down the rabbit hole of 20-step optimization guides. I was editing startup programs in msconfig.exe, killing services in services.msc, and even foolishly trying to \u0026ldquo;download more RAM\u0026rdquo; by messing with pagefile.sys (protip: don\u0026rsquo;t do this). Honestly, every megabyte of RAM I freed up was a bigger rush than actually playing the game.\nThis tinkering addiction escalated. Before I knew it, I\u0026rsquo;d installed Linux, ditching games entirely for the promise of it running faster. And man, the learning curve wasn\u0026rsquo;t a curve, it was a cliff. I spent more time fixing the OS than actually using it.\nTouch-pad scrolls too fast? Have fun digging in random config files in the /etc directory; you need to manually change the file content, because there isn\u0026rsquo;t any proper GUI to fix this. How to change it? Oh, the internet said I should use the vim program to edit the file. Okay, it opened, but why can\u0026rsquo;t I type anything? You have to press i to enter Insert Mode? Okay, copied and pasted my line there\u0026hellip; now how the hell do I save and quit??? Yeah, it was annoying. But it was also so much fun. I realized I love digging into the guts of a computer and making it my own. Now, my life script is: wake up at 12 am, school till 6, eat, fake sleep, and then run random commands till 4am, repeat. And then, later on, I found myself not just playing around with my own laptop but playing around with other things like blocking ads on the router, self-hosted a Netflix and MangaDex at home, running Switch emulation on an iPad, hacking a PS4 to install unlimited games, \u0026hellip;\nThe problem is, my brain isn\u0026rsquo;t big enough to remember all these commands and weird tricks, and they\u0026rsquo;re all so cool I just want to talk about it all day long. So that\u0026rsquo;s where this blog comes in. Think of it as my public brain-dump - a place to store my \u0026lsquo;hidden recipes\u0026rsquo; and all the cool (or weird) things I find. It\u0026rsquo;s my personal notebook, a record of my journey, and maybe, just maybe, it\u0026rsquo;ll help or inspire you to break\u0026hellip; er, I mean, tinker with your own computer, too. :wq\nMy \u0026ldquo;On-Paper\u0026rdquo; Stats # Alright, enough stories. If you\u0026rsquo;re interested in my professional journey, my skills, and all that \u0026ldquo;hire me\u0026rdquo; stuff, you can find the details in the resume I\u0026rsquo;ve embedded below.\n","date":"4 April 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/about/","section":"/home/fel1x","summary":"\u003ch2 class=\"relative group\"\u003eHow I got here\n    \u003cdiv id=\"how-i-got-here\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#how-i-got-here\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eHey! I\u0026rsquo;m \u003cstrong\u003eHoang Xuan Phuc\u003c/strong\u003e, but everyone just calls me \u003cstrong\u003eFelix\u003c/strong\u003e.\u003c/p\u003e","title":"Welcome to my blog!","type":"page"},{"content":"","date":"3 April 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/entertainment/","section":"Tags","summary":"","title":"Entertainment","type":"tags"},{"content":"","date":"3 April 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/gaming/","section":"Tags","summary":"","title":"Gaming","type":"tags"},{"content":"If you thought the biggest gaming news was the upcoming Switch 2, think again. The emulation community has made a breakthrough: you can now run Nintendo Switch games on your iPhone or iPad at buttery-smooth 60FPS—all offline, with no jailbreak, no shady VPNs, and zero sketchiness.\nBackground # What is JIT? Why is it important for emulation? # Just-In-Time (JIT) compilation is a method used by emulators to dramatically improve performance by converting code into machine instructions while the program is running. Normally, an emulator has to interpret every single instruction one-by-one, which is slow. JIT speeds things up by compiling larger chunks of code in real time, allowing games to run closer to native speeds. However, Apple has strict security policies that prevent unauthorized JIT execution on iOS devices. There are some workarounds like JITStreamer or SideJITServer but they all require an internet connection. StikJIT and StosVPN now bypass this restriction by enabling JIT locally, without external servers, making offline Nintendo Switch emulation possible on iPhones. Here\u0026rsquo;s the difference between no JIT vs JIT-enabled when running emulator on iOS.\nPPSSPP On iOS 17 (NO JIT vs JIT) iPhone XR\nStosVPN – Internal VPN for enabling JIT without internet # Unlike commercial VPNs that often collect user data, StosVPN simply creates a virtual VPN on the device itself, tricking the system into thinking a “trusted connection” is being made. This lets iOS enable JIT without needing an actual internet connection—boosting privacy and eliminating security risks.\nThis is a breakthrough that allows Switch emulators to run fully offline with high performance.\n🔐 No internet connection needed – No data sent out – No unusual battery drain.\nEssential Tips Before You Begin # Recommended hardware - IOS 17+, Iphone/Ipad with more than 4 GB RAM. Disable Low Power Mode – Emulation requires full performance. If iOS throttles the CPU, the game will lag. Don’t close important background apps – Some apps like SideStore and LiveContainer must keep running for stable emulation. Prepare a Windows or macOS computer. You’ll need one to set up SideStore initially and to generate a pairing file. It\u0026rsquo;s best to put all the needed apps into one folder. Use that folder to transfer games – You can create a Windows shared folder and mount it in the iOS Files app for easier game management. Installing SideStore # SideStore is a sideloading tool (lets you install apps outside the App Store) that doesn’t require jailbreaking and is regularly updated by the community.\nInstallation steps:\nRead the official guide for more in-depth tutorials: SideStore Docs\nGenerate a pairing file:\nDownload jitterbugpair.exe Run it and transfer the *.mobiledevicepairing file to your phone Install SideStore:\nInstall AltServer Windows Guide Hold Shift + Click the AltServer tray icon and sideload the SideStore IPA Final Tweaks:\nInstall StosVPN from the App Store and enable it Open SideStore, pair it with the pairing file Refresh the app and remove previous AltServer certificates Why we need to refresh Sidestore (you may ask)? Normally, Apple limits sideloaded apps (using a free developer account) to 7 days of usage before you have to “refresh” (i.e., re-sign the IPA with a new certificate). When you install SideStore using AltServer, it stops using AltServer’s certificate and instead creates its own certificate, managed within the app itself. This means you won’t need a computer later—just open SideStore on your phone and tap “Refresh” to handle everything automatically. To do that, SideStore needs a way to simulate a local server environment to trick iOS into thinking the app signing process is legitimate. The trick here is using an internal VPN (StosVPN), which creates a loopback server directly on the device.\nInstalling LiveContainer # Apple allows only 3 sideloaded apps to run at once. LiveContainer bypasses this by running apps “inside” it, like a virtual machine. Here’s how to install it:\nDownload the LiveContainer IPA from HugeBlack’s fork Actions tab (requires GitHub account) Open SideStore with StosVPN enabled and add LiveContainer IPA. In LiveContainer Settings: Tap \u0026ldquo;Patch SideStore/AltStore\u0026rdquo; to reinstall it with tweaks. After installation: Reopen SideStore/AltStore. Return to LiveContainer: Tap \u0026ldquo;Test JIT-Less Mode\u0026rdquo;—if it says \u0026ldquo;Test Passed,\u0026rdquo; you’re good to go. Install a second instance of LiveContainer via the main LiveContainer app. In LiveContainer Settings: Set JIT Enabler to StikJIT (Another LiveContainer). Installing MeloNX \u0026amp; Enable StikJIT \u0026amp; Increasing RAM Limits # Apple limits apps to using only half of the device’s RAM, but GetMoreMemory by HugeBlack bypasses this restriction.\nDownload MeloNX \u0026amp; memory entitlement: MeloNX Repo and StikKIT IPA from StikJIT GitHub.\nAdd all three apps to the Apps section in LiveContainer. For each app: long-press the icon → Settings → Convert to Shared App.\nEnable file picker \u0026amp; local notifications in MeloNX settings. Run memory entitlement and log into your account to enable the entitlement for:\nLiveContainer LiveContainer2 MeloNX If errors occur, clean up Keychain and try again. Reinstall LiveContainer \u0026amp; LiveContainer2 to apply the configuration.\nReinstall SideStore from the app (do not just refresh it).\nUpload the pairing file in StikJIT on LiveContainer2 and enable \u0026ldquo;Auto Quit After Enabling JIT.\u0026rdquo; Run MeloNX via LiveContainer1.\nAdding keys, firmware, games on MeloNX # First-time setup:\nLaunch MeloNX via LiveContainer Choose your prod.keys and title.keys files Select your Switch firmware .zip Go to settings and check to see if it has JIT and extended RAM enabled You\u0026rsquo;re done! Just tap MeloNX from LiveContainer whenever you want to play, even offline. Add games (.NSP or .XCI) by tapping the ➕ button.\nI won’t go into detail on how to acquired keys, firmware, and games since this involves piracy. As far as I know, aside from downloading illegally, you can extract your own keys and firmware from your own Switch. For, uh, testing purposes, here’s a little base64:\nS2V5cyAmIEZpcm13YXJlOiBodHRwczovL3Byb2RrZXlzLm5ldC8NCkdhbWVzOiBodHRwczovL25zd2dhbWUuY29tLw== Some useful tips I’ve come across:\nAlways download both the game and its update file for the best performance. Large files may get stuck during transfers due to MeloNX’s lack of a proper file transfer UI. MeloNX Settings for best performance or least ram usage # Use the following settings to get the best possible performance # Shader Cache: On (may causes games to use much more ram which can cause crashes on devices with not enough ram tho) Disable VSync: Off (Enable if you want more than 30/60fps if your device can handle it) Texture Recompression: On MacroHLE: On Docked Mode: Off Resolution Scale: Use the lowest resolution you still find good where it doesn’t crash Memory Manager: Sometimes \u0026#34;Host Unchecked (fast, unstable / unsafe\u0026#34; or \u0026#34;Host (fast)\u0026#34; has better performance Ignore Missing Services: On Debug Logs: Off Trace Logs: Off MVK: Pre-fill Metal Command Buffers: Off Use the following settings to get the least amount of ram usage in a game # Shader Cache: Off Texture Recompression: On MacroHLE: On Docked Mode: Off Resolution Scale: The lower, the better Expand Guest Ram: Off Ignore Missing Services: On Debug Logs: Off Trace Logs: Off Memory Manager Mode: Host (fast) Disable PTC: On List of games that MeloNX currently supports # Compatibility | MeloNX # 3gb+ Devices # Minecraft: Nintendo Switch Edition (Bedrock requires a 8gb+ ram) Sonic Mania Captain Toad: Treasure Tracker Sniper Rescue Helltaker (Homebrew) VVVVVV Cheez it the game One shot world machine 4gb+ Devices (May work on 3gb devices if on ios 18.2.x) # Hue (probably 3gb devices too) Celeste Mario vs Donkey Kong (requires 2-3 tries the first time) Undertale (probably 3gb devices too) Star Wars: The Force Unleashed Carrion Trombone Champ (requires gyro controls which aren’t implemented yet) Dead Cells Thumper Farming Simulator 20 Super Meat Boy Nintendo Switch Online(All) Devil May Cry 3: Special Edition 6gb+ Devices (May work on 4gb devices if on ios 18.2 - 18.2.x) # Untitled Goose Game SCHiM Oceanhorn Super Mario Maker 2 Unravel 2 Super Mario Bros. Wonder Sonic Superstars Cult of Lamb Super Mario 3D World Portal ANTONBLAST Mario Kart 8 Deluxe Super Mario Bros. U Deluxe Outlast Links awakening Pokémon Legends Arceus PayDay 2 Pokémon Sword Oceanhorn (Some flickering but not unplayable) Farming Simulator 23 Goat Simulator Super Mario 3D World Animal Crossing: New Horizons Diablo 3 Eternal Edition Persona 5 Royal Persona 4 Golden Ni No Kuni 2 8gb+ devices (May work on 6gb devices if on ios 18.2.x) # Outer Wilds (set CPU Mode to Software) Skyrim Super Mario Odyssey Cuphead Arkham City Call of Juarez: Gunslinger Thief Simulator Asterix \u0026amp; Obelix XXL 3 Super Mario Party: Superstars Mario \u0026amp; Sonic: At The Olympic Games Super Mario 3D All-Stars (very slow performance) Need for Speed: Hot Pursuit Breath of the Wild Minecraft (Bedrock, Legacy works down to 3gb devices) Burnout Paradise Remastered Echoes of Wisdom Super Mario RPG Splatoon 2 Mario Rabbids: Kingdom Battle Ori and the Blind Forest Splatoon 3 Xenoblade Chronicles 3 Xenoblade Chronicles: Definite Edition 16gb iPads (also works on 8gb devices if on iOS 18.2.x) # Kirby and the Forgotten Land Lego Starwars: The Skywalker Saga (takes a few tries) Tears of the Kingdom Mario Tennis Aces Games which don’t boot/Boots then crash: # Stray Red Dead Redemption (works on Pomelo) Hogwarts Legacy The Witcher 3 Yoshi’s Crafted World DOOM Eternal Terraria Pikuniku (works on Pomelo) Mario Strikers No man’s sky Wolfenstein 2 Outlast 2 Dying Light Boots but too many graphical glitches # Octopath Travaler 2 Credits \u0026amp; Sources # This guide was compiled from various sources and contributions:\nHugeBlack: Developer of LiveContainer and GetMoreMemory 0-Blu: Developer of StikJIT MeloNX Team: Creators of the MeloNX emulator. You can also join the community here. SideStore Team: For enabling sideloading on iOS Various GitHub Repositories \u0026amp; Documentation: SideStore Docs LiveContainer GitHub StikJIT GitHub MeloNX Repo r/EmulationOniOS community, especially this post How to install MeloNX and get it working with fully offline JIT activation. A step by step guide. : r/EmulationOniOS Final Thoughts # Running Tears of the Kingdom on an iPhone sounds like science fiction—but it’s real. Once it’s all set up, you’ll be gaming at full speed, offline, on hardware that was should be meant for this. Even crazier? This lays the groundwork for full-blown VM emulation. Some users are already running macOS Sonoma on iPad using UTM.\n","date":"3 April 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/emulation-switch-ipad/","section":"Posts","summary":"\u003cp\u003eIf you thought the biggest gaming news was the upcoming Switch 2, think again. The emulation community has made a breakthrough: you can now run Nintendo Switch games on your iPhone or iPad at buttery-smooth 60FPS—all offline, with no jailbreak, no shady VPNs, and zero sketchiness.\u003c/p\u003e","title":"Running Nintendo Switch Games on iPhone at 60FPS – Free, No Jailbreak Needed","type":"posts"},{"content":" Video demo: # The Struggle: Why I Needed This # Have you ever stumbled upon a manga so niche and obscure that it never gets an official English release? That was me—desperately trying to read intimate and underrated series that weren’t even on MangaDex, likely due to copyright issues.\nAt first, I took the long road: hunting for raw Japanese scans, screenshotting pages, and running them through image translation tools. It worked\u0026hellip; barely. The process was painfully slow and completely broke my immersion. Instead of enjoying the story, I spent more time waiting on awkward machine translations. It was, in a word, disruptive.\nThen it hit me: I already had Suwayomi, a powerful manga server that supports multiple sources and local file management. Why not build my own translation pipeline?\nFinding a Better Way # My first attempt was to look for an existing solution. I found a closed-source iOS app that seemed promising, but when I saw their $15/month paywall, I immediately bailed. (In case you\u0026rsquo;re curious, here\u0026rsquo;s the link.)\nThat led me to GitHub, where I stumbled upon this amazing project. The demo results blew me away, and I knew I had to dive in and make it work for my setup.\nPrerequisite # To follow along, you’ll need Suwayomi Server installed. You can check out the official Docker setup here:\nSuwayomi-Server-docker GitHub\nAfter installation, set up the download folder and local source location. Reference:\nSuwayomi Local Source Wiki\nHere’s my folder structure for Suwayomi:\n. ├── data #suwayomi folder │ ├── downloads # default download folder │ │ ├── mangas │ │ │ └── Rawkuma (JA) │ │ │ ├── Batsu Hare │ │ │ │ ├── Chapter 100 ... │ │ │ ├── Grapara! │ │ │ │ ├── Chapter 69 ... │ │ │ ├── Guilty Circle │ │ │ │ ├── Chapter 1 ... │ │ │ ├── Isekai Saikouhou no Guild Leader │ │ │ │ ├── Chapter 1 ... │ │ │ └── Spy X Family │ │ │ ├── Chapter 105.5 ... │ │ └── thumbnails │ ├── translated # local source location │ │ ├── Batsu Hare │ │ │ ├── Chapter 100 ... │ │ ├── Grapara! │ │ │ ├── Chapter 69 ... │ │ ├── Guilty Circle │ │ │ ├── Chapter 1 ... │ │ ├── Isekai Saikouhou no Guild Leader │ │ │ ├── Chapter 1 .... │ │ └── Spy X Family │ │ ├── Chapter 105.5 .... └── manga-image-translator ├── * ... ├── batch-script.py # additional batch script file that I wrote to automate the translation. The First Steps: Getting the Basics Running # The manga-image-translator repo seems pretty solid at first try. It provide me the default script that can translate multiple chapters at once.\nScan the INPUT_FOLDER (.i.e. the downloads folder I set up above) for new chapters. Run the manga translator command. Save the output in a matching OUTPUT_FOLDER. (.i.e the translated folder) Example set up:\nsudo apt install cython3 conda create -n manga-trans python=3.12 pip python -m manga_translator local -v -i ../suwayomi/data/downloads/mangas/Rawkuma\\ \\(JA\\)/Grapara\\!\\ Raw/Chapter\\ 13/ --output ../suwayomi/data/translated/Grapara\\!\\ Raw\\ translated/Chapter\\ 13/ --use-gpu --config-file examples/config-example.json This worked, but there was one big problem: it wasn’t fully automated and has a lot of command flags that I need to remember. I had to manually re-run the script every time a new chapter arrived. Clearly, there had to be a better way.\nAutomating the Pipeline: Watching for New Manga Chapters # I started exploring file-watching solutions and tested various Linux tools like inotify and watchdog. However, they didn’t track entire directories the way I needed. I wanted a script that:\nMonitors INPUT_FOLDER (and subfolders) for new content. Triggers translation automatically when a new chapter appears. Maintains the same folder structure in OUTPUT_FOLDER. Enter Python watchdog library - a game-changer. With it, I wrote a script that:\nWatches the manga download directory. Detects when a new chapter folder appears. Extracts the relative path and mirrors it in the output directory. Here’s the script:\nimport os import time import subprocess from watchdog.observers import Observer from watchdog.events import FileSystemEventHandler # ===== Configuration ===== INPUT_ROOT = \u0026#34;data/downloads/mangas/Rawkuma (JA)\u0026#34; OUTPUT_ROOT = \u0026#34;data/translated\u0026#34; CONFIG_FILE = \u0026#34;examples/config-example.json\u0026#34; # ========================== class TranslationHandler(FileSystemEventHandler): def on_created(self, event): if event.is_directory: print(f\u0026#34;Detected new folder: {event.src_path}\u0026#34;) self.process_new_folder(event.src_path) def process_new_folder(self, input_path): # Get relative path from input root relative_path = os.path.relpath(input_path, INPUT_ROOT) dest_path = os.path.join(OUTPUT_ROOT, relative_path) # Create output directory if it doesn\u0026#39;t exist os.makedirs(dest_path, exist_ok=True) command = [ \u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;manga_translator\u0026#34;, \u0026#34;local\u0026#34;, \u0026#34;-v\u0026#34;, \u0026#34;-i\u0026#34;, input_path, \u0026#34;--dest\u0026#34;, dest_path, \u0026#34;--config-file\u0026#34;, CONFIG_FILE, \u0026#34;--use-gpu\u0026#34;, ] try: print(f\u0026#34;Starting translation for: {relative_path}\u0026#34;) subprocess.run(command, check=True) print(f\u0026#34;Successfully translated: {relative_path}\\n\u0026#34;) except subprocess.CalledProcessError as e: print(f\u0026#34;Error translating {relative_path}:\u0026#34;) print(f\u0026#34;The command failed with return code {e.returncode}\u0026#34;) print(\u0026#34;Command executed:\u0026#34;, \u0026#34; \u0026#34;.join(command)) def main(): # Validate paths if not os.path.isdir(INPUT_ROOT): raise ValueError(f\u0026#34;Input folder does not exist: {INPUT_ROOT}\u0026#34;) os.makedirs(OUTPUT_ROOT, exist_ok=True) # Set up folder observer event_handler = TranslationHandler() observer = Observer() observer.schedule(event_handler, INPUT_ROOT, recursive=True) observer.start() try: print(f\u0026#34;Watching directory tree: {INPUT_ROOT}\u0026#34;) print(f\u0026#34;Mirroring structure to: {OUTPUT_ROOT}\u0026#34;) print(\u0026#34;Press Ctrl+C to stop monitoring...\u0026#34;) while True: time.sleep(1) except KeyboardInterrupt: observer.stop() observer.join() if __name__ == \u0026#34;__main__\u0026#34;: main() Running It as a Background Service or a Docker container # I wanted the script to run 24/7 without manually starting it each time. There are two ways to achieve this:\nRun the Python script as a systemd service (requires tinkering with conda). Run it as a Docker container (recommended).\u0026quot;* Option 1: systemd Service (for local installs, not recommended) # Created a new service file: sudo nano /etc/systemd/system/manga-trans.service Added this configuration: [Unit] Description=Manga translation for Suwayomi After=network.target [Service] User=1000 Group=1000 WorkingDirectory=/DATA/AppData/manga-image-translator ExecStart=/bin/bash -c \u0026#34;source ~/miniforge3/etc/profile.d/conda.sh \u0026amp;\u0026amp; conda activate manga-trans \u0026amp;\u0026amp; python batch-script.py\u0026#34; [Install] WantedBy=multi-user.target Enabled and started the service: sudo systemctl daemon-reload sudo systemctl enable manga-trans.service sudo systemctl start manga-trans.service Option 2: Docker Container (recommended) # Rather than relying on conda, I containerized the entire application and push it to my own registry. To run it, you must first create a .env file to specify where the data gonna stored.\nMANGA_FOLDER=/mnt/storage/media/manga APPDATA=/mnt/storage/appdata/suwayomi TZ=America/New_York You also need to change the default location where the new manga will be download to in the Suwayomi web interface.\nHere\u0026rsquo;s my docker-compose.yml setup for the full stack deployment.\nname: suwayomi services: suwayomi: container_name: suwayomi environment: - EXTENSION_REPOS=[\u0026#34;https://raw.githubusercontent.com/keiyoushi/extensions/repo/index.min.json\u0026#34;] - FLARESOLVERR_ENABLED=true - FLARESOLVERR_URL=http://flaresolverr:8191 - TZ=${TZ} hostname: suwayomi image: ghcr.io/suwayomi/suwayomi-server:preview ports: - 4567:4567 restart: always volumes: - ${MANGA_FOLDER}:/home/suwayomi/data - ${APPDATA}/data:/home/suwayomi/.local/share/Tachidesk flaresolverr: container_name: flaresolverr environment: - TZ=${TZ} hostname: flaresolverr image: ghcr.io/flaresolverr/flaresolverr:latest ports: - 8191:8191 restart: unless-stopped manga-image-translator: # build: # context: ${APPDATA}/manga-image-translator image: ghcr.io/phuchoang2603/manga-image-translator:v0.1.0 container_name: manga-image-translator command: batch-script.py volumes: - ${APPDATA}/manga-image-translator/:/app/ - ${APPDATA}/facehuggingcache:/root/.cache/huggingface/ - ${MANGA_FOLDER}:/app/data ipc: host # For GPU deploy: resources: reservations: devices: - capabilities: [gpu] Now, it runs in the background automatically whenever I download a new manga chapter. No more waiting, no more manual intervention—just seamless reading.\nFinal Thoughts # This project completely transformed how I read untranslated manga. No more screenshots, slow translators, or endless searching. Everything is automated, seamless, and runs quietly in the background.\nIf you\u0026rsquo;re someone who digs through obscure manga titles like me, I highly recommend trying this out.\n","date":"3 March 2025","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/self-hosted-manga-trans/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eVideo demo:\n    \u003cdiv id=\"video-demo\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#video-demo\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\n  \u003cdiv\u003e\n\u003ciframe width=\"100%\" height=\"480\" src=\"https://www.youtube.com/embed/RgkC246Ul44\" title=\"Automatic manga translation for Suwayomi.\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n  \u003c/div\u003e\n\n\n\n\u003chr\u003e\n\n\u003ch2 class=\"relative group\"\u003eThe Struggle: Why I Needed This\n    \u003cdiv id=\"the-struggle-why-i-needed-this\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#the-struggle-why-i-needed-this\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eHave you ever stumbled upon a manga so niche and obscure that it never gets an official English release? That was me—desperately trying to read intimate and underrated series that weren’t even on MangaDex, likely due to copyright issues.\u003c/p\u003e","title":"Automating Manga Translations: My Journey to a Seamless Reading Experience","type":"posts"},{"content":"We all love the idea of being super organized, right? We make these grand plans, give it our all for a few weeks or months, and then, bam! We\u0026rsquo;re back to the good old \u0026ldquo;throw it in the downloads folder with everything else\u0026rdquo; routine. It happens to the best of us, and I\u0026rsquo;m guilty too. But hey, even though I\u0026rsquo;ve still got loads of room for improvement, I can proudly say I\u0026rsquo;ve gotten way faster at finding my files.\nPhase One (2014-2019): The Awakening # I stored all my files in the initial phase in the download folder. At that time, with my first laptop and just one attached hard drive, I wasn\u0026rsquo;t concerned about efficiently organizing my files. Typically, I heavily rely on browsers while using our PCs. Most of my files originated from internet downloads—games, program installation, and office files from Gmail, Facebook, and Messenger. As you may know, none of these were particularly crucial: once I downloaded them, I never looked back. However, my awareness of this inefficient system grew as I began experiencing difficulties locating specific files and noticed duplications due to multiple downloads. The tipping point was when I spent nearly an hour searching for a homework document, resulting in a late submission to my teacher. This prompted me to reconsider and change my approach.\nImage courtesy of Folder Tidy\nPhase Two (2019-2021): Aesthetic Revolution # I initiated using folders to categorize file types and employed Internet Download Manager to organize downloaded files based on filename extensions automatically. As someone who values aesthetics, I went further by customizing folder icons for each category, replacing the default \u0026ldquo;folder\u0026rdquo; icon on Windows. Placing these folders in Quick Access significantly enhanced the efficiency of file retrieval.\nHowever, I found the process inefficient when collaborating online with others for studying or project presentations. Organizing files downloaded from shared Drive folders and re-uploading them became time-consuming. Consequently, I realized the need to alter my file organization methods for both offline and online activities.\nPhase Three (2021-2023): Google Drive Renaissance # I dedicated time to thoroughly reorganizing my Google Drive, which had been messy from its use as temporary storage, before transferring files to my offline storage. Despite long-standing plans to address this, the chaotic state of my Google Drive had deterred me. The turning point came when I had to switch from my laptop to a new PC. In the past, I often neglected the backup process: I would have unplugged my hard drive and connected it to the new PC, risking the loss of essential files. But the thing is, there were many important files on it, and I was told by many sources that I should back it up to several Cloud Storage before executing the change. That also allowed me to rearrange things, e.g., filtering out unnecessary files, moving big files onto the USB, and uploading essential files to Google Drive.\nHere\u0026rsquo;s my Drive after fixing it. The two folders, \u0026ldquo;Application\u0026rdquo; \u0026amp; \u0026ldquo;College,\u0026rdquo; are synced with my offline files\nAdditionally, I discovered the Google Drive for Desktop App. It mirrors my Drive files, mounting them to the chosen location\u0026rsquo;s virtual disk. This means all the changes I made on the offline files will be automatically synced with the files on the cloud, reducing the significant time of waiting for uploading.\nCurrent \u0026amp; Future: Expanding Horizons # Recently, I obtained a 5TB OneDrive from the Microsoft 365 free E5 Developer Pack, expanding my file storage capacity. This includes game installation files, video editing projects, and downloaded online courses. Files for collaboration with friends and teachers, such as homework assignments, IELTS, and SAT papers, are stored on Google Drive. Moreover, with the arrival of a refurbished computer that I turned into a Synology NAS, I now have more options for file storage. This NAS serves as my torrent server for streaming films, shows, audiobooks, etc., through platforms like Jellyfin and Audiobookshelf. I plan to share my journey of building self-hosting in the near future, but for now, I am satisfied with my current setup.\nIn the future, I may adopt new techniques to organize my files more efficiently. As of now, I have switched to macOS, and with the wonderful search app called Raycast (Spotlight alternative), I guess I may return to the era where I would store my files a bit more freely, as opposed to the strict tree structure now. Because with the tree structure method, it still takes me several steps to navigate to the files I want. But with the file search method, I just need to learn how to name the files mnemonic, and I can have my files right in the second.\n","date":"16 November 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/productitvity-file-management/","section":"Posts","summary":"\u003cp\u003eWe all love the idea of being super organized, right? We make these grand plans, give it our all for a few weeks or months, and then, bam! We\u0026rsquo;re back to the good old \u0026ldquo;throw it in the downloads folder with everything else\u0026rdquo; routine. It happens to the best of us, and I\u0026rsquo;m guilty too. But hey, even though I\u0026rsquo;ve still got loads of room for improvement, I can proudly say I\u0026rsquo;ve gotten way faster at finding my files.\u003c/p\u003e","title":"Organizing My Digital Life - A Nerdy Journey Across Drives, File Management","type":"posts"},{"content":"","date":"16 November 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/productivity/","section":"Tags","summary":"","title":"Productivity","type":"tags"},{"content":"Today, I would like to introduce you to the journey of optimizing my workspace environment, or in simpler terms, my desk setup. In fact, I am not someone who strictly adheres to principles in life. However, I really enjoy working in a disciplined environment and always strive to optimize it for effective studying and working. To achieve the setup I have today, I had to tear down and rebuild this workstation more than 10 times. Finally, I am truly satisfied with it. It not only satisfies me aesthetically but also makes me eager to work as soon as I step into this room (well, not quite true because nowadays most of the time I feel most productive when working from a coffee shop\u0026hellip;).\nInitial set-up from 2021, when I was just a 9th-grade middle schooler # Looking at the picture, you would probably guess that this is an extremely simple setup. It shows a Lenovo 14-inch Intel Celeron old school laptop that I inherited from my mom when I was just a 7th-grader. It\u0026rsquo;s a typical low-end Windows laptop from before 2015, and I couldn\u0026rsquo;t even take it anywhere because the connector between the monitor and the keyboard was broken. I remember trying to install some games on it, but the only one that ran smoothly was Halflife 2, which is from the 2000s. Despite its limitations, I feel positive towards this laptop because it served my educational needs and taught me how to \u0026ldquo;optimize\u0026rdquo; things\u0026hellip;\nOver the years, although there were countless times when this low-end laptop frustrated me with its 2GB RAM and inability to open more than three Google Chrome tabs, it still fulfilled my educational requirements. Adding a second monitor that my mom had left over from her office was a game changer for me, especially during the COVID-19 pandemic when I had to attend Zoom classes. This additional monitor played a crucial role in helping me get accepted into three top high schools in my country before I eventually got a new PC. This laptop also taught me the initial steps of exploring the Internet effectively and opened up a world of computer science knowledge.\nTo this day, even though I couldn\u0026rsquo;t keep the laptop because I donated it to my cousin, I still use the second monitor as it has become an integral part of my workflow. Since having two screens, my work productivity has significantly increased as I can multitask and access information more easily. In fact, if given the choice between an extremely high-end laptop with only one screen or a mediocre one with an additional screen, I would not hesitate to choose the latter.\nAfter I got into Amsterdam High School, here\u0026rsquo;s my mum\u0026rsquo;s present. # Boom, a big upgrade coming right there. Like, literally from a 50 USD budget laptop to a 700 USD high-end PC. Not to mention this is also the first time I have had the courage to ask my mom for a present, as I usually shy away from asking my parents to spend money on me. However, this time was different because for the first time in my life, my parents were ready to spend this much for my attendance at Amsterdam High School.\nHowever, as much of a tragedy it may seem, this is also the time when hardware components became so expensive due to the rise of Bitcoin and Cryptocurrency mining. So if I had the opportunity to go back, I would probably wait until November 2021 to buy a new one because with the same specifications, I could have spent only around $400 on this one. Still, I am so grateful for this because it was a big transition from my last laptop and it serves my needs way better than its capabilities.\nEverything got replaced except for the DELL second monitor mentioned above. I also took the old hard disk from my laptop to serve as the second hard drive for my computer. Truth be told, initially, I was very excited to set up my workstation. I often visited Facebook groups and watched videos about workspaces, cable management, decorative accessories, and more from various individuals. After countless times hiding under the table trying to make the cables look organized, I finally got the final setup shown above. However, I was too focused on aesthetics without actually realizing what was most important: productivity.\nAfter about three months of attending online Zoom classes where my bed and chair were so close together, I started developing bad habits. Whenever I felt tired or sleepy during class, it was too easy for me to just fall back onto that bed and have a sleepy \u0026ldquo;day\u0026rdquo;. It was good for my health but definitely not good for my studies, so I once again had to make a change.\nSeparate my \u0026ldquo;working me\u0026rdquo; apart from my \u0026ldquo;sleepy me\u0026rdquo; # The picture above that you see right here shows the setup I had been using for nearly two years. As you can see, there is a table serving as a divider between the chair and the bed. This setup prevents me from \u0026ldquo;sleeping\u0026rdquo; when I have to attend a Zoom class or pull an all-nighter. The table not only serves as a place to put additional things on but also separates my \u0026ldquo;work\u0026rdquo; mode from my \u0026ldquo;entertainment\u0026rdquo; mode.\nAdditionally, the second old monitor has now been turned into a vertical one since the space on the table has become narrower. However, the main reason for this change is because there is some content that I really love to view vertically, such as articles, coding documentation, or Phuong Ly\u0026rsquo;s videos -)). Not only can you view more content at once, but it also resembles a big smartphone, allowing you to view optimized social media sites just like when surfing on phones like Medium, Facebook, Reddit,\u0026hellip;\nHowever, it\u0026rsquo;s easy to have an Icarus feeling - never being satisfied with what you currently have. After using two monitors for a while, I have the urge to have three monitors because when I am learning Coursera MOOC courses or watching YouTube reference materials while coding together, the vertical ones really irritate me because they can\u0026rsquo;t scale fullscreen. But I still have to suppress this urge because I know that I don\u0026rsquo;t have enough money to purchase another one and my VGA doesn\u0026rsquo;t have three ports to export screens. Until then,\u0026hellip;\nI once again changed because now I have 3 monitors and a Hackintosh # This is probably the last update before I leave my home to attend university. As you may have anticipated, after 2 years, I finally got a \u0026ldquo;new\u0026rdquo; monitor from my mom\u0026rsquo;s office leftovers (I\u0026rsquo;m really grateful for the whole office there, you know).\nI also switched my operating system from Windows to MacOS, but instead of buying a brand new Mac, I \u0026ldquo;Hackintoshed\u0026rdquo; my computer. Additionally, I purchased a refurbished RX580 VGA from a Facebook marketplace because my old GT1030 had some compatibility issues with MacOS.\nAnd that\u0026rsquo;s the end of my workspace optimization journey. As you probably expected, having a more modern and optimized workspace can lead to increased productivity, right? Well, that\u0026rsquo;s not entirely true. I\u0026rsquo;ve come to realize that true productivity comes from within and is not solely dependent on the hardware or number of monitors you have.\nThe funny thing is that even though my laptop setup when I was in 9th grade had the lowest specifications, I felt second most productive compared to my different setups mentioned above. I guess it\u0026rsquo;s because there were fewer distractions with that setup; all I could do was study. On the other hand, with the high-end PC setup, it\u0026rsquo;s easy to get distracted and procrastinate with games and social media sites. (I\u0026rsquo;ve put in a lot of effort to counteract this distraction and someday I\u0026rsquo;ll write a blog sharing how I\u0026rsquo;ve fought against it). So here\u0026rsquo;s my current most productive setup, which you may not be ready for yet.\n","date":"4 November 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/productivity-workspace/","section":"Posts","summary":"\u003cp\u003eToday, I would like to introduce you to the journey of optimizing my workspace environment, or in simpler terms, my desk setup. In fact, I am not someone who strictly adheres to principles in life. However, I really enjoy working in a disciplined environment and always strive to optimize it for effective studying and working. To achieve the setup I have today, I had to tear down and rebuild this workstation more than 10 times. Finally, I am truly satisfied with it. It not only satisfies me aesthetically but also makes me eager to work as soon as I step into this room (well, not quite true because nowadays most of the time I feel most productive when working from a coffee shop\u0026hellip;).\u003c/p\u003e","title":"Organizing My Digital Life - How My Workspace Evolved from Chaos to Hackintosh","type":"posts"},{"content":"","date":"20 October 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/figma/","section":"Tags","summary":"","title":"Figma","type":"tags"},{"content":"","date":"20 October 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/flutter/","section":"Tags","summary":"","title":"Flutter","type":"tags"},{"content":"","date":"20 October 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/tags/hackathon/","section":"Tags","summary":"","title":"Hackathon","type":"tags"},{"content":"If you want to learn more about the technical details of how I built the app, please read the article here\nToday, I want to share my experience with you about the project I worked on during the Steam Hacks 2023 competition. Although my team did not win any major awards for this project, it was an incredible learning opportunity for me, particularly in terms of coding. Moreover, I also gained valuable insights into the collaborative process through working with my amazing new friend from Son La - Minh Chau.\nBackground information about STEAM Hacks # STEAM Hacks is a national hackathon organized by STEAMS for Vietnam in collaboration with Ha Noi University of Science and Technology. It offers two tracks, Hipster and Hacker, catering to UX/UI designers and developers respectively.\nAlthough I initially had confidence in joining the Hacker Track, I ultimately decided to participate in the Hipster Track. This choice was driven by my desire to have better overall project management skills and to further enhance my UX/UI abilities. However, it turned out that I still had to write code for the team despite being in the Hipster Track. (you would know the reason later on this article)\nThe hackathon consists of three rounds. The first round (Breaker Challenge) is an individual challenge with separate tracks: Hipster and Hacker. Each track would receive support by various workshops, covering the knowledge required to participate to the competition. At the end of the first round, participants from each track are required to submit an assignment that showcases their newly acquired knowledge. For Hipsters, this involves designing an e-commerce landing page with a high-fidelity wireframe created on Figma, along with user personas and user journey details related to their product. Hackers must submit a functional website developed using Flask framework and incorporating some form of \u0026ldquo;AI\u0026rdquo; feature.\n150 participants from both tracks are then chosen for Round 2: Innovation Spirit, where they form teams of 2 Hipsters and 2 Hackers and choose from four tracks: Sustainability, Education, Mental Health, or Community. Only 15 teams advance to the final round, The Hacking Day, with 7 team awards and 2 individual awards. It seems like I came close to being recognized as one of the best Hipsters since I was invited for an interview among the Top 5 Best Hipsters, but I couldn\u0026rsquo;t make it in the end though 😢 (interestingly, the recipient of the Best Hipster Awards was none other than Minh Chau, a fellow member of my team)\nMy initial idea for both first and the next two round # After placing as a Top 10 Finalist in last year\u0026rsquo;s Samsung competition, I couldn\u0026rsquo;t help but feel a sense of dissatisfaction with my product. It lacked certain features I wanted to include, and there were numerous unresolved issues with the app\u0026rsquo;s development. This drove me to participate in this competition, with a focus on creating another application to finish an overall optimized system for reducing e-waste. In the initial round, where I had to design a landing page for an e-commerce site, I immediately decided to develop a marketplace for trading refurbished devices. This solidified my determination to craft a comprehensive product idea for the subsequent rounds, even though I wasn\u0026rsquo;t certain of advancing beyond round 1.\nRecognizing the importance of further developing this idea, I dedicated significant time to meticulously designing wireframes for the project. However, due to personal circumstances, I found myself dangerously close to missing the round 1 deadline. Thankfully, only by submitting the final version just 30 minutes before the cutoff did I manage to make it on time (Pheww). You can access my completed assignment here. This also qualified me for entry into the next round. Additionally, I had the honor of being recognized as one of the top five Best Hipster participants in the competition because of my performance on this round.\nRound 2 - The bond and conflict arise # Upon receiving the email notifying me about the results and next steps, I also discovered that my old friend - Tu Linh - had also participated in the competition and qualified for the second round. The only difference was that he had chosen to join the Hacker track. After a brief discussion, I decided to join him along with two others: his friend Huy, and a randomly recruited girl named Minh Chau. While the three of us were from Hanoi and Minh Chau hailed from Son La, it was her who took the initiative throughout our collaboration (and also contributed the most). That was also the reason why she became a leader for my team. Well, she is really good at project management and business analytics, but underperforming in software development. That also the time I became the Software Development Lead\nThe initial meeting went smoothly as we all introduced ourselves and comfortably shared our ideas from the first round for brainstorming purposes. While I don\u0026rsquo;t want to come across as boastful, it became apparent to me that my assignment was the only one with the potential to win major awards. Huy and Linh from the Hacker Track supported my viewpoint. They agreed that my already beautiful interface and adherence to guidelines would significantly reduce the time spent on front-end development. However, Minh Chau had some reservations. She did like my idea, but she stated that it just so \u0026ldquo;ordinary\u0026rdquo; and \u0026ldquo;unsafe\u0026rdquo;. After several more meetings filled with debates and discussions, we eventually reached a consensus on what we should create for rounds 2 and 3: a counterfeit product verification system.\nYou might be wondering, is that all? Why were you so easily compromised? Well, I did attempt to negotiate with her. However, as mentioned earlier, choosing the idea of creating a marketplace for refurbished devices would result in my team being disqualified from the STEAM Hacks competition if they discovered any correlation or signs of idea copying from my Samsung product last year. On another note, my primary motivation for participating in this competition was not solely to complete my Samsung product but rather to acquire and expand my knowledge and skills.\nOn the development process # Wow, what a nightmare that was. I can\u0026rsquo;t even begin to describe how unproductive the environment was compared to our typically smooth and comfortable team discussions. I don\u0026rsquo;t want to point fingers or be too hard on myself, but it\u0026rsquo;s clear that none of the Hacker members on our team were pulling their weight. It\u0026rsquo;s not entirely their fault though; they excel in competitive coding, but they weren\u0026rsquo;t adequately prepared for web development or a hackathon like this. So, I found myself having to join them in tackling the code development.\nFurthermore, our team was currently facing a conflicting idea with our leader regarding the counterfeit product verification system. Unlike other student projects that typically involve education or mental health, this idea feels challenging for us to create something meaningful. Initially, our leader proposed using AI to detect fake patterns or incorrect labels, but our team lacks the competence in developing AI-related features. Additionally, implementing such a system seemed impossible and unproductive due to the vast variety of products and their unique fake versions. Moreover, there are also concerns about implementation costs and potential returns.\nWell, turns out, every cloud has a silver lining. Firstly, despite my limited technical knowledge, I embraced the opportunity to learn backend development on my own, driven by a strong determination and purpose. This made me feel alive and empowered. Specifically, I was fortunate to have access to the FlutterFlow Education program, which enabled me to quickly learn and utilize their user-friendly software for creating meaningful applications. This greatly reduced our front-end development time and expanded my knowledge of this powerful tool. Furthermore, I acquired proficiency in Flask to develop an API backend application that automatically serves ML models trained through the Teachable Machine platform. This saved me the tedious task of manually researching and collecting data for training models. In short, this challenging experience has taught me invaluable skills and knowledge.\nComing back to the idea conflicting issue, because I initially had doubts about the leader\u0026rsquo;s counterfeit verification concept, I decided to develop the refurbished marketplace feature alongside it. However, I also noticed her dedication and effort towards this idea, despite its nearly impossible implementation. So I had to come all the way to redevelop the application. Finally, on the last day, things became even more intense than before as we submitted our application just five minutes before the deadline. Although there were still flaws and room for improvement, we managed to qualify for the next round - the Hacking Day\u0026hellip;\nFinal revision \u0026amp; new trajectory # Entering the final round was unexpected for us. Our product was only halfway developed and didn\u0026rsquo;t efficiently solve our problems. When we received the results for the second round at midnight, we celebrated together briefly before diving into a meeting for planning and improvement. During this meeting, we had a tense debate as Chau and I wanted to continue finishing the project for the learning experience, while Linh and Huy would only participate if we had a chance of winning a major award. After much discussion, we agreed to aim for the Pitching Award.\nI was extremely busy during this time as I had to focus on my IELTS practice and essay writing. Additionally, I indulged in my habit of tinkering with tech devices and took the opportunity to develop my own server using an old case my mom had brought home. However, returning to the topic of STEAM Hacks, after resolving our conflicts, we came up with a new idea to improve the process of verifying counterfeit products. This involved implementing barcode scanning and utilizing NLP instructions. We discovered that solely using images to identify fake products was not as effective as incorporating physical interactions such as smell or embossed marks. This idea also stemmed from my recent knowledge about NLP and LangChain that I coincidentally learned about just a few weeks ago. And so, without further ado, our journey continues.\nOur journey began just three days before the Hacking Day, thanks to my procrastination. The day before the final, we had a meeting scheduled with our counselor and advisor. It was embarrassing because our team had barely finished the final version of our product. However, this gave us an opportunity to ask our advisor important questions regarding the challenges we were currently facing. As I delved deeper into NLP and specifically LangChain, I discovered that it was poorly developed and relied heavily on tokenization. After a lengthy discussion, we also learned some valuable techniques for optimizing it. And in the same day, we were able to complete our underperforming product, making it ready for the presentation scheduled for the following day.\nOn the day of our presentation, nothing out of the ordinary occurred. However, one memorable moment stands out in my mind. Our team made the decision to demonstrate the live usage of our application. Unfortunately, it didn\u0026rsquo;t go as planned and resulted in uproarious laughter from the audience. Despite this setback, we were able to complete the presentation without further complications (although our question and answer session left much to be desired). Although our team did not receive any awards in the competition, we still managed to capture a photo with smiles all around.\n","date":"20 October 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/hackathon-steam-hacks-2023/","section":"Posts","summary":"\u003cp\u003e\u003cem\u003eIf you want to learn more about the technical details of how I built the app, please read the article here\u003c/em\u003e\u003c/p\u003e","title":"My STEAM Hacks 2023 experience","type":"posts"},{"content":"This is my product when participating in the STEAM Hacks 2023 competition. If you’re interested in delving deeper into the backstory and my journey throughout the competition, you can find the comprehensive details right here.\nhttps://phuchoang.sbs/hackathon/my-steam-hacks-2023-experience/\nOverview of the main application # 1. Current features: # C2C marketplace Detection of fake and real products using the Teachable Machine scanner Barcode scanner \u0026amp; NLP Instruction 2. Planned features: # View sellers’ and buyers’ locations via Google Maps Bidding Algorithm Forecasting product demand and price changes (e.g. beecost…) Image tagging and recognition for product images Search for products using NLP queries and provide recommendations Assess product quality based on images, reviews, descriptions, etc. Allow users to train or utilize the categorization model. https://github.com/phuchoang2603/techburst-counterfeit\nSimplifying Counterfeit Product Verification: A Journey of Implementing User-Friendly Solutions # In an era where counterfeit products flood the market, safeguarding consumers from purchasing fraudulent items has become a pressing concern. As an individual passionate about leveraging technology to combat this issue, my recent venture involved implementing an application that enables users to easily verify the authenticity of products. The process, although challenging, led to the development of a robust solution that empowers users to make informed purchasing decisions.\nImage from ipcloseup.com\nExploring the Implementation Process # During the initial stages, I explored various potential implementations, each presenting its own set of hurdles. I want to make an Android application which have the Computer Vision feature on it. Initially, I came up with the idea of hosting a .Tflite model offline using Android Studio, which seemed promising, but the dependency management and debugging complexities proved daunting. Despite investing countless hours to resolve library errors, achieving a successful attempt remained elusive. Similarly, attempting to host a Transformer model on HuggingFace appeared overly complex, demanding a deep understanding of Transformer and Tensorflow intricacies.\nHowever, the breakthrough emerged with the decision to develop a Flask backend for a REST API server, specifically tailored to handle Teachable Machine exported models. This decision, inspired by a helpful tutorial I stumbled upon, paved the way for a more feasible and practical approach.\nLink of the tutorial: https://sogalanbat.medium.com/custom-api-for-keras-model-using-cloud-run-9d367a2ea5e8\nOvercoming Challenges and Constraints # Finding a way to accomplish my goal was a challenge, but I persevered. However, when the time came to put my plan into action, I realized it was much more difficult than anticipated. At the outset, my lack of prior knowledge in Machine Learning Model and Python Flask REST API posed a steep learning curve. Existing tutorials primarily focused on deploying entire Flask web apps or Tensorflow models, which didn\u0026rsquo;t directly address the Teachable Machine\u0026rsquo;s unique requirements.\nFurthermore, wrestling with outdated Python packages presented a considerable setback. Efforts to convert image URLs directly to tensor files proved unreliable, often yielding inaccurate results due to the limitations in handling diverse image file types. However, persistence and patience ultimately led me to discover a workaround. I learned how to download image URLs directly and convert them to the compatible Keras format, effectively resolving the earlier challenges.\nThe deployment phase, while initially daunting, became more manageable with the aid of a comprehensible Dockerfile tutorial. Thanks to this newfound understanding, I successfully deployed the application on Google Cloud Platform (GCP).\nExisting Limitations and Strategies for Optimization # Despite the successful implementation, certain limitations persist. The machine learning model\u0026rsquo;s inability to recognize products with 100% accuracy remains a challenge. Additionally, scaling the model to encompass a wider range of products proves challenging due to constraints in data resources.\nThe application cannot detect these physical patterns on real and fake products.\nAlthough it achieved a 70% success rate after conducting 400 tests, relying solely on this mechanism may not be deemed reliable.\nTo address these limitations and optimize the user experience, a novel approach has been conceived. Instead of relying solely on the machine learning model to identify differences between fake and authentic products, the application will facilitate user interaction with informative content. Users will receive guidance on how to physically examine the product, potentially by scanning barcodes or utilizing Optical Character Recognition (OCR) to decipher product numbers. The extracted data will then be passed through ChatGPT (LangChain) to source relevant internet pages based on credibility and popularity. A summarized report will then be relayed back to the user, empowering them to make informed decisions.\nHere\u0026rsquo;s a video demonstration of the app.\nThe Journey Continues # As the application evolves, the aim remains to refine the user experience and enhance the application\u0026rsquo;s capabilities. By continually addressing limitations and leveraging cutting-edge technologies, the vision of creating a user-friendly and effective counterfeit product verification solution inches closer to reality. Through perseverance and innovation, the battle against counterfeit products can be better equipped, allowing consumers to shop with confidence and trust.\n","date":"20 October 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/projects/steam-hacks-2023-techburst-counterfeit/","section":"Projects","summary":"\u003cp\u003eThis is my product when participating in the STEAM Hacks 2023 competition. If you’re interested in delving deeper into the backstory and my journey throughout the competition, you can find the comprehensive details right here.\u003c/p\u003e","title":"TechBurst – Revolutionary AI Solution for Identifying Counterfeit Products","type":"projects"},{"content":"It all started with a computer my mom saved from the trash heap at her office. It was a standard HP Prodesk 600 G4—nothing special, and since I already had a PC and a laptop, I had no idea what to do with it. Just installing Windows on it felt like a waste. I knew it could be something more than just another desktop collecting dust.\nThat simple idea kicked off a journey that completely changed what I thought I was passionate about and what I wanted to do with my life.\nMy First, Clumsy Steps # My first goal was to create my own private server to back up all my photos. I stumbled upon something called Xpenology, which is a clever way to run the software from those fancy, expensive Synology servers on any old computer. I spent a whole night fighting with it. It was a huge pain. The whole thing relied on a specific USB stick that had to be plugged in all the time to trick the software into working. It felt messy and unreliable.\nhttps://voz.vn/t/huong-dan-build-case-nas-xpenlogy-dsm-7-2-full-license-synology-surveillance-9-1-2-tan-dung-pc-cu.845346/\nFrustrated, I kept searching and found Proxmox. This was the \u0026ldquo;aha!\u0026rdquo; moment. Proxmox is an operating system built specifically to run other computers inside of it. Think of it like a digital nesting doll. It was stable, powerful, and let me create a virtual \u0026ldquo;computer\u0026rdquo; just for my photo backups. I could finally experiment and mess around without the fear of crashing everything. The beast was finally tamed.\nFinding a Real Purpose (With a Little Help from a GPU) # The server was running, but I knew it could do more. Things got really interesting when I found an old, barely-working GT 1030 graphics card. It was the kind of thing most people would throw out, but I stuck it in the PC, and suddenly, a whole new world of projects opened up.\nFirst, I built my own personal Netflix. I used a free program called Jellyfin to organize all my movies and TV shows into a slick interface. To get the media, I set up a few helper tools that people call the \u0026ldquo;arr stack\u0026rdquo; (Sonarr for TV, Radarr for movies) that automatically find and download stuff for me. That little graphics card did all the heavy lifting, converting video files on the fly so I could watch anything on my TV, laptop, or phone without any stuttering. If you want more detailed on how to properly set this up, here\u0026rsquo;s the link https://trash-guides.info/.\nNext, I got into AI before ChatGPT was cool. I found a program called Stable Diffusion that can create images from just text descriptions. I had a pretty juvenile idea for a Telegram bot: you send it a picture, which then trigger a web-hook to a script I wrote to identify people clothes and send it to the Stable Diffusion server, which then \u0026hellip; well, let\u0026rsquo;s just say \u0026ldquo;creatively redraw\u0026rdquo; the clothing. It was a ridiculous project born from a 16-year-old\u0026rsquo;s hentai brain, but it taught me a ton about programming and how to connect different services together. Here\u0026rsquo;s the repository if anyone interested https://github.com/phuchoang2603/telegram-sdwebui.\nBut the most meaningful thing I built was for my family. I connected our existing security cameras to a program called Frigate. Using the graphics card, Frigate is smart enough to tell the difference between a person walking up to our door and just a tree swaying in the wind. I then hooked Frigate into Home Assistant, which is like a central brain for smart home gadgets. Now, if someone is walking around our house late at night, my entire family gets an alert on their phones with a picture of what’s happening. It’s given us some real peace of mind, all powered by that old office PC.\nThe Obsession and the Path Forward # I was officially hooked. I found online communities like the r/selfhosted and r/homelab subreddits and discovered I wasn\u0026rsquo;t alone. There are tons of people out there building amazing things in their own homes.\nAt first, all my projects only worked inside my house network. But what if I wanted to watch a movie while I was out, or share my photo server with a friend? I learned how to use something called a Cloudflare Tunnel. It sounds complicated, but it\u0026rsquo;s a super secure way to let my projects be seen by the wider internet without opening up my home network to hackers.\nLooking back, it’s wild. This whole journey started with a free computer that was about to be scrapped. In the process of tinkering, I accidentally taught myself about networking, Linux servers, and coding. I wasn\u0026rsquo;t just building a server; I was discovering something I truly loved to do. It made me realize I didn\u0026rsquo;t want to be a UX/UI designer anymore. I wanted to be the person who builds the stuff that works behind the scenes.\nThat old HP Prodesk didn\u0026rsquo;t just find a new purpose; it gave me one, too.\n","date":"27 August 2023","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/self-hosted-initial-story/","section":"Posts","summary":"\u003cp\u003eIt all started with a computer my mom saved from the trash heap at her office. It was a standard HP Prodesk 600 G4—nothing special, and since I already had a PC and a laptop, I had no idea what to do with it. Just installing Windows on it felt like a waste. I knew it could be something more than just another desktop collecting dust.\u003c/p\u003e","title":"My First Server Was Office Trash: A Self-Hosting Story","type":"posts"},{"content":"This is my product created while participating in Samsung Solve for Tomorrow 2022, a STEM contest organized by Samsung Electronic Company that encourages problem-solving skills and showcases the positive impact of young minds on society, with over 76,000 participants nationwide\nDemo # User experience case study # Website # Built during the first round of STEAM Hacks 2023, Reware Website is a C2C marketplace inspired by platforms like eBay and Shopee. It enables users to buy and sell refurbished electronics and features self-serve kiosks for convenient device trade-ins. Reware promotes sustainable tech use by giving old devices a second life while offering users cash incentives.\n","date":"9 November 2022","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/projects/sft2022-reware-app/","section":"Projects","summary":"\u003cp\u003eThis is my product created while participating in Samsung Solve for Tomorrow 2022, a STEM contest organized by Samsung Electronic Company that encourages problem-solving skills and showcases the positive impact of young minds on society, with over 76,000 participants nationwide\u003c/p\u003e","title":"Reware App – Ewaste Management Solutions","type":"projects"},{"content":"This is my diary about the trip to Sapa Hope Center with 10 colleagues from Summit Education - a renowned English language teaching center in Hanoi. Therefore, this trip is aimed at fostering cultural exchange and teaching English and basic skills to students from Grade 1 to Grade 2 in Sapa, Lao Cai.\nDay 1 # After many days of anticipation for new tasks and challenges – my first time as a cameraman – on the charity trip to Sapa with Summit, today finally arrived. I woke up quite early, having only slept for 3 hours the night before. I should have slept more, but I woke up at 3:40 am. After brushing my teeth, I fumbled on my computer to download videos on photography tutorials from YouTube to watch and learn on the way (I was assigned to take photos even though I\u0026rsquo;ve never held a camera before -))). I downloaded the videos and finished preparing my gear by 4:40 am, then called my dad to drive me.\nUpon arriving at the gathering point, I realized I was early af -)) However, that period was also when I started to play around and learn more about the camera. It was quite fascinating, to be honest, because like everything else, it has its own rules and standards that we can learn in a simple and understandable way. Having said that, after getting on the bus and starting to watch the videos I downloaded earlier, I only got a basic grasp of concepts like lens mm, aperture, shutter speed, ISO, \u0026hellip; And I haven\u0026rsquo;t learned anything about framing angles and positioning yet.\nAt Sapa Hope Center, the most surprising thing for me was the extremely warm welcome from the kids. Although many of them couldn\u0026rsquo;t speak Vietnamese (Sapa is a land of convergence for 5 different ethnic groups), they were enthusiastic and innocent. As soon as I arrived, the kids gave each of us a flower and a welcoming card, which they had drawn themselves. The drawings looked simple, but I believe they came from the sincere and innocent hearts of these little ones.\nDuring the self-introduction, I was quite surprised when the owner of the center turned out to be a young man (seemingly stepping out of \u0026ldquo;Silent Sapa\u0026rdquo;) who had studied abroad in Singapore. After visiting this place, he fell in love with it, despite numerous opportunities in the city. Even his relatives thought he was \u0026ldquo;charmed\u0026rdquo; (there is a local belief in enchantment being quite common here). He shared many stories about Sapa – the specially nutritious herbs, the underprivileged children lacking sufficient food, the uphill journey from home to school taking 2-3 hours each day, and more. Peter Thương, along with the kids at the center, made me feel moved every time I heard those genuine and simple stories.\nAfter having lunch and a break, we spent the entire afternoon getting to know each child individually to better understand their personalities. This helped us plan and manage the teaching program more effectively for the upcoming days. Despite having only known each other for a few hours, we engaged in drawing exercises, played games, and sang together, creating a strong bond. The room that day was filled with laughter.\nThe entire group introducing themselves and playing games\nThe older members teaching the children how to draw\nSharing and chatting together\nPlaying the \u0026ldquo;musical chairs\u0026rdquo; game together\nAfter finishing the activities, the sky had darkened, and we were quite tired from the day of travel. We decided to return to the hotel to rest.\nTaken at Praha Hotel, 86 Violet Street, Sapa\nDay 2 # On that morning, instead of going to the large activity hall like yesterday, our group had to go to a dining place to set up a projector to assist in teaching the children. The reason for this was that our goal for the day was to teach the kids English through themed movies and songs, requiring a dark room to make it easier for the children to see.\nDuring the morning teaching session, we played songs and shared stories about the movie Frozen, then guided the children to fill in blanks or answer questions about the movie content. However, it seemed that some of the children had already seen the movie before, and for some, it was a challenging task. Despite our efforts to maintain their interest in the first 1-2 hours, we couldn\u0026rsquo;t make that teaching session as effective as we hoped.\nAfter lunch, Chau Anh (from our group) decided to show the kids a famous Disney movie about three squirrels to regain their interest. However, after 15 minutes of screening, the kids seemed bored with the movie and began to leave the room. Faced with this situation, our group decided to stop showing movies and return to the large activity hall, directly interacting and teaching the kids how to create and play games.\nIndeed, the children here were too accustomed to watching movies whenever there was a foreign charity group visiting, so they had become disinterested in watching another movie. Instead, they enjoyed running and playing with everyone – something they rarely get to do when charity groups visit. Therefore, when given the opportunity to play freely that day, the adorable kids at Sapa Hope Center were very happy and cooperated completely with the older members, creating laughter for both sides.\nThe older members playing games designed by the children\nAfter the play session, we presented books, notebooks, and clothes to the kids Upon returning to the hotel, we held a meeting with the staff to address the issue of the \u0026ldquo;lesson plan burnout\u0026rdquo; that occurred that day. However, what I remember most from that evening was the conversation between me, Ms. Mai, and Chị Minh Hà\u0026rsquo;s mother. Ms. Mai shared a lot about the difficulties and experiences in understanding the psychology of teaching preschool and elementary school children. (There was one saying from her that I found very insightful: \u0026ldquo;It takes losing nine toes to finally realize we need to protect the remaining one.\u0026rdquo;) Chị Minh Hà\u0026rsquo;s mother also shared a lot about life in Russia.\nDay 3 # On that morning, we continued our teaching sessions for the children at Sapa Hope Center. The goal for our group on this day was to educate the children about personal hygiene skills through lively videos and practical exercises.\nHowever, unlike the previous day, this time, we decided to use the large activity hall (rather than the dining area, which was too dark and dirty by evening) for the lecture and for the convenience of teaching in a more dynamic manner (as discussed and planned at the hotel after reflecting on the previous teaching session). Indeed, moving to a spacious and clean room, as shown in the picture, significantly improved the comfort for both the teachers and the children.\nAdditionally, we must acknowledge the enthusiasm of the two \u0026ldquo;teachers,\u0026rdquo; Nhi and Linh, who brought interesting lessons on personal hygiene to the children. The two of them, along with the entire group, collaborated to guide the children in dancing to songs, creating artwork, and participating in related games. Later, the children practiced by taking fun quizzes, such as coloring objects corresponding to their daily hygiene items.\nWhile I wasn\u0026rsquo;t the main photographer for the morning session, I felt delighted to participate in a different role as a support person, helping the children complete their tasks in a joyful and easy manner.\nAfter completing the teaching session at the center, in the afternoon, our group planned to visit each child\u0026rsquo;s home to better understand their family situations and daily lives. At each home, we spent some time listening to the children\u0026rsquo;s stories and the family\u0026rsquo;s struggles. It was an opportunity to gain insight into the hardships they face and learn about the various professions within each household.\nReflection # It can be said that, although this trip is not too short or too long, it is sufficient to allow me to experience, try new things, and learn. There were many firsts for me on this journey: the first time participating in a charity trip without my parents, the first time having to plan and adapt to everything on my own, the first time being the one behind the camera capturing moments and actions throughout the trip, and more. In addition to the experiential lessons from these firsts, I also met and learned a lot from the children, older members, and staff. Everyone helped me engage and listen to many unique and diverse stories. It can be said that this trip will probably leave a deep memory during my high school years.\n","date":"30 June 2022","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/posts/volunteer-sapa-hope-center/","section":"Posts","summary":"\u003cp\u003e\u003cem\u003eThis is my diary about the trip to Sapa Hope Center with 10 colleagues from Summit Education - a renowned English language teaching center in Hanoi. Therefore, this trip is aimed at fostering cultural exchange and teaching English and basic skills to students from Grade 1 to Grade 2 in Sapa, Lao Cai.\u003c/em\u003e\u003c/p\u003e","title":"Lesson Learned in Three-day Trip to Sapa Hope Center","type":"posts"},{"content":"This product emerged from my participation in the BASF Innovation Challenge, a national competition that empowers students to devise sustainability solutions, where we secured the 3rd Runner Up prize. While our aim was not the highest award, this competition established a robust foundation for me and our team to persist in researching and developing solutions to address the serious issue of e-waste\nPitching # Applications User Interface # ","date":"9 June 2022","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/projects/basf-green-express/","section":"Projects","summary":"\u003cp\u003eThis product emerged from my participation in the BASF Innovation Challenge, a national competition that empowers students to devise sustainability solutions, where we secured the 3rd Runner Up prize. While our aim was not the highest award, this competition established a robust foundation for me and our team to persist in researching and developing solutions to address the serious issue of e-waste\u003c/p\u003e","title":"The Green Express – Initial Business Concept of Fostering Ewaste Reduction","type":"projects"},{"content":"","externalUrl":null,"permalink":"/phuchoang2603.github.io/previews/91/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"}]